{
  "research_topic": "Improving fine-tuning performance of language models.",
  "queries": [
    "Parameter-efficient fine-tuning",
    "LoRA fine-tuning",
    "Adapter tuning",
    "Prefix tuning",
    "Prompt tuning"
  ],
  "research_study_list": [
    {
      "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning ",
      "abstract": "Fine-tuning large pre-trained language models on downstream tasks has become\nan important paradigm in NLP. However, common practice fine-tunes all of the\nparameters in a pre-trained model, which becomes prohibitive when a large\nnumber of downstream tasks are present. Therefore, many fine-tuning methods are\nproposed to learn incremental updates of pre-trained weights in a parameter\nefficient way, e.g., low-rank increments. These methods often evenly distribute\nthe budget of incremental updates across all pre-trained weight matrices, and\noverlook the varying importance of different weight parameters. As a\nconsequence, the fine-tuning performance is suboptimal. To bridge this gap, we\npropose AdaLoRA, which adaptively allocates the parameter budget among weight\nmatrices according to their importance score. In particular, AdaLoRA\nparameterizes the incremental updates in the form of singular value\ndecomposition. Such a novel approach allows us to effectively prune the\nsingular values of unimportant updates, which is essentially to reduce their\nparameter budget but circumvent intensive exact SVD computations. We conduct\nextensive experiments with several pre-trained models on natural language\nprocessing, question answering, and natural language generation to validate the\neffectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable\nimprovement over baselines, especially in the low budget settings. Our code is\npublicly available at https://github.com/QingruZhang/AdaLoRA .",
      "full_text": "Published as a conference paper at ICLR 2023 ADALORA: A DAPTIVE BUDGET ALLOCATION FOR PARAMETER -EFFICIENT FINE -TUNING Qingru Zhang†∗, Minshuo Chen‡, Alexander Bukharin†, Nikos Karampatziakis⋄, Pengcheng He⋄, Yu Cheng⋄, Weizhu Chen⋄ and Tuo Zhao† †Georgia Institute of Technology ‡Princeton University ⋄Microsoft Azure AI {qingru.zhang,abukharin3,tourzhao}@gatech.edu mc0750@princeton.edu {nikosk,penhe,yu.cheng,wzchen}@microsoft.com ABSTRACT Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine- tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/ QingruZhang/AdaLoRA. 1 I NTRODUCTION Pre-trained language models (PLMs) have manifested superior performance in various natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; He et al., 2021b; Radford et al., 2019; Brown et al., 2020). The most common way to adapt pre-trained models to down-stream tasks is to fine-tune all the parameters (full fine-tuning, Qiu et al. (2020); Raffel et al. (2020)). However, pre-trained models typically incurs large memory footprint. For example, BERT model (Devlin et al., 2019) consists up to 300 million parameters; T5 (Raffel et al., 2020) comprises up to 11 billion parameters and GPT-3 (Brown et al., 2020) contains up to 175 billion parameters. When building a NLP system upon these pre-trained models, we usually handle multiple tasks that arrive simultaneously (Radford et al., 2019). Given a large number of down-stream tasks, full fine-tuning requires that each task maintains a separated copy of large models. The resulting memory consumption is prohibitively expensive. To address this issue, researchers have proposed two main lines of research to reduce the fine-tuning parameters, while maintaining or even improving the performance of PLMs. Specifically, one line of research focuses on adding small neural modules to PLMs and fine-tune only these modules for each task – the base model is kept frozen and shared across tasks. In this way, only a small number of task-specific parameters are introduced and updated, greatly enhancing the practicality of large models. For example, adapter tuning (Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2020; ∗Work was done during Qingru Zhang’s internship at Microsoft Azure AI. 1 arXiv:2303.10512v2  [cs.CL]  20 Dec 2023Published as a conference paper at ICLR 2023 Wq Wk Wv Wo Wf1 Wf2 88.50 88.75 89.00 89.25 89.50 89.75 90.00 MNLI Matched Acc 88.58 88.98 89.36 89.28 89.91 89.99 (a) Selected weight matrix 1,2,3 4,5,6 7,8,9 10,11,12 78 80 82 84 86 88MNLI Matched Acc 77.87 85.82 88.15 88.6 (b) Selected layers Figure 1: Given the total trainable parameters as 0.28M, we apply LoRA only to selected weight matrices (left) or selected layers (right) of DeBERTaV3-base and compare the fine-tuning performance on MNLI-m. Figure 1a: we only fine-tune a selected type of weight matrix of every transformer layer, including query/key/value projection (Wq, Wk, Wv), output projection (Wo) in the self-attention, and two weight matrices (Wf1 , Wf2 ) in two-layer FFNs. In Figure 1b, we apply LoRA to every weight matrix of the selected layers. He et al., 2022) inserts small neural modules called adapters between the layers of the base model. Prefix tuning (Li & Liang, 2021) and prompt tuning (Lester et al., 2021) attach additional trainable prefix tokens to the input or hidden layers of the base model. These methods have shown to achieve comparable performance to full fine-tuning, while only updating less than 1% of the original model parameters, significantly releasing the memory consumption. Another line of research proposes to model the incremental update of the pre-trained weights in a parameter-efficient way, without modifying the model architecture (Zaken et al., 2021; Guo et al., 2020; Hu et al., 2022). Given a pre-trained weight matrix1 W(0), for example, diff pruning (Guo et al., 2020) models its incremental update ∆ as a sparse matrix. Diff pruning initializes ∆ as the same dimension as W(0) and then prunes ∆ element-wise based on the magnitude of the entries. As such, diff pruning can increase the parameter efficiency substantially by adaptively retaining important updates and pruning unimportant ones. Nonetheless, diff pruning has several limitations. First, it relies on low-level implementation to speed up the computation of unstructured sparse matrices, which is not well supported by existing deep learning frameworks. Therefore, we have to store ∆ as a dense matrix during training. Second, it needs to update every entry of ∆ with their gradients and then prune them. This results in similar computational cost as full fine-tuning (Guo et al., 2020). To overcome these drawbacks, Hu et al. (2022) propose a method named LoRA, which parameterizes ∆ as a low-rank matrix by the product of two much smaller matrices: W = W(0) + ∆ = W(0) + BA, (1) where W(0), ∆ ∈ Rd1×d2 , A ∈ Rr×d2 and B ∈ Rd1×r with r ≪ {d1, d2}. During fine-tuning, only A and B are updated. The rank r is chosen to be much smaller than the dimension of W (e.g., r = 8 when d1 = d2 = 1024). With less than 0.5% additional trainable parameters, the training overhead can be reduced up to 70%, compared to full fine-tuning. However, LoRA achieves comparable or even better performance than full fine-tuning (Hu et al., 2022). Meanwhile, the product of two samll matrices is more friendly to implement and deploy than unstructured sparse matrices in diff pruning. LoRA still has limitations as it prespecifies the rank r of each incremental matrix ∆ identical. This ignores the fact that the importance of weight matrices varies significantly across modules and layers when fine-tuning pre-trained models. To illustrate this point, we present an concrete example in Figure 1. We compare the performance of LoRA when fine-tuning specific modules or layers with the same number of trainable parameters. Figure 1a shows that fine-tuning feed-forward networks (FFN) achieves better performance than self-attention modules. In addition, Figure 1b demonstrates that weight matrices in top layers are more important than those in bottom layers. Adding more trainable parameters to the critical weight matrices can lead to better model performance. In contrast, adding more parameters to those less important weight matrices yields very marginal gains or even hurt model performance. Given the parameter budget, i.e., the number of total trainable parameters, we always prefer to allocate more parameters to those important modules. Distributing the budget evenly to all weight matrices/layers, like LoRA and other methods (e.g., adapter and prefix tuning), often gives suboptimal performance. To this end, a natural question is: How can we allocate the parameter budget adaptively according to importance of modules to improve the performance of parameter-efficient fine-tuning? 1Unless specified otherwise, we use W(0) to denote any pre-trained weight matrix. 2Published as a conference paper at ICLR 2023 To answer this question, we propose a new method –AdaLoRA (Adaptive Low-Rank Adaptation), which dynamically allocates the parameter budget among weight matrices during LoRA-alike fine- tuning. Specifically, AdaLoRA adjusts the rank of incremental matrices to control their budget. Critical incremental matrices are assigned with high rank such that they can capture more fine-grained and task-specific information. Less importance ones are pruned to have lower rank to prevent overfitting and save the computational budget. There are some methods to control the rank of matrices in the existing literature of matrix approximation (Cai et al., 2010; Koltchinskii et al., 2011; Toh & Yun, 2010). Most of them directly compute singular value decomposition (SVD) of a matrix and then truncate the smallest singular values. Such an operation can manipulate the rank explicitly and, more importantly, minimize the difference between the resulting matrix and the original matrix. However, for fine-tuning large models, it becomes prohibitively expensive to iteratively apply SVD for a large number of high-dimensional weight matrices. Therefore, instead of computing SVD exactly, we parameterize ∆ as ∆ = PΛQ to mimic SVD. The diagonal matrix Λ contains singular values while the orthogonal matrices P and Q represent left/right singular vectors of ∆. To regularize the orthogonality of P and Q, an additional penalty is added to training loss. Such a parameterization avoids the intensive computations of SVD. Besides, another advantage is that we only need to drop the unimportant singular values while the singular vectors are maintained. This preserves the possibility of future recovery and stabilizes the training. See a detailed comparison to LoRA in Section 3. Based on our SVD parameterization, AdaLoRA dynamically adjusts the rank of ∆ = PΛQ by importance scoring. Specifically, we divide the incremental matrix PΛQ into triplets, where each triplet Gi contains the i-th singular value and the corresponding singular vectors. To quantify the importance of triplets, we propose a novel importance metric, which takes account of the contribution of every entry in Gi to the model performance (Sanh et al., 2020; Liang et al., 2021; Zhang et al., 2022). Triplets with low importance scores are granted low priority and hence the singular values are zeroed out. Triplets with high importance are retained for fine-tuning. Moreover, we also propose a global budget scheduler to facilitate the training. In particular, we start from an initial parameter budget, which is slightly higher than the final budget, and then gradually reduce it until matching the target. Such a scheduler can improve the training stability and model performance. Please see Section 3 for a detailed description of our importance metric and budget scheduler. We conduct extensive experiments on a wide range of tasks and models to demonstrate the effec- tiveness of AdaLoRA. Specifically, we evaluate the performance using DeBERTaV3-base (He et al., 2021a) on natural language understanding (GLUE, Wang et al. (2019)) and question answering (SQuADv1, Rajpurkar et al. (2016) and SQuADv2, Rajpurkar et al. (2018)) datasets. We also apply our methods to BART-large (Lewis et al., 2019) and evaluate the performance on natural language generation (XSum, Narayan et al. (2018) and CNN/DailyMail, Hermann et al. (2015)) tasks. We show AdaLoRA consistently outperforms the baseline, especially under low budget settings. For example, with less than 0.1% trainable parameters of full fine-tuning, AdaLoRA achieves a 1.2% F1 improvement on the SQuAD2.0 dataset compared with state-of-the-art approaches. 2 B ACKGROUND Transformer-based Models. A typical transformer model consists of L stacked blocks, where each block contains two submodules: a multi-head attention (MHA) and a fully connected FFN. Given the input sequence X ∈ Rn×d, MHA performs the attention function in parallel h heads: MHA (X) = Concat(head1, ...,headh)Wo, headi = Softmax \u0010 XWqi (XWki )⊤/ p dh \u0011 XWvi , where Wo ∈ Rd×d is an output projection and Wqi , Wki , Wvi ∈ Rd×dh are query, key and value projections of head i. dh is typically set to d/h. The other important module is a FFN which consists of two linear transformations with a ReLU activation in between: FFN(X) = ReLU(XWf1 + b1)Wf2 + b2, where Wf1 ∈ Rd×dm and Wf2 ∈ Rdm×d. Finally, a residual connection is used followed by a layer normalization (Ba et al., 2016). Low Rank Adaptation.LoRA (Hu et al., 2022) models the incremental update of the pre-trained weights by the product of two small matrices. For h = W(0)x, the modified forward pass is: h = W(0)x + ∆x = W(0)x + BAx, (2) where W(0), ∆ ∈ Rd1×d2 , A ∈ Rr×d2 and B ∈ Rd1×r with r ≪ {d1, d2}. A typically adopts a random Gaussion initialization while B is initialized with zero to have ∆ = 0 at the beginning of 3Published as a conference paper at ICLR 2023 training. We further denoteAi∗ as the i-th row ofA, B∗i as the i-th column ofB, and Gi = {Ai∗, B∗i} as the i-th doublet. Hu et al. (2022) only apply LoRA to query and value projections (i.e, Wq and Wv) in the MHAs. He et al. (2022) extend it to weight matrices of FFNs (i.e, Wf1 and Wf2 ), leading to the performance improvement . Meanwhile, they propose a unified view of various efficient tuning methods including adapter tuning, prefix tuning and LoRA. 3 A DALORA M ETHOD Our method contains two important components: (i) SVD-based adaptation, which formulates the incremental matrices in the form of singular value decomposition; (ii) Importance-aware rank allocation, which prunes redundant singular values based on our newly-designed importance metric. 3.1 SVD-B ASED ADAPTATION As mentioned in Section 1, we propose to parameterize the incremental updates of the pre-trained weight matrices in the form of singular value decomposition: W = W(0) + ∆ = W(0) + PΛQ, (3) where P ∈ Rd1×r and Q ∈ Rr×d2 represent the left/right singular vectors of ∆ and the diagonal matrix Λ ∈ Rr×r contains the singular values {λi}1≤i≤r with r ≪ min(d1, d2). We further denote Gi = {P∗i, λi, Qi∗} as the triplet containing the i-th singular value and vectors. In practice, since Λ is diagonal, we only need to save it as a vector in Rr. Λ is initialized with zero while P and Q adopt a random Gaussian initialization to ensure ∆ = 0 at the beginning of training. To enforce the orthogonality of P and Q, i.e., P⊤P = QQ⊤ = I, we utilize the following regularizer2: R(P, Q) = ∥P⊤P − I∥2 F + ∥QQ⊤ − I∥2 F. (4) In our method, Λ is iteratively pruned to adjust the rank after each gradient decent step. As mentioned in Section 1, one can directly compute SVD for every ∆ to manipulate singular values. The computational complexity, however, is O(min(d1, d2)d1d2). It becomes extremely expensive to iteratively apply SVD for a large number of high-dimensional incremental matrices. In contrast, our parameterization avoids intensive SVD computation, greatly releasing the computational overhead. We remark that one can also apply structured pruning to LoRA to control the rank (i.e., pruneBA doublet-wise in (1)), whereas it has the following disadvantages. First, when a doublet is measured as unimportant, we have to prune all of its elements. It makes scarcely possible to reactivate the pruned doublets as their entries are all zeroed out and not trained. In contrast, AdaLoRA only masks out the singular values based on (3) while the singular vectors are always maintained. It preserves the potential of future recovery for the triplets dropped by mistake. Second, A and B of LoRA are not orthogonal, meaning the doublets can be dependent with each other. Discarding the doublets can incur larger variation from the original matrix than truncating the smallest singular values. Therefore, the incremental matrices are often altered dramatically after each step of rank allocation, which causes training instability and even hurts generalization. To demonstrate this point, we present an ablation study in Section 4.4, which compares AdaLoRA with structured pruning for LoRA. 3.2 I MPORTANCE -AWARE RANK ALLOCATION We apply the SVD-based adaptation (3) to every weight matrix including Wq, Wk, Wv, Wf1 and Wf2 of each transformer layer. In order to control the budget, we iteratively prune singular values in correspondence to their importance score during the training. For clear reference, we use k to index the incremental matrix, i.e., ∆k = PkΛkQk for k = 1 , . . . , n, where n is the number of adapted weight matrices. We denote the i-th triplet of ∆k as Gk,i = {Pk,∗i, λk,i, Qk,i∗} and its importance score as Sk,i. We further denote the parameter sets P = {Pk}n k=1, E = {Λk}n k=1, Q = {Qk}n k=1 and training cost as C(P, E, Q). With the regularization (4), the training objective is given by L(P, E, Q) = C(P, E, Q) + γ Pn k=1 R(Pk, Qk), where γ > 0 is the regularization coefficient. At the t-th step, we first take a stochastic gradient step to update P(t) k , Λ(t) k and Q(t) k for k = 1, . . . , n. Specifically, for Λ(t) k ˜Λ(t) k = Λ(t) k − η∇Λk L(P(t), E(t), Q(t)), (5) 2We present the experiments in Appendix G to verify the effectiveness of the regularization. 4Published as a conference paper at ICLR 2023 where η > 0 is learning rate. Then, given importance score S(t) k , the singular values are pruned following Λ(t+1) k = T (˜Λ(t) k , S(t) k ), with T (˜Λ(t) k , S(t) k )ii = \u001a ˜Λ(t) k,ii S(t) k,i is in the top-b(t) of S(t), 0 otherwise, (6) where S(t) = {S(t) k,i}1≤k≤n,1≤i≤r contains the importance score of all triplets. Here b(t) is the budget of remaining singular values at the t-th step, which we explain more in Section 3.3. In this way, we leave more budget to the incremental matrices of higher priority by pruning the singular values of less important ones. In the sequel, we introduce several options to design the importance score. Magnitude of singular valuesis the most straightforward way to quantify the importance of every triplet, i.e., Sk,i = |λk,i|. In this way, only the least significant singular values are discarded. It minimizes the deviation from the original matrix and further stabilizes the training. Many existing methods use this criterion to control the rank of matrix (Cai et al., 2010; Koltchinskii et al., 2011; Toh & Yun, 2010). However, we remark that such a simple metric cannot properly quantify the contribution of parameters to model performance. Sensitivity-based importanceis another option for importance scoring, which quantifies the sensi- tivity of parameters to the training loss (Molchanov et al., 2019; Sanh et al., 2020; Liang et al., 2021; Zhang et al., 2022). The prior work, however, leverages the sensitivity to quantify the importance of single entries and applies it for unstructured pruning that prunes weights element-wise. When it turns to our case, we have to design a new metric as the triplets are discarded group-wise. Every entry’s sensitivity ought to be considered and properly combined to quantify the overall contribution of the triplet to model performance. Therefore, we propose a newly-designed importance metric in account of both the singular value and vectors in triplet Gk,i: Sk,i = s(λk,i) + 1 d1 d1X j=1 s(Pk,ji) + 1 d2 d2X j=1 s(Qk,ij), (7) where we calculate the mean importance of Pk,∗i and Qk,i∗ such that Sk,i does not scale with the number of parameters in Gk,i. Here s(·) is a specific importance function for single entries. We can adopt the sensitivity for s(·), which is defined as the magnitude of the gradient-weight product: I(wij) = |wij∇wij L|, (8) where wij is any trainable parameter. (8) essentially approximates the change in loss when a parameter is zeroed out. If the removal of a parameter has a large influence, then the model is sensitive to it and we should retain it (Molchanov et al., 2019; Liang et al., 2021; Zhang et al., 2022). However, Zhang et al. (2022) point out that the sensitivity in (8) is not yet a reliable importance indi- cator. Such a score is estimated on the sampled mini batch. The stochastic sampling and complicated training dynamics incur high variability and large uncertainty for estimating the sensitivity with (8). Therefore, Zhang et al. (2022) propose to resolve this issue by sensitivity smoothing and uncertainty quantification: I (t) (wij) =β1I (t−1) (wij) + (1− β1)I(t)(wij) (9) U (t) (wij) =β2U (t−1) (wij) + (1− β2) \f\f\fI(t)(wij) − I (t) (wij) \f\f\f, (10) where 0 < β1, β2 < 1. I (t) is the smoothed sensitivity by exponential moving average and U (t) is the uncertainty term quantified by the local variation between I(t) and I (t) . Then they define the importance as the product between I (t) and U (t) , which can be another option for s(·): s(t)(wij) = I (t) (wij) · U (t) (wij). (11) We present a detailed ablation study in Section 4.4 to compare the performance of different importance metrics. We find the proposed metric (7) based on the sensitivity variant (11) generally performs best. We summarize the detailed algorithm in Algorithm 1. 5Published as a conference paper at ICLR 2023 Algorithm 1AdaLoRA 1: Input: Dataset D; total iterations T; budget schedule {b(t)}T t=0; hyperparameters η, γ, β1, β2. 2: for t = 1, . . . , Tdo 3: Sample a mini-batch from D and compute the gradient ∇L(P, E, Q); 4: Compute the sensitivity I(t) in (8) for every parameter in {P, E, Q}; 5: Update I (t) as (9) and U (t) as (10) for every parameter in {P, E, Q}; 6: Compute S(t) k,i by (7), for k = 1, . . . , nand i = 1, . . . , r; 7: Update P(t+1) k = P(t) k − η∇Pk L(P, E, Q) and Q(t+1) k = Q(t) k − η∇Qk L(P, E, Q); 8: Update Λ(t+1) k = T (Λ(t) k − η∇Λk L(P, E, Q), S(t) k ) given the budget b(t). 9: end for 10: Output: The fine-tuned parameters {P(T), E(T), Q(T)}. 3.3 G LOBAL BUDGET SCHEDULER As mentioned in Section 1, adjusting the rank is naturally to control the parameter budget in the context of low-rank adaptation. Hence we define the budget b(t) as the total rank of all incremental matrices, i.e., the number of total singular values. Recall that the budget allocation is iteratively conducted during the fine-tuning. To facilitate the training, we propose a global budget scheduler. Specifically, we start from an initial budgetb(0) that is slightly higher than the target budgetb(T) (e.g., 1.5 times of b(T)). We set the initial rank of each incremental matrix as r = b(0)/n. We warm up the training for ti steps, and then follow a cubic schedule to decrease the budget b(t) until it reaches b(T). Finally, we fix the resulting budget distribution and fine-tune the model for tf steps. The exact equation for the budget schedule is presented in Appendix A. This allows AdaLoRA to explore the parameter space first and then focus on the most important weights later. 4 E XPERIMENTS We implement AdaLoRA for fine-tuning DeBERTaV3-base (He et al., 2021a) and BART-large (Lewis et al., 2019). We evaluate the effectiveness of the proposed algorithm on natural language understanding (GLUE, Wang et al. (2019)), question answering (SQuADv1, Rajpurkar et al. (2016) and SQuADv2, Rajpurkar et al. (2018)), and natural language generation (XSum, Narayan et al. (2018) and CNN/DailyMail Hermann et al. (2015)). All the gains have passed significant tests with p <0.05. Implementation Details. We use PyTorch(Paszke et al., 2019) to implement all the algorithms. Our implementation is based on the publicly available Huggingface Transformers3 (Wolf et al., 2019) code-base. All the experiments are conducted on NVIDIA V100 GPUs. LoRA scales ∆x by α/r where α is a constant in r. As a result, the magnitude of output can be consistent given different r. It reduces the efforts of retuning learning rate when varying r. Typically α is set as 16 or 32 and never tuned (Hu et al., 2022; Yang & Hu, 2020). Following LoRA, we add the same scaling for (3) and fix α as LoRA. Besides, in Algorithm 1, we prune singular values every ∆T steps (e.g., ∆T = 100) such that the pruned triplets can still get updated within these intervals and possibly reactivated in future iterations. Baselines. We compare AdaLoRA with the following methods: • Full fine-tuning is the most common approach for adaptation. During fine-tuning, the model is initialized with pre-trained weights and biases, and all model parameters undergo gradient updates. • Bitfit (Zaken et al., 2021) is an effective parameter-efficient fine-tuning method. The method only fine-tunes bias vectors in the pre-trained model. • Adapter tuning (Houlsby et al., 2019; Pfeiffer et al., 2020) inserts two-layer adapters between transformer blocks. We compare with two types of adapter. Houlsby adapter as proposed in Houlsby et al. (2019) is inserted between the self-attention module and the FFN module followed by a subsequent residual connection. Recently, Pfeiffer et al. (2020) propose a more efficient design with adapters only applied after FFN modules and LayerNorm modules (Ba et al., 2016), which we call 3https://github.com/huggingface/transformers 6Published as a conference paper at ICLR 2023 Table 1: Results with DeBERTaV3-base on GLUE development set. The best results on each dataset are shown in bold. We report the average correlation for STS-B.Full FT, HAdapter and PAdapter represent full fine-tuning, Houlsby adapter, and Pfeiffer adapter respectively. We report mean of5 runs using different random seeds. Method # Params MNLI SST-2 CoLA QQP QNLI RTE MRPC STS-B All m/mm Acc Mcc Acc/F1 Acc Acc Acc Corr Ave. Full FT 184M 89.90/90.12 95.63 69.19 92.40/89.80 94.03 83.75 89.46 91.60 88.09 BitFit 0.1M 89.37/89.91 94.84 66.96 88.41/84.95 92.24 78.70 87.75 91.35 86.02 HAdapter 1.22M 90.13/90.17 95.53 68.64 91.91/89.27 94.11 84.48 89.95 91.48 88.12 PAdapter 1.18M 90.33/90.39 95.61 68.77 92.04/89.40 94.29 85.20 89.46 91.54 88.24 LoRAr=8 1.33M 90.65/90.69 94.95 69.82 91.99/89.38 93.87 85.20 89.95 91.60 88.34 AdaLoRA 1.27M 90.76/90.79 96.10 71.45 92.23/89.74 94.55 88.09 90.69 91.84 89.31 HAdapter 0.61M 90.12/90.23 95.30 67.87 91.65/88.95 93.76 85.56 89.22 91.30 87.93 PAdapter 0.60M 90.15/90.28 95.53 69.48 91.62/88.86 93.98 84.12 89.22 91.52 88.04 HAdapter 0.31M 90.10/90.02 95.41 67.65 91.54/88.81 93.52 83.39 89.25 91.31 87.60 PAdapter 0.30M 89.89/90.06 94.72 69.06 91.40/88.62 93.87 84.48 89.71 91.38 87.90 LoRAr=2 0.33M 90.30/90.38 94.95 68.71 91.61/88.91 94.03 85.56 89.71 91.68 88.15 AdaLoRA 0.32M 90.66/90.70 95.80 70.04 91.78/89.16 94.49 87.36 90.44 91.63 88.86 Pfeiffer adapter. The number of trainable parameters is determined by the number of layers, the hidden dimension of adapters and the dimension of their inputs. • LoRA (Hu et al., 2022) is a state-of-the-art method for parameter-efficient fine-tuning. The method parameterizes incremental updates by two small matrices and only fine-tune them. The number of trainable parameter is controlled by the rank r and the number of adapted weight matrices n. Hu et al. (2022) apply LoRA to query and value projections only. In empirical, we find that applying LoRA to all weight matrices, i.e., Wq, Wk, Wv, Wf1 and Wf2 , can further improve its performance (Please see Appendix F). Hence, we compare with this generalized LoRA to maximize its performance. We use publicly available implementation 4 to run all the baselines. Please refer to Hu et al. (2022) and reference therein for details. 4.1 N ATURAL LANGUAGE UNDERSTANDING Models and Datasets.We evaluate the fine-tuning performance of DeBERTaV3-base (He et al., 2021a) using the proposed algorithm. We conduct experiments on the General Language Understand- ing Evaluation (GLUE, Wang et al. 2019) benchmark. The benchmark includes two single-sentence classification tasks, three similarity and paraphrase tasks and four natural language inference tasks. Dataset details are summarized in Appendix B. Implementation Details. DeBERTaV3-base consists of 183 millions parameters. We compare AdaLoRA with the baselines under different budget levels, for example, given the total trainable parameters as 0.3/0.6/1.2 million. In order to match the parameter budget, we select the hidden dimensions of adapters from {8, 16, 32, 64}, set the rank r of LoRA as {2, 4, 8}, and choose the final budget b(T) of AdaLoRA from {144, 288, 576}. Then we set b(0) as 1.5 times of b(T) for AdaLoRA and select the regularization coefficient γ from {0.1, 0.3, 0.5}. We set the exponential moving average parameters β1 and β2 as their default value 0.85. We select the learning rate from {5 × 10−5, 8 × 10−5, 1 × 10−4, 2 × 10−4}. More details are presented in Appendix C. Main results.We compare AdaLoRA with the baseline methods under different budget settings. Table 1 shows experimental results on the GLUE development set. We see that AdaLoRA achieves better or on par performance compared with existing approaches on all datasets under all budget levels. For example, when the parameter budget is 0.3M, AdaLoRA achieves 87.36% accuracy on RTE, which is 1.8% higher than the best-performing baseline. Besides, AdaLoRA with extreme low budget can often perform better than the baselines with higher budget. For example, AdaLoRA achieve 70.04% Mcc. score on CoLA with 0.3M fine-tuning parameters, which is higher than all baseline methods with lager budget (e.g., 0.6M and 1.2M). 4.2 Q UESTION ANSWERING Models and Datasets.We evaluate performance of the proposed algorithm on two question answering (QA) datasets: SQuAD v1.1 (Rajpurkar et al., 2016) and SQuADv2.0 (Rajpurkar et al., 2018), where 4https://github.com/microsoft/LoRA 7Published as a conference paper at ICLR 2023 Table 2: Results with DeBERTaV3-base on SQuAD v1.1 and SQuADv2.0. Here # Params is the number of trainable parameters relative to that in full fine-tuning. We report EM/F1. The best results in each setting are shown in bold. SQuADv1.1 SQuADv2.0 Full FT 86.0 / 92.7 85.4 / 88.4 # Params 0.08% 0.16% 0.32% 0.65% 0.08% 0.16% 0.32% 0.65% HAdapter 84.4/91.5 85.3/92.1 86.1/92.7 86.7/92.9 83.4/86.6 84.3/87.3 84.9/87.9 85.4/88.3 PAdapter 84.4/91.7 85.9/92.5 86.2/92.8 86.6/93.0 84.2/87.2 84.5/87.6 84.9/87.8 84.5/87.5 LoRA 86.4/92.8 86.6/92.9 86.7/93.1 86.7/93.1 84.7/87.5 83.6/86.7 84.5/87.4 85.0/88.0 AdaLoRA 87.2/93.4 87.5/93.6 87.5/93.7 87.6/93.7 85.6/88.7 85.7/88.8 85.5/88.6 86.0/88.9 we use AdaLoRA to fine-tune DeBERTaV3-base. These tasks are treated as a sequence labeling problem, where we predict the probability of each token being the start and end of the answer span. Dataset details can be found in Appendix D. Implementation Details.We compare AdaLoRA with the baseline methods under different parameter budgets. That is we have the number of trainable parameters as 0.08%/0.16%/0.32%/0.65% of total pre-trained parameters. To match the budget requirements, we select the hidden dimensions of adapters from {4, 8, 16, 32, 64}, set the rank r of LoRA as {1, 2, 4, 8} and choose the final total rank b(T) of AdaLoRA from {72, 144, 288, 576}. We set the batch size as 16. We use AdamW (Loshchilov & Hutter, 2019) as the optimizer and we set the learning rate as 1 × 10−3 for AdaLoRA. Please refer to Appendix D for more details. Main Results. Table 2 summarizes experimental results when we fine-tune DeBERTaV3-base under 4 different budget settings: 0.08%, 0.16%, 0.32% and 0.65% of total pre-trained parameters. From the result, we see that AdaLoRA consistently outperforms existing approaches under all the budget levels in term of two evaluation metrics: exact match (EM) and F1. Notice that the performance of Houlsby adapter and Pfeiffer adapter are notably decreased when we reduce the parameter budget. In contrast, our method shows the consistent performance under different budget levels. For example, AdaLoRA achieves 88.7% F1 on SQuADv2.0 with the smallest budget 0.08%. It is close to its performance under the high budget and it is also 1.2% higher than the best-performing baseline. 4.3 N ATURAL LANGUAGE GENERATION Table 3: Results with BART-large on XSum and CNN/DailyMail. Here# Params is the number of trainable parameters relative to that in full fine-tuning. We report R-1/2/L. The best results are shown in bold. # Params Method XSum CNN/DailyMail 100% Full FT 45.49 / 22.33 / 37.26 44.16 / 21.28 / 40.90 2.20% LoRA 43.95 / 20.72 / 35.68 45.03 / 21.84 / 42.15 AdaLoRA 44.72 / 21.46 / 36.4645.00 / 21.89 / 42.16 1.10% LoRA 43.40 / 20.20 / 35.20 44.72 / 21.58 / 41.84 AdaLoRA 44.35 / 21.13 / 36.1344.96 / 21.77 / 42.09 0.26% LoRA 43.18 / 19.89 / 34.92 43.95 / 20.91 / 40.98 AdaLoRA 43.55 / 20.17 / 35.2044.39 / 21.28 / 41.50 0.13% LoRA 42.81 / 19.68 / 34.73 43.68 / 20.63 / 40.71 AdaLoRA 43.29 / 19.95 / 35.0443.94 / 20.83 / 40.96 Models and Datasets.To provide a comparison with the state-of-the-art in natural language gener- ation (NLG) tasks, we apply AdaLoRA to fine-tune a BART-large model (Lewis et al., 2019). We evaluate model performance on two datasets: XSum (Narayan et al., 2018) and CNN/DailyMail (Hermann et al., 2015). Implementation Details.Similarly as DeBERTav3-base, we apply low-rank/SVD-based adaptation to every weight matrix of both encoder and decoder layers. We report ROUGE 1/2/L scores (R-1/2/L, Lin (2004)). We set the training epochs as 15. For XSum, we set the beam length as 8 and batch size 8Published as a conference paper at ICLR 2023 as 64. For CNN/DailyMail, we set the beam length as 4 and batch size as 32. Please see Appendix E for the detailed configuration. Main Results.Experimental results are summarized in Table 3, where we compare the fine-tuning performance under four budget levels: the number of trainable parameters is 0.13%, 0.26%, 1.10% and 2.20% of total pre-trained parameters. We see that AdaLoRA achieves better or on par performance compared with the baseline on both datasets (XSum and CNN/DailyMail) under all the budget levels. For example, AdaLoRA achieves 21.13 R-2 score when budget level is 1.10%, compared with 19.89 for LoRA. 4.4 A NALYSIS Different budget levels.Figure 2 illustrates experimental results of fine-tuning DeBERTaV3-base under different budget levels. We see that on all the three datasets (MNLI-m, SQuADv2.0 and XSum), AdaLoRA achieves consistent performance improvement under all the budget levels compared with the baseline. The performance gain is more significant when increasing the budget for the XSum task, suggesting a high budget can help NLG tasks. Note that on the MNLI and SQuADv2.0 datasets, the performance of AdaLoRA under low budget levels (≤ 1%) can match the results of high budget settings. For example, AdaLoRA achieves 88.78% F1 on SQuADv2.0 when the budget is 0.16%. It is close to the performance (88.89% F1) of the highest budget (4.65%) with a more significant gain over the baseline. 0.08 0.16 0.32 0.65 0.96 1.30 1.95 2.88 # Params (%) 90.2 90.3 90.4 90.5 90.6 90.7Acc (MNLI-m) LoRA  AdaLoRA (a) MNLI 0.08 0.16 0.32 0.65 1.30 2.70 4.65 # Params (%) 87.0 87.5 88.0 88.5 89.0 F1  (b) SQuADv2.0 0.13 0.26 1.1 2.2 4.5 7.9 12.5 # Params (%) 20.0 20.5 21.0 21.5 ROUGE-2  (c) XSum Figure 2: Fine-tuning performance under different budget levels. We compare AdaLoRA with the generalized LoRA that applies to every weight matrix. Comparison to low-rank parameterization.As mentioned in Section 3.1, one can alternatively prune LoRA doublet-wise to conduct the rank allocation. In this case, the doublets are zeroed out entirely, raising the barrier to reactivate them. It can cause training instability and hurt the generalization when some crucial doublets are pruned by mistake. In Table 4, we compare AdaLoRA with pruning LoRA on three datasets (SST-2, RTE, and CoLA) to illustrate this point. We apply the same importance score, budget scheduler and training setups as Section 4.1 for pruning LoRA. We can see that AdaLoRA outperforms pruning LoRA on all the datasets under all the budget levels. Table 4: We present two ablation studies in this table: (i) Comparison between AdaLoRA and structured pruning on LoRA. (ii) Comparison of different importance metrics for AdaLoRA. SST-2 RTE CoLA # Params 0.08% 0.16% 0.65% 0.08% 0.16% 0.65% 0.08% 0.16% 0.65% Prune LoRA 94.84 94.50 94.95 86.28 86.15 87.00 66.71 69.29 69.57 AdaLoRA 95.52 95.80 96.10 87.36 87.73 88.09 70.21 70.04 71.45 s(·) = I(·) 94.61 95.30 95.64 87.36 87.71 88.10 66.71 68.83 70.19 Si = |λi| 95.41 95.41 95.87 87.00 86.28 88.00 67.67 68.44 70.38 Variants of the importance score.Recall that in AdaLoRA, the importance score is defined by the sensitivity and uncertainty of every entry in the triplet (7). In Table 4, we examine two variants of the importance score: (i) changing s(·) in (7) to sensitivity-only; (ii) directly defining Si as |λi|. From the results, we can see that the proposed importance score generally performs best. The other two variants can degenerate the model performance up to 0.9%. The role of two components.We remark that both two components of our method - SVD adaptation and adaptive budget allocation, play vital roles for the performance gain. To demonstrate it, we 9Published as a conference paper at ICLR 2023 compare AdaLoRA with the following variants: (i) SVD-LoRA: fine-tuning only with the proposed SVD-based adaptation in (3) and (4); (ii) LoRA regu: LoRA with orthogonal regularization (4) on A and B; (iii) AdaLoRAγ = 0: AdaLoRA without orthogonal regularization (4). Table 5 present the results when fine-tuning DeBERTaVe-base on SST-2 and MNLI. We can see that fine-tuning only with SVD adaptation shows an improvement over LoRA but cannot match the performance of AdaLoRA. Meanwhile, without SVD orthogonal regularization, the performance of AdaLoRA can degenerate. These results validate that both components contribute to the model performance. Table 5: We present ablation studies about SVD-based adaptation, orthogonal regularization, and budget allocation in this table. For MNLI, we report the average score of m/mm acc. SST-2 MNLI # Params 0.08% 0.16% 0.32% 0.65% 0.08% 0.16% 0.32% 0.65% LoRA 94.38 94.95 - 94.95 90.19 90.34 - 90.57 LoRAregu - 94.61 94.72 94.61 - 90.30 90.40 90.66 SVD-LoRA 95.33 95.18 95.07 95.53 90.28 90.25 90.52 90.62 AdaLoRAγ = 0 95.41 95.10 95.30 95.10 90.37 90.34 90.56 90.43 AdaLoRA 95.64 95.80 96.10 96.10 90.65 90.68 90.66 90.77 The resulting budget distribution.Figure 3 shows the resulting rank of each incremental matrix of DeBERTaV3-base fine-tuned with AdaLoRA. We find that AdaLoRA always prefers to allocating more budget to FFNs and top layers. Such behavior aligns with our empirical conclusions presented in Figure 1 that weight matrices of FFN moduels and top layers are more important for model performance. Hence, it validates that our proposed importance metric can guide AdaLoRA to focus on crucial modules. Meanwhile, the rank distribution generated by AdaLoRA is consistent across different budget levels, tasks and models. It means the number of remaining parameters is linearly scaled with b(T) and hence we can tune b(T) to control the remaining parameters. 1 2 3 4 5 6 7 8 9 10 11 12 Layer Wf2 Wf1 Wo Wv Wk Wq 4 1 5 2 3 5 5 6 10 5 5 0 6 9 9 9 12 11 12 12 12 12 12 2 7 3 5 8 8 10 12 12 12 12 12 5 6 6 10 6 10 11 11 11 12 12 11 9 5 4 5 5 10 9 9 11 12 12 12 12 3 2 5 4 7 7 7 10 11 11 10 3 0 2 4 6 8 10 12 The ﬁnal rank Figure 3: The resulting rank of each incremental matrix when fine-tuning DeBERTaV3-base on MNLI with AdaLoRA. Here the x-axis is the layer index and the y-axis represents different types of adapted weight matrices. 5 C ONCLUSION We propose a parameter-efficient fine-tuning method – AdaLoRA that adaptively allocates the parameter budget according to importance scoring. In AdaLoRA, we parameterize the incremental updates of weight matrices in the form of singular value decomposition. Then, we dynamically allocate the parameter budget among incremental matrices by manipulating the singular values based on a new importance metric. Such an a pproach effectively improves the model performance and parameter efficiency. We conduct extensive experiments on natural language processing, question answering and natural language generation tasks. Results show that AdaLoRA outperforms existing approaches. 10Published as a conference paper at ICLR 2023 REFERENCES Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process- ing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543, 2021a. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations, 2021b. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Vladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 39(5):2302–2329, 2011. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https: //aclanthology.org/2021.emnlp-main.243. 11Published as a conference paper at ICLR 2023 Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pp. 4582–4597. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.353. URL https://doi.org/10.18653/v1/2021. acl-long.353. Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, and Weizhu Chen. Super tickets in pre-trained language models: From model compression to improving generalization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6524–6538, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.510. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74–81, 2004. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019. Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 11264–11272. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.01152. Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the sum- mary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745, 2018. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ¨opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e- Buc, Emily B. Fox, and Roman Garnett (eds.),Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024–8035, 2019. Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter- fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained models for natural language processing: A survey. Science China Technological Sciences, 63(10): 1872–1897, 2020. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020. 12Published as a conference paper at ICLR 2023 Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784–789, Melbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in neural information processing systems, 30, 2017. Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by fine-tuning. 2020. Kim-Chuan Toh and Sangwoon Yun. An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems. Pacific Journal of optimization, 6(615-640):15, 2010. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv preprint, abs/1910.03771, 2019. Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522, 2020. Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021. Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, and Tuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight importance. In International Conference on Machine Learning, pp. 26809–26823. PMLR, 2022. 13Published as a conference paper at ICLR 2023 A G LOBAL BUDGET SCHEDULE As mentioned in Section 3.3, we propose a global budget scheduler to gradually decrease the budget b(t) following a cubic schedule. The detailed equation is given as follows: b(t) =    b(0) 0 ≤ t < ti b(T) + \u0000 b(0) − b(T)\u0001\u0010 1 − t−ti−tf T−ti−tf \u00113 ti ≤ t < T− tf b(T) o.w. . (12) B GLUE D ATASET STATISTICS We present the dataset statistics of GLUE (Wang et al., 2019) in the following table. Table 6: Summary of the GLUE benchmark. Corpus Task #Train #Dev #Test #Label Metrics Single-Sentence Classification (GLUE) CoLA Acceptability 8.5k 1k 1k 2 Matthews corr SST Sentiment 67k 872 1.8k 2 Accuracy Pairwise Text Classification (GLUE) MNLI NLI 393k 20k 20k 3 Accuracy RTE NLI 2.5k 276 3k 2 Accuracy QQP Paraphrase 364k 40k 391k 2 Accuracy/F1 MRPC Paraphrase 3.7k 408 1.7k 2 Accuracy/F1 QNLI QA/NLI 108k 5.7k 5.7k 2 Accuracy Text Similarity (GLUE) STS-B Similarity 7k 1.5k 1.4k 1 Pearson/Spearman corr C N ATURAL LANGUAGE UNDERSTANDING C.1 B UDGET CONFIGURATION For each budget level, we tune the final budget b(T) for AdaLoRA, the rank r for LoRA, the hidden dimension d for two adapters to match the budget requirements. Table 7: Detailed budget setup for GLUE benchmark. # Params Houlsby Adapter (d) Pfeiffer Adapter ( d) LoRA ( r) AdaLoRA ( b(T)) 1.2M 32 64 8 576 0.6M 16 32 4 288 0.3M 8 16 2 144 Alternatively, we can also set the final average rank ¯r(T) = b(T)/n for AdaLoRA to control the budget, which is set as 2, 4, and 8 given the final budget as 144, 288, and 576 respectively. Then we select the initial rank r from {4, 6, 12} for the final average rank {2, 4, 8} respectively. C.2 T RAINING DETAILS We tune the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best learning rate for every method. For each dataset, the batch size is set as identical for every method. 14Published as a conference paper at ICLR 2023 Table 8: Hyper-parameter setup of AdaLoRA for GLUE benchmark. Dataset learning rate batch size # epochs γ t i ∆T tf MNLI 5 × 10−4 32 7 0.1 8000 100 50000 RTE 1.2 × 10−3 32 50 0.3 600 1 1800 QNLI 1.2 × 10−3 32 5 0.1 2000 100 8000 MRPC 1 × 10−3 32 30 0.1 600 1 1800 QQP 5 × 10−4 32 5 0.1 8000 100 25000 SST-2 8 × 10−4 32 24 0.1 6000 100 22000 CoLA 5 × 10−4 32 25 0.5 800 10 3500 STS-B 2.2 × 10−3 32 25 0.1 800 10 2000 D Q UESTION ANSWERING D.1 B UDGET CONFIGURATION Given the budget, we control the trainable parameters for each method as the following table. Table 9: Detailed budget setup for question answering. # Params Houlsby Adapter Pfeiffer Adapter LoRA AdaLoRA d d r b (T)/¯r(T)/r 0.65% 32 64 8 576 / 8 / 12 0.32% 16 32 4 288 / 4 / 6 0.16% 8 16 2 144 / 2 / 4 0.08% 4 8 1 72 / 1 / 2 D.2 T RAINING DETAILS We set the batch size as 16. We select the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best-performing learning rate for every method. The configuration of AdaLoRA is listed in the following table. Table 10: Hyper-parameter setup of AdaLoRA for question answering tasks. Dataset learning rate batch size # epochs γ t i ∆T tf SQuADv1.1 1 × 10−3 16 10 0.1 5000 100 25000 SQuADv2.0 1 × 10−3 16 12 0.1 5000 100 50000 D.3 D ATASET The statistics of question answering datasets are summarized in Table 11. Table 11: Statistics of the SQuAD dataset. # Train # Validation SQuAD v1.1 87,599 10,570 SQuAD v2.0 130,319 11,873 E N ATURAL LANGUAGE GENERATION E.1 B UDGET CONFIGURATION Given the budget, we control the trainable parameters for each method as the following table. 15Published as a conference paper at ICLR 2023 Table 12: Detailed budget setup for summarization tasks. # Params Houlsby Adapter Pfeiffer Adapter LoRA AdaLoRA d d r b (T)/¯r(T)/r 0.65% 32 64 8 576 / 8 / 12 0.32% 16 32 4 288 / 4 / 6 0.16% 8 16 2 144 / 2 / 4 0.08% 4 8 1 72 / 1 / 2 E.2 T RAINING DETAILS We set the batch size as 16. We select the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best-performing learning rate for every method. The configuration of AdaLoRA is listed in the following table. Table 13: Hyper-parameter setup of AdaLoRA for summarization tasks. Dataset learning rate batch size # epochs γ t i ∆T tf XSum 5 × 10−4 64 25 0.1 6000 100 50000 CNN/DailyMail 5 × 10−4 32 15 0.1 5000 100 85000 F A BLATION STUDY FOR LORA As mentioned in Section 4, we find that the performance of LoRA can be further improved when applying it to every weight matrix, compared to fine-tuning Wq and Wv only (Hu et al., 2022). This observation aligns with the empirical results of He et al. (2022). In Table 14, we follow the same training configuration as Section 4.1 and present an ablation study to illustrate this point. Table 14: We compare the fine-tuning performance when apply LoRA to every weight matrix or Wq, Wv only. The parameter budget is fixed as 0.3M. We report accuracy for QQP and MRPC, accuracy(m) for MNLI, and average correlation for STS-B. MNLI QQP CoLA RTE QNLI SST-2 MRPC STS-B LoRA (Wq, Wk) 89.80 90.48 67.04 83.75 93.69 94.84 90.20 91.05 LoRA (all) 90.30 91.61 68.71 85.56 94.31 94.95 90.44 91.68 G O RTHOGONAL REGULARIZATION To verify the effectiveness of (4), we plot ∥P⊤P − I∥2 F and ∥QQ⊤ − I∥2 F to show whether P and Q are regularized to be orthogonal. We fine-tune a DeBERTaV3-base model on SST-2 with AdaLoRA and follow the same training configuration as Section 4.1. We set γ as 0.1 and plot the two terms along the training horizon. From Figure 4, we can see that two regularization terms can be optimized to a very small value (e.g., 0.001) at the beginning of training. Therefore, both P and Q can be enforced to be orthogonal quickly during the initial warm-up of AdaLoRA. It ensures that the triplets are not dependent with each other. H C OMPARISON OF TRAINING COST We compare the training cost between AdaLoRA and LoRA in the following table. We use two methods to fine-tune DeBERTaV3-base on a single NVIDIA V100 GPU. We do training only and set hyperparameters, e.g., batch size and training epochs, the same as in Section 4. Table 15 shows that AdaLoRA incurs 11% additional training time on MNLI and 16% on SQuADv2 under different budgets. The memory footprint of two methods are quite close. Such results demonstrate that AdaLoRA does not incur significant training overheads. The reason behind is that 16Published as a conference paper at ICLR 2023 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 100 ||P⊤P −I||2 F (a) P of Wo at the first layer. 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 100 ||QQ⊤ −I||2 F (b) Q of Wo at the first layer. 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 100 ||P⊤P −I||2 F (c) P of Wf2 at the first layer. 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 ||QQ⊤ −I||2 F (d) Q of Wf2 at the first layer Figure 4: We plot the ∥P⊤P − I∥2 F and ∥QQ⊤ − I∥2 F when fine-tuning DeBERTaV3-base on SST-2. Table 15: Comparison of practical training cost between AdaLoRA and LoRA. Dataset # Param Method GPU Mem Time/epoch MNLI 0.08% LoRA 11.094 GB 105 min AdaLoRA 11.104 GB 116 min 0.16% LoRA 11.098 GB 105 min AdaLoRA 11.110 GB 117 min 0.65% LoRA 11.128 GB 105 min AdaLoRA 11.188 GB 117 min SST-2 0.08% LoRA 13.138 GB 60 min AdaLoRA 13.148 GB 71 min 0.16% LoRA 13.142 GB 61 min AdaLoRA 13.164 GB 71 min 0.65% LoRA 13.170 GB 61 min AdaLoRA 13.226 GB 71 min we only evaluate the importance score for small incremental matrices PΛQ. Their total number of parameters is usually less than 1% of pre-trained weights. Therefore, it does not lead to significant computational cost to update the importance scores of these well-structured small matrices, compared to forward-backward pass of full model. 17",
      "meta_data": {
        "arxiv_id": "2303.10512v2",
        "authors": [
          "Qingru Zhang",
          "Minshuo Chen",
          "Alexander Bukharin",
          "Nikos Karampatziakis",
          "Pengcheng He",
          "Yu Cheng",
          "Weizhu Chen",
          "Tuo Zhao"
        ],
        "published_date": "2023-03-18T22:36:25Z",
        "pdf_url": "https://arxiv.org/pdf/2303.10512v2.pdf",
        "github_url": "https://github.com/microsoft/LoRA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the suboptimal performance of parameter-efficient fine-tuning methods (like LoRA) that evenly distribute the budget of incremental updates across all pre-trained weight matrices, overlooking their varying importance. It proposes AdaLoRA (Adaptive Low-Rank Adaptation), a method that adaptively allocates the parameter budget among weight matrices according to their importance score. AdaLoRA parameterizes incremental updates using singular value decomposition (SVD) and prunes singular values of unimportant updates to reduce parameter budget, circumventing intensive exact SVD computations. It demonstrates notable improvements over baselines, especially in low budget settings, across NLP, QA, and NLG tasks.",
        "methodology": "AdaLoRA consists of two main components: (i) SVD-based adaptation and (ii) Importance-aware rank allocation. For SVD-based adaptation, incremental updates (Delta) are parameterized as Delta = PΛQ, where P and Q are orthogonal matrices representing singular vectors, and Λ is a diagonal matrix of singular values. An orthogonality regularizer R(P, Q) = ||P^T P - I||_F^2 + ||QQ^T - I||_F^2 is added to the training loss to enforce orthogonality and avoid intensive SVD computations. For importance-aware rank allocation, singular values are iteratively pruned based on importance scores. A novel importance metric Sk,i is proposed, combining the smoothed sensitivity and uncertainty of each singular value (λk,i) and its corresponding singular vectors (Pk,∗i, Qk,i∗) within a triplet Gk,i. The sensitivity is defined by the magnitude of the gradient-weight product, smoothed by an exponential moving average, and multiplied by an uncertainty term. A global budget scheduler gradually reduces the total rank (b(t)) from an initial higher budget to a target budget using a cubic schedule, allowing initial exploration and later focus on important weights.",
        "experimental_setup": "AdaLoRA's effectiveness is evaluated on DeBERTaV3-base for Natural Language Understanding (GLUE benchmark including MNLI, SST-2, CoLA, QQP, QNLI, RTE, MRPC, STS-B) and Question Answering (SQuADv1.1, SQuADv2.0). It is also applied to BART-large for Natural Language Generation (XSum, CNN/DailyMail). Evaluation metrics include Accuracy, F1, EM, and ROUGE 1/2/L scores. Experiments are conducted across different parameter budget levels (e.g., 0.08% to 2.20% of total pre-trained parameters). Baselines for comparison include Full fine-tuning, BitFit, Houlsby adapter, Pfeiffer adapter, and LoRA (generalized to apply to all weight matrices). Implementations are in PyTorch, based on Huggingface Transformers, using NVIDIA V100 GPUs. Hyperparameters like learning rate, batch size, epochs, regularization coefficient (γ), and budget schedule parameters (ti, ∆T, tf) are tuned per dataset.",
        "limitations": "The paper notes that AdaLoRA incurs additional training time compared to LoRA, specifically 11% on MNLI and 16% on SQuADv2 under different budgets, although it argues this overhead is not significant because importance scoring is performed only for small incremental matrices. Another potential limitation is the need to tune several hyperparameters related to the budget scheduler and regularization coefficient (γ, β1, β2, ti, ∆T, tf). The comparison with structured pruning of LoRA suggests that if crucial doublets are pruned by mistake and zeroed out entirely, it can hinder reactivation and cause training instability, implying a robustness advantage for AdaLoRA's approach of masking singular values while maintaining singular vectors, but also highlighting the criticality of the importance scoring and pruning mechanism.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "self.c_attn = lora.MergedLinear(nx, n_state * 3, r=config.lora_attn_dim, lora_alpha=config.lora_attn_alpha, lora_dropout=config.lora_dropout, enable_lora=[True, False, True], fan_in_fan_out=True, merge_weights=False) # From examples/NLG/src/model.py\nif args.lora_dim > 0: lora.mark_only_lora_as_trainable(lm_net) # From examples/NLG/src/gpt2_ft.py",
        "experimental_info": "Experimental settings related to SVD-based adaptation (LoRA implementation) are:\n- lora_attn_dim: LoRA attention dimension. (Default: 0 in GPT2Config, configurable via --lora_dim argument in gpt2_ft.py and gpt2_beam.py)\n- lora_attn_alpha: LoRA attention alpha. (Default: 128 in GPT2Config, configurable via --lora_alpha argument in gpt2_ft.py and gpt2_beam.py)\n- lora_dropout: Dropout probability for LoRA layers. (Default: 0.0 in GPT2Config, configurable via --lora_dropout argument in gpt2_ft.py)\n\nThe provided repository content implements the SVD-based adaptation component of AdaLoRA through the use of the loralib library. However, it does not contain explicit implementations of AdaLoRA's importance-aware rank allocation, singular value pruning based on importance scores (Sk,i), an orthogonality regularizer R(P, Q) in the loss, or a global budget scheduler with a cubic schedule, which are key parts of the described method."
      }
    },
    {
      "title": "Parameter-Efficient Fine-Tuning Design Spaces",
      "abstract": "Parameter-efficient fine-tuning aims to achieve performance comparable to\nfine-tuning, using fewer trainable parameters. Several strategies (e.g.,\nAdapters, prefix tuning, BitFit, and LoRA) have been proposed. However, their\ndesigns are hand-crafted separately, and it remains unclear whether certain\ndesign patterns exist for parameter-efficient fine-tuning. Thus, we present a\nparameter-efficient fine-tuning design paradigm and discover design patterns\nthat are applicable to different experimental settings. Instead of focusing on\ndesigning another individual tuning strategy, we introduce parameter-efficient\nfine-tuning design spaces that parameterize tuning structures and tuning\nstrategies. Specifically, any design space is characterized by four components:\nlayer grouping, trainable parameter allocation, tunable groups, and strategy\nassignment. Starting from an initial design space, we progressively refine the\nspace based on the model quality of each design choice and make greedy\nselection at each stage over these four components. We discover the following\ndesign patterns: (i) group layers in a spindle pattern; (ii) allocate the\nnumber of trainable parameters to layers uniformly; (iii) tune all the groups;\n(iv) assign proper tuning strategies to different groups. These design patterns\nresult in new parameter-efficient fine-tuning methods. We show experimentally\nthat these methods consistently and significantly outperform investigated\nparameter-efficient fine-tuning strategies across different backbone models and\ndifferent tasks in natural language processing.",
      "full_text": "PARAMETER -EFFICIENT FINE -TUNING DESIGN SPACES Jiaao Chen†∗, Aston Zhang‡, Xingjian Shi‡, Mu Li‡, Alex Smola‡, Diyi Yang⋄ †Georgia Institute of Technology,‡Amazon Web Services, ⋄Stanford University ABSTRACT Parameter-efﬁcient ﬁne-tuning aims to achieve performance comparable to ﬁne-tuning, using fewer trainable parameters. Several strategies (e.g., Adapters, preﬁx tuning, BitFit, and LoRA) have been proposed. However, their designs are hand-crafted separately, and it remains unclear whether cer- tain design patterns exist for parameter-efﬁcient ﬁne-tuning. Thus, we present a parameter-efﬁcient ﬁne-tuning design paradigm and discover design patterns that are applicable to different experi- mental settings. Instead of focusing on designing another individual tuning strategy, we introduce parameter-efﬁcient ﬁne-tuning design spaces that parameterize tuning structures and tuning strate- gies. Speciﬁcally, any design space is characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from an initial design space, we progressively reﬁne the space based on the model quality of each design choice and make greedy selection at each stage over these four components. We discover the following design patterns: (i) group layers in a spindle pattern; (ii) allocate the number of trainable parameters to layers uni- formly; (iii) tune all the groups; (iv) assign proper tuning strategies to different groups. These design patterns result in new parameter-efﬁcient ﬁne-tuning methods. We show experimentally that these methods consistently and signiﬁcantly outperform investigated parameter-efﬁcient ﬁne-tuning strategies across different backbone models and different tasks in natural language processing1. 1 Introduction Large pretrained models have achieved the state-of-the-art performances across a wide variety of downstream natural language processing tasks through ﬁne-tuning on task-speciﬁc labeled data [Devlin et al., 2019, Liu et al., 2019, Yang et al., 2019, Joshi et al., 2019, Sun et al., 2019, Clark et al., 2019, Lewis et al., 2020a, Bao et al., 2020, He et al., 2020, Raffel et al., 2020, Ziems et al., 2022]. However, ﬁne-tuning all the parameters and storing them separately for different tasks is expensive in terms of computation and storage overhead (e.g., 355M parameters for RoBERTa [Liu et al., 2019] and 175B parameters for GPT- 3 [Brown et al., 2020]). This makes it difﬁcult to deploy in real-world natural language processing (NLP) systems composed of multiple tasks. To adapt general knowledge in pretrained models to speciﬁc down-stream tasks in a more parameter-efﬁcient way, various strategies have been proposed where only a small number of (extra) parameters are learned while the remaining pretrained parameters are frozen [Houlsby et al., 2019a, Pfeiffer et al., 2021, Li and Liang, 2021, Brown et al., 2020, Lester et al., 2021a, Schick and Sch ¨utze, 2021, Ziems et al., 2022]. Adapter tuning [Houlsby et al., 2019a] is among the earliest strategies to steer pretrained models with a limited number of parameters. It inserts adapters (small neural modules) to each layer of the pretrained network and only the adapters are trained at the ﬁne-tuning time. Inspired by the success of prompting methods that control pretrained language models through textual prompts [Brown et al., 2020], preﬁx tuning [Li and Liang, 2021] and prompt tuning [Lester et al., 2021b] prepend additional tunable tokens to the input or hidden layers and only train these soft prompts when ﬁne-tuning on downstream tasks. BitFit [Zaken et al., 2021] updates the bias terms in pretrained models while freezing the remaining parameters. LoRA [Hu et al., 2021] decomposes attention weight gradients into low-rank matrices to reduce the number of trainable parameters. With promising results from such research, He et al. [2022] proposed a uniﬁed view of these existing strategies and ∗Work done during an internship at Amazon Web Services. Correspondence to Jiaao Chen<jiaaochen@gatech.edu> and Aston Zhang <astonz@amazon.com>. 1Code is available at: https://github.com/amazon-science/peft-design-spaces . arXiv:2301.01821v1  [cs.CL]  4 Jan 2023P P P L P L A B L A B L… Layer Grouping P L Strategy Assignment Trainable Parameter Allocation Tunable Groups p ⇥ p Figure 1: A parameter-efﬁcient ﬁne-tuning design space. It is characterized by (i) layer grouping (how to group consecutive layers), (ii) trainable parameter allocation (how to allocate the number of trainable parameters to layers), (iii) tunable groups (which groups will be ﬁnetuned), and (iv) strategy assignment (how to assign proper strategies, such as among Adapter, Preﬁx, BitFit, and LoRA, to groups). illustrated differences and connections among them. Like its antecedents, the resulting method is stillequally assigned to different pretrained layers. Despite being effective, most parameter-efﬁcient ﬁne-tuning strategies have been developed via manual design pro- cesses, without much consideration of whether design patterns exist across these different strategies and how such patterns might apply to different backbone models and downstream tasks. Moreover, different strategies are usually applied separately; thus, it is unclear which strategy works best when and where [Mao et al., 2022], as well as how these different strategies reinforce or complement each other. In this light, our goal is to understand the parameter- efﬁcient ﬁne-tuning design in a more comprehensive view and discover design patterns that are both interpretable and applicable across different experimental settings. Instead of designing yet another individual strategy that is equally applied to different pretrained layers, we introduce parameter-efﬁcient ﬁne-tuning design spaces that parameterize both tuning structures and strategies. More con- cretely, any of these design spaces is characterized by four major components as shown in Figure 1: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from a relatively unconstrained parameter-efﬁcient ﬁne-tuning design space, we progressively reﬁne the space by comparing the overall quality of models randomly sampled from design spaces enforced with different constraints (e.g., each group has the same number of layers). Throughout the experimental process, we discover several design patterns for parameter-efﬁcient ﬁne-tuning, such as group layers in a spindle pattern, allocate the number of trainable parameters to layers uniformly, tune all the groups, and assign proper tuning strategies to different groups. We fur- ther introduce new parameter-efﬁcient ﬁne-tuning methods that adopt all these discovered design patterns. Extensive experiments show that our methods consistently outperform investigated parameter-efﬁcient ﬁne-tuning strategies. Al- though we use T5 [Raffel et al., 2020] and classiﬁcation tasks as the working example, we ﬁnd that our methods with all these discovered design patters are applicable to other backbones (e.g., RoBERTa [Liu et al., 2019], BART [Lewis et al., 2020b], and XLNet [Yang et al., 2019]) and different natural language processing tasks (e.g., summarization, machine translation, and eight SuperGLUE datasets). Our contributions can be summarized as follows: (i) We introduce parameter-efﬁcient ﬁne-tuning design spaces. (ii) Based on these design spaces, we discover several design patterns in parameter-efﬁcient ﬁne-tuning via comprehen- sive experiments. (iii) Our discovered design patterns lead to parameter-efﬁcient ﬁne-tuning methods, consistently outperforming investigated parameter-efﬁcient ﬁne-tuning strategies across different backbone models and different NLP tasks. 22 Related Work Our work is closely related to and built upon the research about the network design spaces and parameter-efﬁcient ﬁne-tuning. We discuss the connections and differences below. Network Design Spaces A lot of works designed neural network models via an ad-hoc discovery of new design choices that improve performances [Radosavovic et al., 2019], such as the use of deeper architectures or residuals. Recently, there have been works [Radosavovic et al., 2020, You et al., 2020, Radosavovic et al., 2019] performing at the design space level to discover new design principles for convolutional neural networks [Radosavovic et al., 2020] and graph neural networks [You et al., 2020]. Inspired by this line of research, we focus on the design space perspective to rethink parameter-efﬁcient ﬁne-tuning, with the goal of discovering design patterns that are applicable to different experimental settings. Parameter-Efﬁcient Fine-Tuning for NLP As pretrained models grow in size, storing ﬁne-tuned models becomes exceedingly expensive, and ﬁne-tuning becomes infeasible for those without extremely high compute resources. A growing body of research has been devoted to ﬁnding parameter-efﬁcient alternatives for adapting large-scale pre- trained models with reduced memory and storage costs. Houlsby et al. [2019b] proposed to adapt large models using bottleneck layers (with skip-connections) between each layer. This idea has been extended in many domains [Stick- land and Murray, 2019, Pfeiffer et al., 2020, Rebufﬁ et al., 2017, Lin et al., 2020]. Other works have aimed to avoid introducing additional parameters by identifying and training only a subset of all model parameters [Zhao et al., 2020, Guo et al., 2020, Mallya et al., 2018, Radiya-Dixit and Wang, 2020, Sung et al., 2021, Zaken et al., 2021]. Recent works also explored the idea of rank decomposition based on parameterized hypercomplex multiplications via the Kro- necker product [Zhang et al., 2021a] and injecting trainable rank decomposition matrices into each layer [Hu et al., 2021, Karimi Mahabadi et al., 2021]. Li and Liang [2021] introduced preﬁx-tuning that prepends a set of preﬁxes to autoregressive language models or prepends preﬁxes for both encoders and decoders. The preﬁx parameters are updated while the pretrained parameters are ﬁxed. Lester et al. [2021a] proposed a similar method, but only added virtual tokens at the embedding layer of large-scale models rather than discrete prompts [Deng et al., 2022, Zhong et al., 2022]. Bari et al. [2022] proposed semi-parametric prompt tuning that converges more easily, where memory prompts are input-adaptive without the need for tuning. Recently, He et al. [2022] and Ding et al. [2022] proposed a uniﬁed view of the existing parameter-efﬁcient ﬁne-tuning strategies and illustrated the difference and connections among them. Mao et al. [2022] also introduced a uniﬁed framework to combine different methods through mixture- of-experts. In contrast to these aforementioned works that assign their individual method equally to different pretrained layers, we focus on more general design spaces of parameter-efﬁcient ﬁne-tuning. This could provide a more comprehensive view of parameter-efﬁcient ﬁne-tuning in terms of both the tuning structures and tuning strategies. Through experiments where we progressively reﬁne design spaces, we discover design patterns for parameter-efﬁcient ﬁne-tuning. 3 Components of Design Spaces When deﬁning design spaces of parameter-efﬁcient ﬁne-tuning, we aim to cover key design components and provide a representative set of choices in each design component. Note that our goal is not to enumerate all possible design spaces, but to demonstrate how the use of design spaces can help inform parameter-efﬁcient ﬁne-tuning research. Concretely, in our work, the parameter-efﬁcient ﬁne-tuning design spaces are formed by a representative set of choices in parameter-efﬁcient ﬁne-tuning, which consists of the following four components: (i) layer grouping, (ii) trainable parameter allocation, (iii) tunable groups, and (iv) strategy assignment. Following the illustrated design space exam- ple in Figure 1, we describe these four design components in detail below and will explore their design choices in Section 4. Layer Grouping Different layers in pretrained models capture different information and behave differently. For example, Jawahar et al. [2019] found that the {3, 4, 5, 6, 7, 9, 12}-th layers have the most representation power in BERT and every layer captures a different type of information ranging from the surface, syntactic, to the semantic level representation of text. For instance, the 9th layer has predictive power for semantic tasks such as checking random swapping of coordinated clausal conjuncts, while the 3rd layer performs best in surface tasks like predicting sentence length. Therefore when adapting these pretrained models to downstream tasks, how to group layers with similar behaviors together is critical to the design and application of proper parameter-efﬁcient ﬁne-tuning strategies. For this design component, we study the patterns of how to group consecutive layers in pretrained models (e.g., transformer layers in T5) during the ﬁne-tuning process. 3Trainable Parameter Allocation In parameter-efﬁcient ﬁne-tuning, the total number of trainable parameters is usually preset, such as a small portion of the total number of parameters in the pretrained models. We will study different design choices for how to allocate a predeﬁned number of trainable parameters to layers. Tunable Groups Zaken et al. [2021] found that not all the parameters need to be tuned during ﬁne-tuning on the downstream tasks. For instance, BitFit [Zaken et al., 2021] only updates the bias parameters in pretrained models while freezing the remaining parameters. Thus, we study which groups need to be learned during parameter-efﬁcient ﬁne-tuning to attain better performances. Strategy Assignment In order to improve the parameter efﬁciency, different sets of strategies [Li and Liang, 2021, Lester et al., 2021a, Houlsby et al., 2019a, Hu et al., 2021] have been proposed where only a small number of (ex- tra) parameters are tuned and the remaining parameters in these pretrained models are frozen to adapt their general knowledge to speciﬁc down-stream tasks. Inspired by effectiveness of offering architectural ﬂexibility [Zhang et al., 2021a,b], we hypothesize that different groups might beneﬁt from different proper strategies (or combinations) for capturing different types of information. More formally, given a set of individual strategies Afor assignment, for any group Gi, assign a subset Ui ⊂A to each layer in Gi. 4 Discovering Design Patterns Building on these four different design components of PEFT design spaces, we will start from a relatively uncon- strained design space and progressively discover the design patterns. 4.1 Design Space Experimental Setup We ﬁrst describe our experimental setup for discovering the design patterns. Note that our process is generic for other tasks and future pretrained backbone models. Datasets Our process for discovering design patterns of PEFT is based on the average performances on the widely- used GLUE benchmark [Wang et al., 2018]. It covers a wide range of natural language understanding tasks. First, single-sentence tasks include (i) Stanford Sentiment Treebank (SST-2) and (ii) Corpus of Linguistic Acceptability (CoLA). Second, similarity and paraphrase tasks include (i) Quora Question Pairs (QQP), (ii) Semantic Textual Sim- ilarity Benchmark (STS-B), and (iii) Microsoft Research Paraphrase Corpus (MRPC). Third, inference tasks include (i) Multi-Genre Natural Language Inference (MNLI), (ii) Question Natural Language Inference (QNLI), and (iii) Rec- ognizing Textual Entailment (RTE). To compare performances, the Matthews correlation is measured for CoLA; the Spearman correlation is used for STS-B, and accuracy is measured for the rest GLUE tasks. Pretrained Backbone Models and Model Settings We use T5-base/3b [Raffel et al., 2020] as the main pretrained backbone models for discovering design patterns via our PEFT design spaces. We use Hugging Face 2 for our imple- mentations and follow the default settings. During the exploration, we set the total number of trainable parameters (in the percentage of that in the backbone model) to 0.5% by following He et al. [2022]. 4.2 Discovering Design Patterns Using T5-base In this subsection, we describe the empirical process for discovering the design patterns using T5-base (pretrained backbone model) as the working example. Each PEFT design space (denoted as Si) consists of a set of models ( Si- models) that satisfy constraints characterizing the space with respect to layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. To discover design patterns, we start from a relatively unconstrained PEFT design space ( S0). Then we progressively reﬁne design spaces (from S0 to S1:4) by comparing overall quality of models in design spaces enforced with different constraints (e.g., each group has the same number of layers). To quantify the overall quality of models in any design space Si with a low-compute, low-epoch regime [Radosavovic et al., 2020], we randomly sample 100 models from Si, ﬁne-tune with 3 epochs 3, and compute the average of the GLUE average performances. 2https://huggingface.co/docs/transformers/index 3We set the low epoch by observing whether it is enough for models to obtain stable performances to draw consistent conclusions (See Table 7 in the Appendix). 4We emphasize that our goal is to demonstrate how the perspective of design spaces can help inform PEFT research, rather than to ﬁnd out the “best” design space or method. For computational efﬁciency, it is beyond the scope of this work to enumerate all possible constraints with respect to the design space components (Section 3). 4.2.1 The Initial S0 Design Space The initial relatively unconstrained design space S0 consists of all models without constraints on the design space components (Section 3). Individual PEFT strategies consist of Adapter, Preﬁx, BitFit, and LoRA. One can think of this S0 design space as a set of random models ( S0-models) with random design patterns. Speciﬁcally, without grouping constraints, each layer of the pretrained layer has a half chance to be tuned: if tuned, random strategies (or combinations) with a random amount of trainable parameters are assigned to that layer. Before comparing more subtle design patterns such as how to properly assign tunable strategies among Adapter, Preﬁx, BitFit, and LoRA, we begin with exploring how to group layers and how to allocate the total number of trainable parameters to layers. 4.2.2 The S1 Design Space with Additional Grouping Constraints Inspired by Radosavovic et al. [2020], we also consider 4 groups (G1, . . . , G4, in the order of forward pass) in the experiments 4. Denote by Ni the number of layers in Gi. As illustrated in Figure 2, we compare the following layer grouping patterns: (i) Increasing (Ni+1 > Ni): the number of layers in groups gradually increases; (ii) Uniform (Ni+1 = Ni): the number of layers in groups is the same; (iii) Decreasing (Ni+1 < Ni): the number of layers in groups gradually decreases; (iv) Spindle (N1 < N2 = N3 > N4): the numbers of layers in groups at both ends are smaller; and (v) Bottleneck (N1 > N2 = N3 < N4): the numbers of layers in groups at both ends are bigger. Figure 2: Layer grouping patterns, where the horizontal and vertical axes represent groups (G1, . . . , G4) and numbers of layers in groups. These layer grouping patterns lead to 5 different design spaces. Any of these 5 design spaces consists of all models in the S0 design space that satisfy one of these grouping pattern constraints. To compare the overall model qualities of different design spaces, we (i) randomly sample 100 models from the S0 design space that satisfy each grouping pattern constraint (Figure 2); (ii) ﬁne-tune with 3 epochs; and (iii) compute the average performances for each design space. We will follow this procedure as we progressively add new constraints later. The averaged performances are shown in Table 1 5. We ﬁnd that models from the design space with the spindle grouping pattern (Figure 2) consistently outperform those from the other design spaces across all the 8 GLUE tasks. This may be due to the complexities of information captured in different layers of large pretrained models, which favor information adaptation in the discovered layer grouping pattern. From now on, we will group layers in a spindle pattern. We refer to S0 with this additional design pattern as the new S1 design space. 4.2.3 The S2 Design Space with Additional Parameter Constraints We continue to explore design patterns in trainable parameter allocation to reﬁne the S1 design space. Denote by ni the number of trainable parameters for the i-th layer of the pretrained backbone model, we compare the following design patterns: (i) Increasing (ni+1 ≥ni): the number of trainable parameters in every layer gradually increases (or remains the same); (ii) Uniform (ni+1 = ni): the number of trainable parameters in every layer is the same; and (iii) Decreasing (ni+1 ≤ni): the number of trainable parameters in every layer gradually decreases (or remains the same). Following the procedure described in Section 4.2.2, we obtain 100 models for each of these 3 new design spaces. Table 2 reports the average performances of these 3 design spaces. The uniform allocation design pattern obtains the highest GLUE average performance, making this relatively simple, interpretable design pattern favorable. 4The experimental results with 8 groups are shown in the Table 16 in the Appendix. 5The training time for the step is shown in the Table 18 in the Appendix. 5Table 1: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 76.9 70.1 72.5 73.3 63.6 71.7 73.8 24.3 65.7 Increasing 85.3 74.9 77.2 77.5 66.8 76.2 76.0 33.0 70.8 Uniform 84.8 73.7 78.1 78.6 68.5 77.8 79.2 36.1 72.1 Decreasing 81.9 72.1 78.3 76.7 67.3 75.9 78.6 28.7 70.0 Spindle 86.9 75.5 79.8 79.4 69.8 78.3 80.1 37.3 73.3 Bottleneck 84.5 74.6 76.9 78.1 69.2 76.2 78.6 32.1 71.3 Table 2: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different parameter allocation constraints to the S1 design space. Param Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg Increasing 87.2 77.9 79.4 78.7 71.6 77.6 81.4 32.0 73.2 Uniform 87.8 77.4 80.1 80.5 73.9 78.1 80.4 34.3 74.0 Decreasing 86.4 75.8 78.4 77.0 70.4 77.1 78.7 35.8 72.4 We will allocate the number of trainable parameters to layers uniformly. We refer to S1 with this additional design pattern as the new S2 design space. 4.2.4 The S3 Design Space with Additional Tunable Group Constraints Before digging into the strategy assignment design patterns, it is necessary to examine which groups need to be tuned. After all, it is only meaningful to study assigning strategies to different groups after we ﬁnd out which groups need to be ﬁne-tuned. As shown in Table 3, we explore various design patterns in tunable groups to further constrain the S2 design space. Based on the GLUE average performances, we ﬁnd that all the groups need to be tuned to obtain the best performances. This suggests that all the groups of pretrained layers have captured useful information that should be adapted to the downstream tasks. We will tune all the groups. We refer to S2 with this additional design pattern as the new S3 design space. 4.2.5 The S4 Design Space with Additional Strategy Constraints Finally, we study the subtle design pattern with respect to assigning proper strategies by further constraining the derived S3 design space. Speciﬁcally, each design space consists of models that assign a subset of {Adapter (A), Preﬁx (P), BitFit (B), and LoRA (L) }to all layers of any group Gi (i = 1, . . . ,4). We begin by adding different G1 strategy assignment constraints to the S3 space. Following the same pattern discovery procedure (Section 4.2.2), we discover strategy assignment patterns for G1. Then we progressively add Gi (i >1) strategy assignment constraints together with the discovered strategy assignment patterns for all Gj (j = 1, . . . , i−1) to the S3 space. Due to space limit, we present results of this process in the Appendix ( G1 in Table 8, G2 Table 9, G3 in Table 10, and G4 in Table 11), which suggests strategy assignment ofG1-(A, L) – G2-(A, P) – G3-(A, P, B) –G4-(P, B, L) for the T5-base pretrained backbone model. We will assign the discovered proper tuning strategies to groups.We refer to S3 with this additional design pattern as the new S4 design space, which consists of the ﬁnal S4-model. 4.3 Discovering Design Patterns Using T5-3b We then repeat the above process on T5-3b to examine if the design patterns we discovered using smaller models (T5- base) still apply when we use larger models. The results are shown in Table 12 (layer grouping), Table 13 (trainable parameter allocation), Table 14 (tunable groups) and Table 15 (strategy assignment) in the Appendix. We observe that the design patterns still apply when larger models like T5-3b are used: (i) grouping layers in a spindle pattern (Table 12), (ii) uniformly allocating the number of trainable parameters to layers (Table 13), (iii) tuning all the groups 6Table 3: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different tunable group constraints to the S2 design space. Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1 82.6 72.1 77.6 70.6 65.3 71.9 77.6 27.6 68.2 G2 83.3 72.8 77.5 72.8 63.6 72.8 77.5 27.5 68.4 G3 83.6 73.3 78.2 73.3 66.4 71.3 77.9 22.9 68.4 G4 83.2 73.0 77.9 73.7 63.9 72.0 77.9 27.9 68.7 G1, G2 83.5 73.2 78.0 75.4 67.7 73.2 78.0 28.0 69.6 G3, G4 87.8 74.6 78.3 76.9 68.6 74.3 78.3 28.3 70.7 G1, G2, G3 86.0 75.8 79.0 77.8 71.8 78.8 79.0 33.0 72.6 G2, G3, G4 85.2 76.6 79.1 78.6 70.1 77.6 79.1 31.9 72.2 G1,G2,G3,G4 88.3 77.4 82.1 81.5 74.9 79.4 81.4 34.3 74.9 Table 4: Performances of different tuning methods on the GLUE datasets using the T5-base (upper part) and T5-3b (lower part) pretrained backbone models, respectively. The results are averaged over 20 random runs (with standard deviations as subscripts). The S4-model and the S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05(∗) or even p <0.01(∗∗). Method SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Average full 95.2 87.1 93.7 89.4 80.1 89.4 90.7 51.1 84.5 Adapter 94.6 85.5 89.8 86.7 75.3 86.7 89.1 59.2 83.3 Preﬁx 94.0 81.6 87.8 83.4 64.3 83.1 84.8 34.0 76.6 BitFit 94.4 84.5 90.6 88.3 74.3 86.6 90.1 57.7 83.3 LoRA 94.8 84.7 91.6 88.5 75.8 86.3 88.7 51.5 82.7 S4-model 95.5∗∗ 1.7 87.6∗∗ 1.0 92.7∗∗ 1.1 88.8∗∗ 1.0 80.4∗ 2.3 87.4∗ 2.0 91.2∗∗ 2.4 62.2∗ 3.2 85.7 full 97.4 91.4 96.3 89.7 91.1 90.6 92.5 67.1 89.5 Adapter 96.3 89.9 94.7 87.8 83.4 90 89.7 65.2 87.1 Preﬁx 96.3 82.8 88.9 85.5 78.3 83.5 85.4 42.7 80.4 BitFit 95.8 89.5 93.5 88.5 86.2 90.7 88.6 64.2 87.1 LoRA 96.2 90.6 94.9 89.1 91.2 91.1 91.1 67.4 88.9 S4-3b-model 97.2∗∗ 1.8 91.6∗∗ 1.2 96.6∗∗ 1.0 89.5∗∗ 1.5 91.5∗ 2.8 91.5∗ 2.5 91.9∗ 2.0 69.7∗ 3.4 89.9 (Table 14), and (iv) tuning different groups with proper strategies (Table 15). For T5-3b, the discovered proper strategy assignment is G1-(P, L) –G2-(A, L) – G3-(P, B, L) –G4-(A, P, B). We refer to the ﬁnal design space asS4-3b and the ﬁnal model in this space as S4-3b-model. 5 Evaluation The S4-model (Section 4.2.5) and S4-3b-model (Section 4.3) adopt all the design patterns that have been discovered by using T5-base and T5-3b, respectively. As a result, they are both new methods of PEFT. We will evaluate their effectiveness when applied to different pretrained backbone models and different NLP tasks. 5.1 Experimental Setup Datasets Besides the GLUE datasets [Wang et al., 2018] (Section 4.1), we further evaluate our methods on two generation tasks used by He et al. [2022]: (i) Abstractive Summarization using XSum [Narayan et al., 2018], and (ii) Machine Translation using the WMT 2016 en-ro dataset [Bojar et al., 2016]. We report ROUGE scores [Lin, 2004] on the XSum test set, and BLEU scores [Papineni et al., 2002] on the en-ro test set. Models and Model Settings We mainly compare our methods with the following baselines: (i) Full Fine-tuning (full): it ﬁne-tunes all the model parameters in the pretrained models; (ii) Adapter [Houlsby et al., 2019a]: it adds adapter modules to each transformer layer; (iii) Preﬁx [Li and Liang, 2021]: it optimizes a set of small continuous vectors prepended to transformer layers; (iv) BitFit [Zaken et al., 2021]: it only updates the bias terms in pretrained models; (v) LoRA [Hu et al., 2021]: it decomposes the attention weight into low-rank matrices to reduce the number of trainable parameters. Besides T5 [Raffel et al., 2020], we additionally apply our methods to other backbone models 7Table 5: Performances of different tuning methods on GLUE datasets using the RoBERTa-base (upper part) and RoBERTa-large (lower part) pretrained backbone models. The results are averaged over 20 random runs (with standard deviations as subscripts). Here we also include two baselines: (i) S0-model, where all the designs are randomly selected for RoBERTa as in the S0 design space; (ii) S3-model, where strategies are randomly assigned to different RoBERTa layer groups as in the S3 design space. The S4-model and S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05(∗) or even p <0.01(∗∗). Method SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Average full 94.8 87.6 92.8 91.9 80.8 90.3 90.2 63.6 86.5 Adapter 94.2 87.1 93.1 90.2 71.5 89.7 88.5 60.8 84.4 Preﬁx 94.0 86.8 91.3 90.5 74.5 90.3 88.2 61.5 84.6 BitFit 93.7 84.8 91.3 84.5 77.8 90.8 90.0 61.8 84.3 LoRA 94.9 87.5 93.1 90.8 83.1 90.0 89.6 62.6 86.4 S0-model 94.2 95.3 90.4 90.6 75.6 89.6 88.0 60.9 85.6 S3-model 94.3 87.2 92.8 91.0 81.8 90.3 89.2 63.2 86.2 S4-model 94.81.6 87.8∗∗ 0.8 93.4∗∗ 1.3 91.6∗ 1.2 85.8∗∗ 1.8 90.4∗ 2.0 90.0∗∗ 1.8 63.2∗ 3.5 87.1 full 96.4 90.2 94.7 92.2 86.6 92.4 90.9 68.0 88.9 Adapter 96.6 90.5 94.8 91.7 80.1 92.1 90.9 67.8 88.1 Preﬁx 95.7 87.6 92.1 88.7 82.3 89.6 87.4 62.8 85.7 BitFit 96.1 88.0 93.4 90.2 86.2 90.9 92.7 64.2 87.7 LoRA 96.2 90.6 94.7 91.6 87.4 92.0 89.7 68.2 88.8 S0-model 95.5 86.5 92.3 89.8 84.6 89.2 86.3 61.2 85.6 S3-model 96.3 89.4 93.8 90.2 85.9 90.8 90.9 63.4 87.6 S4-3b-model 96.6∗∗ 1.3 90.8∗ 1.1 95.1∗∗ 0.8 92.0∗∗ 1.2 87.22.8 92.3∗ 2.2 91.8∗∗ 1.8 68.4∗ 3.2 89.3 including RoBERTa-base/large [Liu et al., 2019] and BART-base/large [Lewis et al., 2020a]. We use the default settings. We set the total number of trainable parameters (in the percentage of that in the backbone model) by following He et al. [2022]. Speciﬁcally, this value is set to 0.5% for Adapter, Preﬁx, LoRA, and our methods, and 0.1% for BitFit. For all the experiments, we followed Liu et al. [2019] to set the linear decay scheduler with a warmup ratio of 0.06 for training. The batch size was 128 for base models and 64 for large models. The maximum learning rate was 5e −5 and the maximum number of training epochs was set to be either 5 or 10. All the experiments were performed using 8 A100 GPUs. 5.2 Effectiveness on GLUE with T5 Backbones Table 6: Performances of different tuning methods on generation tasks (XSUM and en-ro) using the BART-base (upper part) and BART-large (lower part) pretrained backbone models. Method XSUM(R-1/2/L) en-ro (BLEU) full 40.5/19.2/34.8 34.5 Adapter 37.7/17.9/33.1 33.3 Preﬁx 38.2/18.4/32.4 33.8 BitFit 37.2/17.5/31.4 33.2 LoRA 38.9/18.6/33.5 33.6 PA 39.3/18.7/33.8 33.8 S4-model 40.2/19.3/34.2 34.1 full 45.1/22.3/37.2 37.9 Adapter 43.8/20.8/35.7 35.3 Preﬁx 43.4/20.4/35.5 35.6 BitFit 42.8/18.7/33.2 35.2 LoRA 42.9/19.4/34.8 35.8 PA 43.9/20.6/35.6 36.4 S4-3b-model 44.3/21.7/36.8 37.2 With our discovered design patterns, we ﬁne-tune T5-base (S4-model) and T5-3b ( S4-3b-model) on GLUE and compare them with all the baseline methods. The results are shown in Table 4, where the key measure is the GLUE average performance (last column). We ﬁnd that our S4-model and S4- 3b-model consistently outperform the investigated methods in the key measure. By tuning only 0.5% parameters, our methods even outperform the full ﬁne-tuning baseline where all the parameters are tuned, indicating the effectiveness of our discov- ered PEFT design patterns. 5.3 General Effectiveness on GLUE with RoBERTa Backbones We directly apply the S4-model and S4-3b-model (adopting design patterns discovered using T5- base and T5-3b) to ﬁne-tune the RoBERTa-base and RoBERTa-large pretrained backbone models (with no extra discovery process), respectively. We keep all the other settings the same and evaluate them on GLUE datasets. We also compare with variant methods randomly sampled from two de- 8sign spaces: (i) S0-model, where all the designs are randomly selected for RoBERTa as in S0; (ii) S3-model, where strategies are randomly assigned to different RoBERTa layer groups as in S3. Table 5 shows that (i) the design pat- terns (adopted by S4-model and S4-3b-model) discovered using T5 models are applicable to the RoBERTa backbone models and outperform the investigated methods in GLUE average performances with no extra discovery process;(ii) improved performances fromS0-models, S3-models, to S4-(3b)-models support adding more constraints in the pattern discovery process (Section 4). 5.4 General Effectiveness on Generation Tasks with BART Backbones Like in Section 5.3, we further directly apply the S4-model and S4-3b-model (adopting design patterns discovered using T5-base and T5-3b) to ﬁne-tune the BART-base and BART-large pretrained backbone models (without additional discovery process.), respectively. We evaluate the models on two generation tasks: summarization (XSUM) and machine translation (en-ro) following He et al. [2022]. We also compare with PA (parallel adapter) using the same number of trainable parameters [He et al., 2022]. Table 6 shows that our methods, although adopting design patterns discovered from classiﬁcation tasks using T5, still outperform investigated PEFT strategies on generation tasks with different BART backbones. 6 Conclusion PEFT adapts knowledge in pretrained models to down-stream tasks in a more parameter-efﬁcient fashion. Instead of focusing on designing another strategy in the ﬁrst place, we introduced PEFT design spaces. We empirically discovered several design patterns in PEFT. These design patterns led to new PEFT methods. Experiments showed that these methods consistently outperform investigated PEFT strategies across different backbone models and different tasks in natural language processing. References Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, 2019. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. InAdvances in neural information processing systems, pages 5754–5764, 2019. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics , 8:64–77, 2019. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223, 2019. Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2019. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, transla- tion, and comprehension. SCL, 2020a. Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming Zhou, et al. Unilmv2: Pseudo-masked language models for uniﬁed language model pre-training. arXiv preprint arXiv:2002.12804, 2020. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2020. Caleb Ziems, Jiaao Chen, Camille Harris, Jessica Anderson, and Diyi Yang. V ALUE: Understanding dialect disparity in NLU. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 91: Long Papers) , pages 3701–3720, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.258. URL https://aclanthology.org/2022.acl-long.258. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , vol- ume 97 of Proceedings of Machine Learning Research , pages 2790–2799. PMLR, 09–15 Jun 2019a. URL http://proceedings.mlr.press/v97/houlsby19a.html. Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non- destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 487–503, Online, April 2021. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2021.eacl-main.39. Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation, 2021. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning, 2021a. Timo Schick and Hinrich Sch ¨utze. Exploiting cloze-questions for few-shot text classiﬁcation and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 255–269, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.20. URL https://aclanthology.org/2021.eacl-main.20. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 3045–3059, Online and Punta Cana, Dominican Republic, November 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243. Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning for transformer- based masked language-models, 2021. URL https://arxiv.org/abs/2106.10199. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a uniﬁed view of parameter-efﬁcient transfer learning. In International Conference on Learning Representations, 2022. Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. UniPELT: A uniﬁed framework for parameter-efﬁcient language model tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 6253–6264, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.433. URL https: //aclanthology.org/2022.acl-long.433. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy- anov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language genera- tion, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics , pages 7871–7880, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://www.aclweb.org/anthology/2020.acl-main.703. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Doll ´ar. On network design spaces for visual recognition, 2019. URL https://arxiv.org/abs/1905.13214. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Designing network design spaces, 2020. URL https://arxiv.org/abs/2003.13678. Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks, 2020. URL https://arxiv. org/abs/2011.08843. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019b. Asa Cooper Stickland and Iain Murray. Bert and pals: Projected attention layers for efﬁcient adaptation in multi-task learning. In International Conference on Machine Learning, pages 5986–5995. PMLR, 2019. 10Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non- destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. arXiv preprint arXiv:1705.08045, 2017. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter- efﬁcient transfer learning. arXiv preprint arXiv:2004.03829, 2020. Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Sch ¨utze. Masking as an efﬁcient alternative to ﬁnetuning for pretrained language models. arXiv preprint arXiv:2004.12406, 2020. Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efﬁcient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV), pages 67–82, 2018. Evani Radiya-Dixit and Xin Wang. How ﬁne can ﬁne-tuning be? learning efﬁcient language models. In International Conference on Artiﬁcial Intelligence and Statistics, pages 2435–2443. PMLR, 2020. Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with ﬁxed sparse masks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Pro- cessing Systems, volume 34, pages 24193–24205. Curran Associates, Inc., 2021. URL https://proceedings. neurips.cc/paper/2021/file/cb2653f548f8709598e8b5156738cc51-Paper.pdf. Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Hui, and Jie Fu. Beyond fully-connected lay- ers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters. In International Conference on Learning Representations, 2021a. Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022–1035, 2021. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning, 2022. URL https: //arxiv.org/abs/2205.12548. Wanjun Zhong, Yifan Gao, Ning Ding, Zhiyuan Liu, Ming Zhou, Jiahai Wang, Jian Yin, and Nan Duan. Improving task generalization via uniﬁed schema prompt, 2022. URL https://arxiv.org/abs/2208.03229. M Saiful Bari, Aston Zhang, Shuai Zheng, Xingjian Shi, Yi Zhu, Shaﬁq Joty, and Mu Li. Spt: Semi-parametric prompt tuning for multitask prompted learning. arXiv preprint arXiv:2212.10929, 2022. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. Delta tuning: A comprehensive study of parameter efﬁcient methods for pre-trained language models, 2022. URL https://arxiv.org/abs/2203.06904. Ganesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. What does BERT learn about the structure of language? In Pro- ceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651–3657, Florence, Italy, July 2019. Association for Computational Linguistics. Aston Zhang, Yi Tay, Yikang Shen, Alvin Chan, and Shuai Zhang. Self-instantiated recurrent units with dynamic soft recursion. Advances in Neural Information Processing Systems, 34:6503–6514, 2021b. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware convo- lutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206. Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Ji- meno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur ´elie N ´ev´eol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Mar- cos Zampieri. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131–198, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2301. URL https://aclanthology.org/W16-2301. 11Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002. 12A More Experimental Results Table 7: Average performances (low-compute, low-epoch regime: 100 random models, tuning epochs = 1, 2, 3, 4, 20 for ﬁve different blocks) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different grouping constraints to the S0 design space. Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg 1 epochs Increasing 73.2 63.3 67.8 68.8 63.8 67.2 64.1 11.0 59.9 Uniform 72.8 64.1 63.4 63.4 62.5 69.8 65.8 12.1 59.2 Decreasing 72.4 63.2 65.1 69.8 59.3 62.7 63.6 18.7 59.4 Spindle 72.6 64.8 66.8 71.1 62.1 62.3 64.8 12.3 59.6 Bottleneck 72.2 63.7 65.3 68.3 61.2 63.2 66.6 12.1 59.0 2 epochs Increasing 76.2 69.3 73.2 76.5 65.8 72.2 74.0 21.0 66.0 Uniform 74.8 70.9 74.1 75.6 66.5 73.4 71.2 22.1 66.1 Decreasing 71.4 70.1 72.1 76.8 64.3 71.7 73.6 18.7 64.8 Spindle 76.6 71.9 71.8 74.4 67.5 73.5 71.8 22.3 66.2 Bottleneck 74.2 71.1 69.6 73.3 65.2 73.3 73.6 24.1 65.5 3 epochs Increasing 85.3 74.9 77.2 77.5 66.8 76.2 76.0 33.0 70.8 Uniform 84.8 73.7 78.1 78.6 68.5 77.8 79.2 36.1 72.1 Decreasing 81.9 72.1 78.3 76.7 67.3 75.9 78.6 28.7 69.9 Spindle 86.9 75.5 79.8 79.4 69.8 78.3 80.1 47.3 74.6 Bottleneck 84.5 74.6 76.9 78.1 69.2 76.2 78.6 32.1 71.3 4 epochs Increasing 88.3 78.5 80.2 80.5 70.8 80.2 80.0 37.0 74.4 Uniform 88.8 78.9 81.9 81.5 71.5 80.8 81.4 39.1 75.4 Decreasing 87.6 74.1 80.8 81.7 79.3 78.9 79.6 38.7 75.1 Spindle 89.6 79.8 83.6 82.8 71.8 81.3 82.1 39.3 76.3 Bottleneck 86.5 77.6 82.7 81.1 70.2 70.9 81.6 36.1 73.3 20 epochs Increasing 92.3 83.3 86.2 82.5 71.8 82.2 84.0 51.0 79.1 Uniform 92.8 83.9 86.1 83.6 72.5 83.8 84.2 52.1 79.9 Decreasing 91.4 82.1 85.1 83.1 69.3 81.7 83.6 48.7 78.1 Spindle 93.6 84.8 87.8 84.4 73.5 84.3 85.8 52.3 80.8 Bottleneck 92.1 82.6 85.6 83.3 71.2 83.2 84.6 52.1 79.3 B General Effectiveness on SuperGLUE with XLNet Backbones We also directly use the S4-model and S4-3b-model (adopting design patterns discovered using T5-base and T5-3b) to ﬁne-tune the XLNet-base and XLNet-large pretrained backbone models without any extra discovery process. We keep all the other settings the same and evaluate them on SuperGLUE datasets. Table 17 reiterates the fact that our PEFT design patterns discovered from T5 models are generelizable to the XLNet backbone models and outperform the investigated methods in other tasks (SuperGLUE) with no additional discovery process. C On the Discovery Sequence In this work, we follow the discovery sequence of “grouping patterns – trainable parameter allocation – tunable groups – strategy assignment”: 1. To explore and understand the design patterns in all the layers in large pre-trained models in scale, it is necessary and more efﬁcient to study the layers in the unit of groups. So we start with the grouping patterns. 13Table 8: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G1 strategy assignment con- straints to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1-Adapter (A) 89.8 83.5 84.9 80.8 72.5 80.8 78.5 37.7 76.1 G1-Preﬁx (P) 89.3 83.1 84.4 80.1 70.1 80.0 77.6 33.0 74.7 G1-BitFit (B) 89.0 82.9 84.1 81.4 72.0 81.1 77.0 30.8 74.8 G1-LoRA (L) 89.9 83.6 85.0 81.1 71.8 81.0 78.8 35.3 75.8 G1-(P, L) 89.1 82.8 85.1 81.2 71.9 81.5 79.1 35.0 75.7 G1-(A, P) 89.8 82.8 84.8 81.1 72.2 81.3 79.2 36.4 75.9 G1-(A, L) 89.6 83.8 85.6 81.3 72.9 81.7 79.5 36.8 76.4 G1-(A, P, L) 89.6 83.5 85.2 81.5 72.2 81.4 79.2 35.2 75.9 G1-(P, B, L) 89.3 83.6 85.5 81.6 72.3 81.0 78.8 35.7 76.0 G1-(A, P, B) 89.2 83.3 84.8 81.8 72.5 81.1 78.6 35.6 75.8 G1-(A, B, L) 89.8 83.4 84.8 81.1 72.6 81.6 79.4 34.8 75.9 G1-(A, P, B, L) 90.0 83.1 85.3 81.6 72.6 81.4 79.2 36.5 76.1 Table 9: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G2 strategy assignment con- straints with G1-(L, A) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G2-Adapter (A) 91.6 84.3 85.5 82.3 73.5 82.8 81.3 38.8 77.5 G2-Preﬁx (P) 89.6 84.0 86.5 81.5 73.3 82.5 80.5 36.2 76.7 G2-BitFit (B) 91.2 83.6 85.7 82.9 72.6 82.6 80.8 33.1 76.5 G2-LoRA (L) 91.4 84.4 86.1 82.0 72.8 81.8 81.6 39.8 77.4 G2-(P, L) 91.6 84.6 86.8 81.8 73.8 82.8 82.0 38.5 77.7 G2-(A, P) 92.2 84.2 87.1 82.2 74.4 83.0 82.5 40.8 78.3 G2-(A, L) 92.0 84.4 86.5 81.8 73.6 82.6 82.2 40.1 77.9 G2-(A, P, L) 91.8 84.8 86.8 81.8 74.1 83.0 82.1 37.9 77.7 G2-(P, B, L) 91.6 84.1 87.1 82.0 74.0 82.9 82.4 35.8 77.4 G2-(A, P, B) 91.8 84.2 86.8 82.1 73.7 83.3 82.2 41.2 78.1 G2-(A, B, L) 92.2 84.3 86.1 82.0 74.1 83.2 82.0 37.6 77.6 G2-(A, P, B, L) 92.0 84.1 87.0 81.9 74.2 83.1 81.3 42.4 78.1 2. Once ﬁguring out the optimal grouping patterns, it is then important to explore how to allocate the trainable parameters to these different groups in order to study more subtle designs with fair comparisons (e.g., this would allow comparing different patterns of strategy assignments without the impact from different trainable parameters.). 3. Next, it becomes inﬂuential to examine which groups need to be learned during ﬁne-tuning before we dig into the strategy assignment patterns. Because it is only meaningful to study assigning strategies to different groups after we ﬁgure out which groups need to be learned. 4. Finally, we study the tuning strategy assignment, which is the most subtle design. 14Table 10: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G3 strategy assignment constraints with G1-(L, A) – G2-(P, A) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G3-Adapter (A) 92.5 85.3 87.5 83.3 73.9 84.0 83.8 44.9 79.4 G3-Preﬁx (P) 91.5 84.7 86.7 82.6 74.2 83.8 82.9 40.5 78.4 G3-BitFit (B) 91.9 84.3 87.0 82.0 73.6 84.1 83.3 36.1 77.8 G3-LoRA (L) 92.8 85.4 87.8 83.5 74.7 82.4 84.0 44.0 79.3 G3-(P, L) 93.0 85.2 88.3 83.8 75.2 84.4 84.2 37.9 79.0 G3-(A, P) 92.4 85.6 88.1 83.6 75.0 84.2 84.0 41.8 79.3 G3-(A, L) 92.0 85.9 88.2 83.1 75.3 84.3 83.9 42.2 79.4 G3-(A, P, L) 92.6 86.0 87.5 83.4 75.6 84.6 83.5 43.9 79.6 G3-(P, B, L) 92.7 85.8 87.2 83.7 75.2 84.5 83.8 40.8 79.2 G3-(A, P, B) 93.3 85.8 88.6 84.0 75.5 84.9 84.1 42.1 79.8 G3-(A, B, L) 93.7 86.5 88.0 83.2 75.8 84.2 84.2 39.7 79.4 G3-(A, P, B, L) 93.3 85.6 87.7 83.8 75.2 84.3 84.4 41.6 79.4 Table 11: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G4 strategy assignment constraints with G1-(A, L) – G2-(A, P) – G3-(A, P, B) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G4-Adapter (A) 93.8 85.8 88.6 84.8 76.3 85.8 86.0 48.5 81.2 G4-Preﬁx (P) 93.5 85.2 88.3 83.6 76.8 85.3 85.6 44.8 80.3 G4-BitFit (B) 94.1 85.3 88.9 84.4 77.1 85.4 86.2 46.1 80.9 G4-LoRA (L) 94.0 86.0 89.2 85.0 77.2 85.5 85.8 47.7 81.3 G4-(P, L) 94.3 86.2 89.3 85.8 78.0 86.0 88.2 47.2 81.8 G4-(A, P) 94.1 86.2 89.6 85.4 77.9 86.2 86.9 45.3 81.4 G4-(A, L) 94.2 85.9 89.2 85.5 77.8 86.2 88.0 46.8 81.7 G4-(A, P, L) 94.1 85.8 88.8 85.7 77.4 86.5 87.9 44.8 81.3 G4-(P, B, L) 94.6 86.4 90.4 86.1 78.2 86.8 88.5 47.2 82.3 G4-(A, P, B) 94.5 86.0 89.6 86.0 78.0 86.2 88.1 44.8 81.6 G4-(A, B, L) 94.3 86.4 89.2 85.6 78.2 86.4 88.3 46.6 81.9 G4-(A, P, B, L) 94.2 86.2 89.2 85.9 78.5 86.1 88.0 45.3 81.6 Table 12: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 80.3 72.1 74.7 72.8 76.9 75.2 71.0 32.2 69.4 Increasing 84.4 75.7 83.0 78.3 82.7 80.3 76.3 42.1 75.3 Uniform 86.8 77.1 82.6 76.2 83.8 81.6 77.3 48.9 76.8 Decreasing 83.2 74.3 81.8 77.3 82.8 79.9 76.5 40.8 74.5 Spindle 88.6 78.8 83.7 77.7 84.2 80.9 78.3 44.6 77.1 Bottleneck 86.3 77.0 82.2 75.6 83.3 80.2 77.1 41.5 75.4 15Table 13: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer parameter constraints to the S1 design space. Parameter Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg Increasing 90.3 79.3 84.9 79.3 85.2 82.8 79.2 50.1 78.9 Uniform 90.6 80.8 84.6 79.7 85.5 82.4 78.9 50.8 79.1 Decreasing 88.6 78.2 83.5 78.1 84.4 81.5 78.1 49.6 77.7 Table 14: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different tuning groups constraints to the S2 design space. Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1 88.3 78.3 82.2 77.4 82.1 80.7 76.1 49.4 76.8 G2 89.1 78.8 82.1 77.2 82.3 81.2 76.4 49.6 77.1 G3 89.6 78.5 82.6 78.1 83.8 81.9 77.4 48.7 77.5 G4 89.8 79.3 82.7 77.9 83.5 81.9 77.9 48.5 77.1 G1, G2 90.1 80.2 83.4 78.5 84.3 82.4 78.5 51.1 78.5 G3, G4 90.5 80.6 83.8 78.7 84.2 83 78.2 50.3 78.6 G1, G2, G3 90.6 80.3 84.9 79.3 84.7 82.9 79.3 50.2 79.0 G2, G3, G4 90.8 80.9 84.6 79.1 85.1 83.1 79.1 49.2 78.9 G1, G2, G3, G4 91.1 81.4 85.2 80.4 85.9 83.5 80.0 51.6 79.9 16Table 15: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different strategy assignment con- straints following the process in Section 4.2.5. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1-Adapter (A) 91.1 81.4 86.1 80.5 86.7 83.3 80.1 50.8 80.0 G1-Preﬁx (P) 90.8 81.1 85.5 80.2 86.2 83.1 79.8 50.2 79.6 G1-BitFit (B) 90.2 81.3 85.1 79.6 85.8 82.8 79.6 49.5 79.2 G1-LoRA (L) 91.4 81.9 86.2 80.8 86.4 83.9 80.8 49.6 80.0 G1-(P, L) 91.8 82.9 86.8 81.3 87.1 84.2 81.6 52.3 81.0 G1-(A, P) 91.3 81.9 86.4 81.1 85.6 83.7 80.7 52.8 80.1 G1-(A, L) 91.6 82.3 86.1 81.5 85.8 84.9 81.5 51.8 80.6 G1-(A, P, L) 91.1 81.7 85.8 81.2 86.4 84.2 80.9 52.3 80.4 G1-(P, B, L) 91.5 82.8 86.3 81.4 86.1 83.6 81.2 51.5 80.5 G1-(A, P, B) 91.3 82.3 86.7 80.8 86.8 84.3 80.7 51.8 80.5 G1-(A, B, L) 91.7 82.5 86.2 81.3 86.3 84.6 81.3 51.7 80.7 G1-(A, P, B, L) 91.6 82.3 86.2 81.1 86.6 84.2 81.1 51.1 80.5 G2-Adapter (A) 92.1 82.5 86.4 81.8 87.2 84.8 81.8 53.8 81.3 G2-Preﬁx (P) 91.8 83.1 87.2 81.6 86.2 84.4 81.1 52.8 81.0 G2-BitFit (B) 91.2 82.1 86.4 81.1 86.3 84.6 80.3 53.1 80.6 G2-LoRA (L) 92.6 82.9 87.5 81.3 87.4 85.1 81.9 52.2 81.4 G2-(P, L) 91.6 82.7 87.6 81.6 87.8 85.3 82.1 52.8 81.4 G2-(A, P) 92.1 83.3 87.5 81.9 87.4 85.5 81.8 53.1 81.5 G2-(A, L) 92.5 83.7 88.1 82.2 87.4 85.7 82.9 53.6 82.1 G2-(A, P, L) 92.3 83.4 87.4 81.6 87.1 85.3 81.4 53.2 81.4 G2-(P, B, L) 91.8 83.1 87.4 81.5 87.2 85.1 82.7 53.8 81.5 G2-(A, P, B) 91.5 82.6 87.8 81.3 86.5 85.2 82.1 54.2 81.4 G2-(A, B, L) 92.6 83.5 87.2 82 87.3 86.5 82.5 52.8 81.8 G2-(A, P, B, L) 92.8 83.2 87.6 81.6 87.5 85.5 82.4 51.2 81.5 G3-Adapter (A) 92.6 84.1 88.3 81.8 87.8 85.4 82.8 55.2 82.2 G3-Preﬁx (P) 92.1 83.3 87.6 81.4 87.1 85.4 82.6 53.5 81.6 G3-BitFit (B) 92.4 83.9 88.4 82.1 87.2 85.8 82.4 53.3 81.9 G3-LoRA (L) 93.1 84.3 87.7 82.4 87.8 86.2 83.1 54.3 82.3 G3-(P, L) 92.8 84.1 88.7 82.6 88.2 86.2 83.3 54.7 82.6 G3-(A, P) 93.1 83.8 89.1 82.3 88.1 85.8 82.6 55.1 82.5 G3-(A, L) 92.7 84.5 88.4 82.8 88.2 86.1 83.5 54.6 82.6 G3-(A, P, L) 92.8 84.6 88.1 82.5 87.7 85.5 83.2 53.8 82.3 G3-(P, B, L) 93.6 84.9 89.3 83.1 88.2 86.5 83.9 55.8 83.2 G3-(A, P, B) 93.3 83.9 88.5 82.2 88.4 86.2 83.5 55.3 82.6 G3-(A, B, L) 93.4 84.2 88.9 82.6 87.8 85.8 84.2 54.9 82.7 G3-(A, P, B, L) 92.2 84.4 88.7 82.3 88.5 86.2 84.2 54.2 82.5 G4-Adapter (A) 92.8 85.2 89.1 83.5 87.8 86.5 84.2 56.3 83.2 G4-Preﬁx (P) 92.8 84.6 89.5 82.6 87.4 86.5 83.8 55.8 82.8 G4-BitFit (B) 93.8 84.9 89.5 83.3 88.7 86.8 84.4 55.2 83.3 G4-LoRA (L) 93.3 84.7 89.3 82.7 88.3 86.2 82.7 54.7 82.7 G4-(P, L) 93.8 85.3 89.6 83.6 88.6 86.8 84.6 56.3 83.5 G4-(A, P) 93.8 84.9 89.8 84.3 88.5 86.6 84.8 56.7 83.6 G4-(A, L) 93.7 85.6 89.5 84.1 88.2 86.6 85.2 55.4 83.5 G4-(A, P, L) 94.2 85.2 89.6 83.9 88.2 86.4 84.9 55.9 83.5 G4-(P, B, L) 93.8 85.9 89.8 83.6 88.6 86.9 85.2 56.3 83.7 G4-(A, P, B) 94.4 85.7 90.1 84.8 88.9 87.2 85.3 57.3 84.2 G4-(A, B, L) 93.8 85.3 89.5 84.1 88.8 86.7 85.5 56.6 83.7 G4-(A, P, B, L) 94.1 85.4 89.7 84.4 88.5 86.5 85.2 56.8 83.8 17Table 16: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Layer grouping is based on 8 groups. Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 76.9 70.1 72.5 73.3 63.6 71.7 73.8 24.3 65.7 Increasing 83.2 74 .1 76 .6 77 .1 67 .7 76.8 74.7 30.0 70.0 Uniform 83.6 73.4 78.0 77.9 68.2 76.4 78.6 34.2 71.3 Decreasing 80.3 71.6 77.4 75.5 67.0 75.3 77.2 26.4 68.9 Spindle 86.2 74.3 79.1 78.6 68.5 77.4 79.5 35.1 72.3 Bottleneck 83.2 73.1 75.8 77.6 67.9 75.3 78.2 31.4 70.3 Table 17: Performances of different tuning methods on the SuperGLUE datasets using the XLNet-base (upper part) and XLNet-large (lower part) pretrained backbone models, respectively. The results are averaged over 10 random runs. The S4-model and S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05 (*) or even p <0.01 (**). Method BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC Average Adapter 72.8 71.3/78.0 64.0 67.0/24.5 71.0/71.8 76.2 65.0 60.8 66.2 Preﬁx 72.0 70.5/77.0 63.3 66.4/23.8 69.9/71.0 75.5 64.4 60.8 65.9 BitFit 71.8 70.0/76.2 62.8 65.8/22.6 69.4/70.6 74.5 64.8 60.6 65.2 LoRA 72.2 71.1/77.8 64.7 67.4/24.8 70.8/71.3 76.8 65.1 61.1 66.4 S4-model 73.8∗∗ 71.7/78.4∗ 65.9∗∗ 68.2/25.5∗∗ 71.1/72.0∗ 78.4∗∗ 65.8∗ 62.6∗ 67.5 Adapter 74.4 71.4/81.1 67.4 68.8/26.4 71.7/72.4 80.8 68.0 64.6 68.8 Preﬁx 72.4 70.0/78.3 66.9 68.8/25.8 70.9/71.2 78.8 66.9 64.0 67.7 BitFit 71.1 70.7/79.8 68.0 68.6/25.4 71.1/71.6 80.4 67.2 64.3 68.1 LoRA 74.1 72.1/80.9 67.9 69.1/26.8 72.0/72.8 81.0 67.8 64.4 69.0 S4-3b-model 76.8∗∗ 74.6/81.9∗∗ 68.6∗∗ 69.5/27.1∗ 72.4/73.3∗ 81.2∗ 68.2∗∗ 64.8∗ 69.7 Table 18: Total training time (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model with 8 A100 GPUs from S0 to S1. SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA 18 mins 22 mins 20 mins 40 mins 8 mins 12 mins 8 mins 6 mins 18",
      "meta_data": {
        "arxiv_id": "2301.01821v1",
        "authors": [
          "Jiaao Chen",
          "Aston Zhang",
          "Xingjian Shi",
          "Mu Li",
          "Alex Smola",
          "Diyi Yang"
        ],
        "published_date": "2023-01-04T21:00:18Z",
        "pdf_url": "https://arxiv.org/pdf/2301.01821v1.pdf",
        "github_url": "https://github.com/amazon-science/peft-design-spaces"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces parameter-efficient fine-tuning (PEFT) design spaces to systematically discover design patterns for adapting large pretrained models. It identifies four key design patterns: (i) grouping layers in a spindle pattern, (ii) allocating trainable parameters uniformly across layers, (iii) tuning all layer groups, and (iv) assigning proper tuning strategies to different groups. These discovered patterns lead to new PEFT methods that consistently and significantly outperform existing individual PEFT strategies across various backbone models and NLP tasks, demonstrating a more comprehensive and effective approach to PEFT design.",
        "methodology": "The methodology involves defining PEFT design spaces characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment (Adapter, Preﬁx, BitFit, LoRA). The research progressively refines an initial unconstrained design space (S0) through a greedy selection process. At each stage, 100 models are randomly sampled from a design space enforced with specific constraints (e.g., a particular layer grouping pattern), fine-tuned for 3 epochs on GLUE benchmarks, and their average performance is used to compare design choices and identify optimal patterns. This iterative refinement leads to the discovery of the four main design patterns.",
        "experimental_setup": "For pattern discovery, GLUE benchmark datasets (SST-2, CoLA, QQP, STS-B, MRPC, MNLI, QNLI, RTE) were used, with T5-base and T5-3b as backbone models. Trainable parameters were set to 0.5% of the backbone model's total parameters, and models were tuned for 3 epochs. For evaluation, the discovered methods (S4-model and S4-3b-model) were tested on GLUE datasets, XSum (abstractive summarization), and WMT 2016 en-ro (machine translation). Backbone models included T5-base/3b, RoBERTa-base/large, BART-base/large, and XLNet-base/large (for SuperGLUE). Baselines included Full Fine-tuning, Adapter, Preﬁx, BitFit, LoRA, and PA (parallel adapter). Hyperparameters included a linear decay scheduler with 0.06 warmup ratio, batch sizes of 128 (base) or 64 (large), max learning rate 5e-5, and 5 or 10 training epochs, utilizing 8 A100 GPUs.",
        "limitations": "The study acknowledges that its goal was not to enumerate all possible design spaces but to demonstrate the utility of design spaces for PEFT research. For computational efficiency, it was beyond the scope to exhaustively enumerate all possible constraints for each design space component. The progressive refinement process relies on greedy selection at each stage, meaning the discovered patterns represent locally optimal choices rather than a globally optimal combination, suggesting there might be better combinations not found by this sequential approach.",
        "future_research_directions": "Not explicitly mentioned, but implied directions include: exploring more extensive and diverse design spaces (beyond the 4 components and their specific choices), investigating alternative and more efficient search algorithms for pattern discovery (moving beyond greedy selection), theoretically analyzing why the discovered patterns are effective, and extending the applicability of these design patterns to other domains beyond NLP, such as computer vision or speech processing.",
        "experimental_code": "The `adapter-transformers` library, as indicated by `models/setup.py` and `models/transformers/__init__.py`, includes an `adapters` module supporting various PEFT strategies. The `_import_structure` in `models/transformers/__init__.py` lists classes such as \"AdapterConfig\", \"AdapterFusionConfig\", \"LoRAConfig\", \"MAMConfig\", \"PfeifferConfig\", \"PrefixTuningConfig\", and \"PEFTConfig\", which define the PEFT design spaces and strategy assignments (Adapter, Prefix, LoRA).\n\nThe implementation sections for preparing and evaluating on GLUE benchmarks are:\n\nFile Path: models/transformers/data/processors/glue.py\nContent:\n```python\ndef glue_convert_examples_to_features(\n    examples: Union[List[InputExample], \"tf.data.Dataset\"],\n    tokenizer: PreTrainedTokenizer,\n    max_length: Optional[int] = None,\n    task=None,\n    label_list=None,\n    output_mode=None,\n):\n    warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n    if is_tf_available() and isinstance(examples, tf.data.Dataset):\n        if task is None:\n            raise ValueError(\"When calling glue_convert_examples_to_features from TF, the task parameter is required.\")\n        return _tf_glue_convert_examples_to_features(examples, tokenizer, max_length=max_length, task=task)\n    return _glue_convert_examples_to_features(\n        examples, tokenizer, max_length=max_length, task=task, label_list=label_list, output_mode=output_mode\n    )\n\n\n# ... (other functions and classes like _tf_glue_convert_examples_to_features, _glue_convert_examples_to_features, MrpcProcessor, MnliProcessor, etc.) ...\n\nglue_tasks_num_labels = {\n    \"cola\": 2,\n    \"mnli\": 3,\n    \"mrpc\": 2,\n    \"sst-2\": 2,\n    \"sts-b\": 1,\n    \"qqp\": 2,\n    \"qnli\": 2,\n    \"rte\": 2,\n    \"wnli\": 2,\n}\n\nglue_processors = {\n    \"cola\": ColaProcessor,\n    \"mnli\": MnliProcessor,\n    \"mnli-mm\": MnliMismatchedProcessor,\n    \"mrpc\": MrpcProcessor,\n    \"sst-2\": Sst2Processor,\n    \"sts-b\": StsbProcessor,\n    \"qqp\": QqpProcessor,\n    \"qnli\": QnliProcessor,\n    \"rte\": RteProcessor,\n    \"wnli\": WnliProcessor,\n}\n\nglue_output_modes = {\n    \"cola\": \"classification\",\n    \"mnli\": \"classification\",\n    \"mnli-mm\": \"classification\",\n    \"mrpc\": \"classification\",\n    \"sst-2\": \"classification\",\n    \"sts-b\": \"regression\",\n    \"qqp\": \"classification\",\n    \"qnli\": \"classification\",\n    \"rte\": \"classification\",\n    \"wnli\": \"classification\",\n}\n```\n\nFile Path: models/transformers/data/datasets/glue.py\nContent:\n```python\n@dataclass\nclass GlueDataTrainingArguments:\n    task_name: str = field(metadata={\"help\": \"The name of the task to train on: \" + \", \".join(glue_processors.keys())})\n    data_dir: str = field(\n        metadata={\"help\": \"The input data dir. Should contain the .tsv files (or other data files) for the task.\"}\n    )\n    max_seq_length: int = field(\n        default=128,\n        metadata={\n            \"help\": (\n                \"The maximum total input sequence length after tokenization. Sequences longer \"\n                \"than this will be truncated, sequences shorter will be padded.\"\n            )\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n\n    def __post_init__(self):\n        self.task_name = self.task_name.lower()\n\n\nclass Split(Enum):\n    train = \"train\"\n    dev = \"dev\"\n    test = \"test\"\n\n\nclass GlueDataset(Dataset):\n    args: GlueDataTrainingArguments\n    output_mode: str\n    features: List[InputFeatures]\n\n    def __init__(\n        self,\n        args: GlueDataTrainingArguments,\n        tokenizer: PreTrainedTokenizerBase,\n        limit_length: Optional[int] = None,\n        mode: Union[str, Split] = Split.train,\n        cache_dir: Optional[str] = None,\n    ):\n        # ... (initialization and data loading logic using glue_processors and glue_convert_examples_to_features) ...\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, i) -> InputFeatures:\n        return self.features[i]\n\n    def get_labels(self):\n        return self.label_list\n```\n\nFile Path: models/transformers/data/metrics/__init__.py\nContent:\n```python\ndef simple_accuracy(preds, labels):\n    warnings.warn(DEPRECATION_WARNING, FutureWarning)\n    requires_backends(simple_accuracy, \"sklearn\")\n    return (preds == labels).mean()\n\n\ndef acc_and_f1(preds, labels):\n    warnings.warn(DEPRECATION_WARNING, FutureWarning)\n    requires_backends(acc_and_f1, \"sklearn\")\n    acc = simple_accuracy(preds, labels)\n    f1 = f1_score(y_true=labels, y_pred=preds)\n    return {\n        \"acc\": acc,\n        \"f1\": f1,\n        \"acc_and_f1\": (acc + f1) / 2,\n    }\n\n\ndef pearson_and_spearman(preds, labels):\n    warnings.warn(DEPRECATION_WARNING, FutureWarning)\n    requires_backends(pearson_and_spearman, \"sklearn\")\n    pearson_corr = pearsonr(preds, labels)[0]\n    spearman_corr = spearmanr(preds, labels)[0]\n    return {\n        \"pearson\": pearson_corr,\n        \"spearmanr\": spearman_corr,\n        \"corr\": (pearson_corr + spearman_corr) / 2,\n    }\n\n\ndef glue_compute_metrics(task_name, preds, labels):\n    warnings.warn(DEPRECATION_WARNING, FutureWarning)\n    requires_backends(glue_compute_metrics, \"sklearn\")\n    assert len(preds) == len(labels), f\"Predictions and labels have mismatched lengths {len(preds)} and {len(labels)}\"\n    if task_name == \"cola\":\n        return {\"mcc\": matthews_corrcoef(labels, preds)}\n    elif task_name == \"sst-2\":\n        return {\"acc\": simple_accuracy(preds, labels)}\n    elif task_name == \"mrpc\":\n        return acc_and_f1(preds, labels)\n    elif task_name == \"sts-b\":\n        return pearson_and_spearman(preds, labels)\n    elif task_name == \"qqp\":\n        return acc_and_f1(preds, labels)}\n    elif task_name == \"mnli\":\n        return {\"mnli/acc\": simple_accuracy(preds, labels)}\n    elif task_name == \"mnli-mm\":\n        return {\"mnli-mm/acc\": simple_accuracy(preds, labels)}\n    elif task_name == \"qnli\":\n        return {\"acc\": simple_accuracy(preds, labels)}\n    elif task_name == \"rte\":\n        return {\"acc\": simple_accuracy(preds, labels)}\n    elif task_name == \"wnli\":\n        return {\"acc\": simple_accuracy(preds, labels)}\n    elif task_name == \"hans\":\n        return {\"acc\": simple_accuracy(preds, labels)}\n    else:\n        raise KeyError(task_name)\n```",
        "experimental_info": "The greedy selection process involves fine-tuning models and comparing their average performance on GLUE benchmarks. The experimental settings for a single fine-tuning run, as suggested by the `TrainCommand` and `GlueDataTrainingArguments`, include:\n\n*   **Task**: `task` (e.g., 'text_classification')\n*   **Model**: `model` (e.g., 'bert-base-uncased')\n*   **Input Data**: `train_data` (path to CSV/TSV), `validation_data` (optional path), `validation_split` (default 0.1 if `validation_data` not provided)\n*   **Input Data Parsing**: `column_label` (default 0), `column_text` (default 1), `column_id` (default 2), `skip_first_row` (boolean flag)\n*   **Output Directory**: `output` (default './')\n*   **Batch Sizes**: `train_batch_size` (default 32), `valid_batch_size` (default 64)\n*   **Optimizer Settings**: `learning_rate` (default 3e-5), `adam_epsilon` (default 1e-8)\n*   **Sequence Length**: `max_seq_length` (default 128 for GLUE datasets)\n\nThe method specifies \"100 models are randomly sampled\" and \"fine-tuned for 3 epochs\". While the sampling and epoch count are central to the overall experimental design, these specific numerical values (`100` and `3`) are not directly found in the provided code snippets, which focus on the implementation of the building blocks rather than the high-level orchestration script."
      }
    },
    {
      "title": "Visual Prompt Tuning for Generative Transfer Learning",
      "abstract": "Transferring knowledge from an image synthesis model trained on a large\ndataset is a promising direction for learning generative image models from\nvarious domains efficiently. While previous works have studied GAN models, we\npresent a recipe for learning vision transformers by generative knowledge\ntransfer. We base our framework on state-of-the-art generative vision\ntransformers that represent an image as a sequence of visual tokens to the\nautoregressive or non-autoregressive transformers. To adapt to a new domain, we\nemploy prompt tuning, which prepends learnable tokens called prompt to the\nimage token sequence, and introduce a new prompt design for our task. We study\non a variety of visual domains, including visual task adaptation\nbenchmark~\\cite{zhai2019large}, with varying amount of training images, and\nshow effectiveness of knowledge transfer and a significantly better image\ngeneration quality over existing works.",
      "full_text": "Visual Prompt Tuning for Generative Transfer Learning Kihyuk Sohn, Yuan Hao, Jos´e Lezama, Luisa Polania, Huiwen Chang, Han Zhang, Irfan Essa, Lu Jiang Google Research Abstract Transferring knowledge from an image synthesis model trained on a large dataset is a promising direction for learn- ing generative image models from various domains efﬁ- ciently. While previous works have studied GAN models, we present a recipe for learning vision transformers by gener- ative knowledge transfer. We base our framework on state- of-the-art generative vision transformers that represent an image as a sequence of visual tokens to the autoregressive or non-autoregressive transformers. To adapt to a new do- main, we employ prompt tuning, which prepends learnable tokens called prompt to the image token sequence, and in- troduce a new prompt design for our task. We study on a variety of visual domains, including visual task adaptation benchmark [71], with varying amount of training images, and show effectiveness of knowledge transfer and a signiﬁ- cantly better image generation quality over existing works. 1. Introduction Image synthesis has achieved tremendous progress re- cently with the improvement of deep generative models [2, 11,18,60,62]. The goal of image synthesis is to generate di- verse and plausible scenes resembling the training images. A good image synthesis system can capture the appearance of objects and model their interactions to generalize and cre- ate novel scenes. However, the generalization ability is usu- ally determined by the amount of training images. Without sufﬁcient data, the synthesis results are often unsatisfactory. Transfer learning, a cornerstone invention in deep learn- ing, has been proving its indispensable role across a broad array of computer vision tasks, including classiﬁcation [31], object detection [16, 17], image segmentation [20, 21], etc. However, transfer learning is not yet a de facto technique for image synthesis. While recent efforts have shown suc- cess in transferring knowledge from pretrained Generative Adversarial Network (GAN) models [42, 53, 64, 68], these demonstrations are limited by narrow visual domains, e.g., faces or cars [42, 68], as illustrated in Fig. 1, or requiring a non-trivial amount of training data [53, 64] to transfer to an off-manifold distribution. In fact, some recent works [56,73] have found that, even when the training data is limited in quantity, learning GANs from scratch with advanced techniques outperforms GAN transfer approaches, implying that the transfer learning may not even be necessary for generative modeling. Such obser- vation is in direct contrast to the essential role of transfer learning for discriminative models, 1 which suggests trans- fer learning for image synthesis remains under-exploited. In this work, we approach the transfer learning for image synthesis using generative vision transformers, an emerg- ing class of image synthesis models, such as DALL·E [47], Taming Transformer [14], MaskGIT [6], CogView [12], N ¨UW A [67], or Parti [70], that excel in several image syn- thesis tasks. We closely follow the recipe of transfer learn- ing for image classiﬁcation [31], in which a source model is ﬁrst trained on a large dataset ( e.g., ImageNet) and then transferred to a diverse collection of downstream tasks, ex- cept in our setting the input and output are reversed and the model generates images from a class label. Our study em- ploys the visual task adaptation benchmark (or VTAB) [71], a standard and challenging benchmark for studying transfer learning. VTAB consists of 19 visual recognition tasks and compiles images from diverse and distinctly different visual domains, such as natural (e.g., ﬂowers, scenes), specialized (e.g., satellite, medical), or structured (e.g., road scenes). We present a transfer learning framework using prompt tuning [34,36]. While the technique has been used for trans- fer learning of discriminative models for vision tasks [1,26], to our knowledge, this work appears to be the ﬁrst to adopt prompt tuning for transfer learning of image synthe- sis. Moreover, we propose two technical innovations. First, a parameter efﬁcient design of prompt token generator that admits condition variables ( e.g., class, instance), a key for controllable image synthesis yet often neglected in prompt tuning for discriminative transfer [26, 34]. Second, a mar- quee header prompt that engineers ( e.g., composes and in- terpolates) learned prompts to enhance generation diversity. We conduct a large-scale empirical study to understand the mechanics of generative transfer learning for autoregres- 1Here discriminative models refer to a board of machine learning mod- els that directly model the conditional distribution of the target variables. 1 arXiv:2210.00990v1  [cs.CV]  3 Oct 2022Figure 1. Image synthesis by knowledge transfer. Unlike previous works using GANs as source model and test transfer on relatively narrow visual domains, we transfer knowledge of generative vision transformers [6,14] to a comprehensive list of visual domains, including natural (e.g., scene, ﬂower), specialized (e.g., satellite, medical), and structured (e.g., road scenes, synthetic, infograph, sketch), as deﬁned by the visual task adaptation benchmark [71], with a few training images (e.g., as low as 2 images per class). sive [14, 47] and non-autoregressive [6] generative trans- formers. To this end, we show that generative vision trans- formers with prompt tuning outperforms the prior state-of- the-art held by GANs [53,64] through a vast margin. More- over, in contrast to prior works [53, 64] limited to show transfer to a few visual domains, we show the efﬁcacy of knowledge transfer from pretrained ImageNet models to 19 downstream tasks of diverse visual distributions and vary- ing amounts of training data in VTAB. Fig. 1 compares vi- sual domains, showing the great expansion on the varieties of downstream tasks to what is achieved by previous works. On the on-manifold distributions on which previous studies mainly focused, our method slashes the prior state-of-the- art in FID from 71 to 24 on Places [74] and 86 to 16 on Animal Face [54] datasets. Moreover, the proposed method is used to demonstrate the few-shot generative transfer ca- pabilities (Sec. 4.2), showing extreme data efﬁciency while being able to generate images that are realistic and diverse, while following the target distribution. In summary, our contributions are as follows: • We present a generative visual transfer learning frame- work for vision transformers with prompt tuning [34], proposing a novel prompt token generator design and a prompt engineering method for image synthesis. • We conduct a large-scale empirical study for genera- tive transfer learning to validate our method on a vari- ety of visual domains (e.g., VTAB [71]) and scenarios (e.g., few-shot). To this end, we show state-of-the-art image synthesis performance. • To our knowledge, we are the ﬁrst to employ prompt tuning for transfer learning of image synthesis, and provide one-of-the-ﬁrst substantial empirical evidence on the necessity of knowledge transfer for data and compute efﬁcient generative image modeling using the standard visual transfer learning benchmark. 2. Preliminary 2.1. Generative Vision Transformers This paper uses generative vision transformers to denote the vision transformer models for image synthesis. Gener- ally, there are two types of generative vision transformers: AutoRegressive (AR) and Non-AutoRegressive (NAR) trans- formers, both consisting of two stages [14,47]: image quan- tization and decoding. The ﬁrst stage is the same between the two types of models in which the image is quantized into a grid of discrete tokens by a Vector-Quantized (VQ) auto- encoder [14, 48, 60, 69]. The VQ encoder quantizes image patches into integer indices (or tokens) in a codebook. The 2D image is then ﬂattened into a 1D sequence to which a special token indicating its class label is prepended. AR and NAR transformers differ in the second stage of decoding. AR transformers [7], including DALL·E [47], Taming Transformer [14], N¨UW A [67], CogView [12], and Parti [70], are inspired by the AR language model [3, 39]. They learn an AR decoder on the ﬂattened token sequence to generate image tokens sequentially based on the previ- ously generated tokens. As illustrated in Fig. 2, the genera- tion follows a raster scan ordering, generating tokens from left to right, line-by-line. Finally, the generated tokens are mapped to the pixel space using the VQ decoder. On the other hand, NAR transformers [15,19,33], which are originally proposed for machine translation, are re- cently extended to improve the AR image decoding [6, 35, 72]. Unlike their AR counterpart, NAR transformers ( e.g., MaskGIT [6], Token-Critic [35], BLT [32]) are bidirec- tional and are trained on the masked modeling proxy task of BERT [10]. During inference, the model adopts a non- 2BERT /  AR transformer Pretrain on ImageNet BERT /  AR transformer Flowers, Retinopathy, Kitti, …  Mutable Frozen transfer t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8t=0 t=1 t=2 t=100 t=160 t=256t=0 Autoregressive Decoding Non-autoregressive (parallel) Decoding Prompt token Visual token Figure 2. Our method transfers knowledge from generative vision transformers (e.g., autoregressive [14] or non-autoregressive [6]) trained on a large dataset to various visual domains by prepending learnable prompt tokens (green) to visual tokens (blue). autoregressive decoding method to synthesize an image in a few steps [6,19,32,35]. As shown in Fig. 2, the NAR trans- former starts from a blank canvas with all tokens masked out, and generate an image in 8 steps or so. In each step, it predicts all tokens in parallel while retaining ones with the highest prediction scores. The remaining tokens are masked out and will be predicted in the next iteration until all tokens are generated. NAR transformers [6, 35] show faster infer- ence than their AR counterparts ( e.g., [14]) while offering on-par or superior ﬁdelity and diversity. 2.2. Prompt Tuning Prompt tuning [34, 36] has been introduced recently in natural language processing as a way of efﬁciently adapt- ing pretrained large language models to downstream tasks. Here, prompt is a sequence of additional tokens prepended to a token sequence. In prompt engineering [4], their values are often chosen by heuristic. On the other hand, in prompt tuning [34, 36], tokens are parameterized by learnable pa- rameters and their parameters are updated via a gradient de- scent to adopt transformers to the downstream tasks. Due to its simplicity and as transformers getting popular, prompt tuning has been also applied to some vision tasks for knowledge transfer, e.g., image classiﬁcation [1, 26], detec- tion and segmentation [41]. To our knowledge, we appear to be the ﬁrst to use prompt tuning for image synthesis. 3. Visual Prompt for Generative Transfer Our goal is to design a transfer learning framework for image synthesis using vision transformers. Starting from a generative vision transformer pretrained on a large dataset (e.g., ImageNet), we discuss a method to adapt transformers on various target domains (e.g., VTAB). Sec. 3.1 presents a visual prompt tuning for AR and NAR transformers. Then, in Sec. 3.2, we propose a novel prompt, named marquee header prompt, tailored to NAR transformers to trade-off generation ﬁdelity and diversity. 3.1. Building and Learning Visual Prompt Fig. 2 overviews the proposed generative transfer learn- ing framework. We aim at transferring a generative prior, parameterized by generative vision transformers, while uti- lizing the same VQ encoder and decoder trained from the large source dataset. We employ a prompt tuning [26,34,36] that uses a sequence of learnable tokens ( e.g., green blocks with a solid line in Fig. 2) to adapt to target distributions, while leaving transformer parameters frozen. In the follow- ing sections, we discuss how to learn (Sec. 3.1.1) a prompt token generator designed for a conditional image generation (Sec. 3.1.2) and use them for image synthesis (Sec. 3.2). 3.1.1 Learning Visual Prompt A sequence of prompt tokens is prepended to visual tokens to guide the pretrained transformer models to the target dis- tribution. Prompt tuning, learning parameters of token gen- erator, is done by gradient descent with respective loss func- tions, while ﬁxing parameters of pretrained transformers. To be speciﬁc, let Z= {zi}H×W i=1 be a sequence of visual tokens (i.e., an output of VQ encoder followed by the vec- torization) and Pφ= {ps;φ}S s=1 be a sequence of prompt to- kens. For AR transformer, the loss is given as follows: LAR = Ex∼PX [ −log Pθ(Z|Pφ) ] (1) Pθ(Z|Pφ) = ∏H×W i=1 Pθ(zi|z<i,Pφ) (2) For NAR transformer, we follow the loss of MaskGIT [6]: LNAR = Ex∼PX,M∼PM [ −log Pθ(ZM|ZM,Pφ) ] (3) Pθ(ZM|ZM,Pφ) = ∏ i∈M Pθ(zi|ZM,Pφ) (4) where M⊂{1,...,H ×W}is a set of visual token indices sampled from a masking schedule distributionPM, Mis its complement, and ZM = {zi}i∈M. Prompt tuning proceeds by minimizing respective loss functions with respect to the prompt parameters φwhile ﬁxing transformer parameters θ: φ∗= arg min φ LAR/NAR (5) While our focus is at the prompt tuning due to its effec- tiveness and compute-efﬁciency for large source models, we note that the proposed learning framework is amenable with other transfer learning methods, such as adapter [25] or ﬁne- tuning [31], with learnable prompts, as shown in Sec. 5.4. After prompt tuning, we generate visual tokens for image synthesis by iterative decoding. For AR transformer, 3(a) Baseline prompt token generators of length S conditioned on class. Transformer Transformer decode MLPC MLPP class / instance position MLPT B x 1 x P x F 1 x S x P x F B x S x P x F S MLPF factor 1 x 1 x 1 x F 𝝨F B x S x P D (b) The proposed parameter efﬁcient prompt token generator via factorization of class / instance and position. ⊕ is an element-wise sum, ⊙ is an element-wise product, ΣF is a sum over F dimension. S: sequence length, B: batch size, P: feature dimension, D: token dimension. 100 101 102 Sequence length (S) 106 107 # parameters Baseline Proposed (F=1) Proposed (F=4) Proposed (F=16) (c) Number of parameters with respect to the sequence length and different number of factors F. Figure 3. Prompt token generators and their use in transformer. (a) a straightforward extension of baseline prompt token generators [26,34, 36] with a class condition. When using an MLP with a single dense layer of P units, the number of trainable parameters is P·(C·S+D). (b) The proposed parameter efﬁcient prompt token generators that factorizes data dependent conditions (e.g., class, instance) and token position. Under a similar design choice as baseline models, the number of trainable parameters is P·(F·(C+S)+D), which could be signiﬁcantly fewer when F≪min(C, S). (c) Number of parameters for prompt token generators with respect to the sequence length (S), while setting P = 768, D = 768, and C = 100with different number of factors F. 1: for i ←1 to H ×W do 2: ˆzi∼Pθ(zi|ˆz<i, Pφ) 3: end for For NAR model, scheduled parallel decoding [6] is used: Require: M = {}, T, {n1, ..., nT}, ∑T t=1 nt= H ×W 1: for t ←1 to T do 2: ˆzi∼Pθ(zi|ˆZM, Pφ), ∀i ∈M 3: M ←M ∪{arg topki∈ M ( Pθ(zi|ˆZM, Pφ), k= nt ) } 4: end for where {n1,...,n T}is a masking schedule that decides the number of tokens to decode at each decoding step. We refer to [6] for details on decoding for NAR transformer. Illus- trations of decoding steps for both models are in Fig. 2. 3.1.2 Prompt Token Generator Design For discriminative transfer learning, prompts are designed without condition variables [26]. For generation, it is ben- eﬁcial to have condition variables ( e.g., class, attribute) for better control in generation. We accomplish this with rather a straightforward extension of existing prompt designs us- ing a class-condition, Pφ(c), as in Fig. 3a. One caveat of the baseline token generator design is that the number of learnable parameters increases as the product of three factors: the number of classes C, the prompt se- quence length Sand the feature dimension P. For example, when using a prompt of length S=128, hidden P=768 and embedding dimension D=768, the token generator would introduce 10.4M parameters forC=100 class conditions, as in Fig. 3c. The bottleneck occurs at the 3d weight tensor of size C×S×P. To make it parameter efﬁcient, we propose a factorized token generator, as in Fig. 3b. Speciﬁcally, we encode class and sequence position index via MLP C and MLP P with F factors, respectively. The MLP outputs are element-wise summed, multiplied by an 1d factor vector from MLP F, and reduced along the factor dimension. The output is then fed to MLPT to produce a prompt of lengthS. As in Fig. 3c, the number of parameters of the proposed ar- chitecture is greatly reduced, requiring only 0.76M param- eters, down from 10.4M, for a prompt of length 128 when F= 1.2 An implementation of the proposed token genera- tor in Flax [22] is in Fig. 13 of Appendix. We empirically ﬁnd that F= 1is sufﬁcient for NAR transformers, demon- strating extreme parameter efﬁciency. For AR transformers, we need extra capacity and use F= 16. Moreover, we build a new type of prompt tokens condi- tioned on individual data instances, inspired by the instance- conditioned GAN [5]. We assign each data a unique index and map it into a distinct embedding via MLPC. When both class label and instance index are used, instance index is simply treated as an extra class, indexed from C. To train the model, we sample between class label and instance in- dex. As we explain below in Sec. 3.2, instance conditioned prompts add more ﬁne-grained control on generation. 3.2. Engineering Learned Prompts An interesting aspect of generative transformers in con- trast to GANs is their iterative decoding. For example, as illustrated in Fig. 2, AR transformers [14] decode tokens se- quentially given previously decoded tokens, and NAR trans- formers [6] use a scheduled parallel decoding. Given the wealth of learned prompts conditioned on the class and instance proposed in Sec. 3.1, we propose a novel prompt engineering strategy, a “Marquee Header” prompt, of the iterative transformer decoding, for enhancing the gen- eration diversity. The idea is simple – similarly to the latent 2The proposed factorization can be extended to incorporate the “depth” position of deep visual prompt [26] to reduce the number of parameters. 4(a) Image synthesis using instance-conditioned prompts. (b) Image synthesis using a marquee header prompt between instance (blue) and class (red) conditioned prompts. (c) Image synthesis using a marquee header prompt between instance-conditioned prompts (blue and red). Figure 4. Iterative decoding of NAR transformers. (4a) instance prompts generate images of high-ﬁdelity but with low diversity. Marquee header prompts enhance generation diversity by interpolating (4b) from instance to class prompts or (4c) between instance prompts. variable interpolation of GANs, we interpolate the learned prompt representations ( e.g., outputs of MLP C). Yet, due to the iterative decoding, the interpolation between prompts is carried out over multiple decoding steps. This is illus- trated in Fig. 4b, where we start the decoding process us- ing instance-conditioned prompts (blue header) but gradu- ally transition to a class-conditioned prompt (red header) over decoding steps. Compared to the generation in Fig. 4a where we use instance-conditioned prompts all along, the proposed prompt engineering strategy enhances the gener- ation diversity while being controlled in that synthesized images follow certain characteristics ( e.g., pose, color pat- tern, hairiness) of reference instances. In addition, it is also plausible to construct a marquee header prompt between instance-conditioned prompts, as in Fig. 4c. We provide a marquee header prompt formulation: PMT(t) = (1 −wt)PMT1 + wtPMT2 (6) wt = min {( t−1 Tcutoﬀ −1 )2 ,1 } (7) where t= 1,...,T is a decoding step, Tcutoﬀ ≤T is a cutoff step, and P MTi is a prompt representation ( e.g., an output of MLP C). The schedule in Eq. (7) makes a smooth transi- tion of prompts from PMT1 to PMT2. Note that there could be various marquee header prompt formulations, which we leave their investigations as a future work. 4. Experiments We evaluate the efﬁcacy of the generative transfer learn- ing on diverse visual domains and varying amounts of train- ing data and compare with existing methods. In Sec. 4.1, we test on visual task adaptation benchmark (VTAB) [71] and demonstrate state-of-the-art image generation performance with knowledge transfer. In Sec. 4.2, we verify our method on diverse few-shot transfer learning tasks. 4.1. Generative Transfer on VTAB Dataset. Towards developing a generative transfer method generalizable across domains and distributions, we employ the visual task adaptation benchmark (VTAB) [71] – a suite of 19 visual recognition tasks based on 16 datasets. It covers diverse image domains ( e.g., natural, structured, and spe- cialized such as medical or satellite imagery) and tasks (e.g., object and scene recognition, distance classiﬁcation, count- ing), making it a valuable asset not only for discriminative, but also for generative transfer learning. The dataset infor- mation is provided in Appendix B.1.1. 5Model (# tr params) Mean C101 Flowers Pet DTD Kitti SUN EuroSATResisc MineGAN [64] (88M) 151.5 102.4 132.1 130.1 87.4 117.9 77.5 111.5 81.0 cGANTransfer [53] (105M) 85.1 89.6 61.6 48.6 70.3 48.9 31.1 45.6 50.3 Non-Autoregressive Prompt (S= 1) (0.67M) 53.7 13.5 13.8 11.9 25.8 32.3 7.3 45.9 28.5 Prompt (S= 16) (0.68M) 39.9 12.7 13.2 11.1 26.0 30.0 7.4 35.8 24.9 Prompt (S= 128) (0.76M) 36.4 12.9 13.4 10.9 25.9 29.9 7.7 38.4 24.8 Scratch (172M) 42.7 72.7 57.2 70.3 66.1 33.8 9.2 39.5 32.0 Autoregressive Prompt (S= 1) (0.86M) 58.4 45.5 28.9 42.4 37.1 66.9 18.9 37.3 35.1 Prompt (S= 16) (0.88M) 45.8 41.4 19.6 36.6 33.4 41.4 16.4 32.6 28.8 Prompt (S= 256) (1.06M) 39.0 39.6 17.3 34.9 32.5 37.1 15.0 29.6 26.7 Prompt (S= 256,F= 16) (5.16M) 36.9 27.2 14.1 27.2 30.0 34.6 12.8 26.4 22.2 Scratch (306M) 39.6 76.0 56.1 52.5 92.7 31.6 13.5 19.4 29.5 Table 1. FIDs (lower the better) of image generation models on VTAB tasks. The number of trainable parameters (second column) are computed assuming 100 classes. The mean FID over 19 VTAB tasks (third column) and those for dataset with a small to mid-scale training data are reported. Complete results are in Appendix B.1.3. The best and the second best results are highlighted in each column. (a) SUN397 (FID=7.7; NAR + Prompt)  (b) DTD (FID=25.9; NAR + Prompt)  (c) Resisc (FID=24.8; NAR + Prompt) (d) SUN397 (FID=12.8; AR + Prompt)  (e) DTD (FID=30.0; AR + Prompt)  (f) Resisc: (FID=22.2; AR + Prompt) Figure 5. Class conditional generation using NAR (top; S=128) and AR (bottom; S=256, F=16) transformers with prompt tuning. Setting. We study class-conditional image generation mod- els on the VTAB (full) tasks. Class-conditional prompts are trained on the “train” split, using the same hyperparameters across tasks as provided in Appendix B.1.2. We investigate generative transfer of AR and NAR trans- formers using class-conditional Taming Transformer [14] and MaskGIT [6], respectively, trained on256×256 images of ImageNet dataset as source models. Both models contain 24 transformer layers, comprised of306M and 172M model parameters, respectively. Baselines. We compare our method with GAN-based gen- erative transfer learning methods, including MineGAN [64] and cGANTransfer [53]. Note that both of these algorithms use a BigGAN [2] trained on ImageNet as a source. It is worth noting that the BigGAN model is trained on128×128 images and its validation FID on ImageNet is 7.4. This is better than that of our pretrained AR transformer (18.7) and almost on par with that of NAR transformer (6.2). We further compare with generative transformers trained from scratch on VTAB. To highlight the compute efﬁciency, models are trained with a comparable compute budget (e.g., same number of train epochs) to transfer learning models. Hyperparameters are provided in Appendix B.1.2. We pro- vide more in-depth analysis without compute budget restric- tions in Sec. 5.4. Evaluation. We use Frechet Inception Distance (FID) [24] as a quantitative metric. We generate20kimages from each model and compare with images from a respective dataset. We sample 20kimages if the dataset is larger than 20k. Results. We report FIDs of models trained and evaluated 6on VTAB tasks in Tab. 1 averaged over 3 runs. Due to lim- ited space, we report results on tasks with small to mid-scale train set in addition to the mean FID over 19 datasets. Com- plete results are given in Appendix B.1.3. In addition, we provide images generated by various transfer learning meth- ods for thorough visual inspection. See Appendix B.1 for evaluation details and more extensive comparison. We see that prompt tuning is effective for both AR and NAR gener- ative transformers, especially when the number of training images is small (e.g., ≤10k). Between AR and NAR trans- formers, we ﬁnd that NAR model transfers better than the AR counterpart. Nevertheless, both generative transformers with class-conditional prompt tuning show signiﬁcant gain in performance when compared to GAN-based baselines. We see that the prompt tuning of generative transform- ers beneﬁts greatly from a long prompt, reducing mean FID from 53.7 to 36.4 by increasing the length from 1 to 128. This is achieved by only adding less than 0.1M parameters, thanks to our parameter-efﬁcient design of the prompt token generator. Nevertheless, this comes at an increased cost at generation time due to increased sequence length. Empiri- cally, we ﬁnd that using128 tokens for the prompt increases the overall generation time by 25%, as shown in Tab. 4. AR transformers also beneﬁt from the longer prompt. On the other hand, AR transformers generally requires prompts with more learnable parameters, which is achieved by in- creasing the number of factors. The performance is still on par with that achievable with the baseline prompt, while us- ing signiﬁcantly less number of parameters ( 5.6M instead of 20.5M), as shown in Sec. 5.3. In Fig. 5, we show generated images using 128 prompt tokens for NAR transformers and 256 prompt tokens (with F= 16) for AR transformers on a few VTAB tasks. More generated images are in Appendix B.1.4. Despite learning less than 0.5% of the transformer parameters, the learned prompts are able to change the generation process of pre- trained generative transformers to follow the target distribu- tion. 4.2. Few-shot Generative Transfer After validation on VTAB, we delve deeper into a few- shot generative transfer, where the number of training im- ages is further reduced. We limit our study to transfer of an NAR transformer,i.e., MaskGIT [6], but with more compar- isons to existing few-shot image generation models, either with [53, 64] or without [56, 73] knowledge transfer. Dataset. We study few-shot generative transfer learning on Places [74], ImageNet [9], and Animal Face [54]. Follow- ing [53, 64], for Places and ImageNet, we select 5 classes 3 and use 500 images per class for training. For Animal Face, 3Cock, Tape player, Broccoli, Fire engine, Harvester for ImageNet, and Alley, Arch, Art gallery, Auditorium, Ballroom for Places. Dataset ImageNetPlacesAnimal FaceDog FaceCat Face (shot) (500) (500) (100) (389) (160) MineGAN [64] 61.8† 82.3† – 93.0∗ 54.5∗ cGANTransfer [53]– 71.1‡ 85.9‡ – – DiffAug [73] – – – 58.5∗ 42.4∗ LeCam GAN [56]– – – 54.9∗ 34.2∗ Ours (class) 16.9 24.2 16.3 65.4 40.2 Ours (instance) 19.6 19.5 13.3 26.0 31.2 Table 2. FIDs of image generation models on few-shot benchmark. Numbers with †, ‡, ∗are from [64], [53], [56], respectively. we consider two scenarios – following [53], we use 100 im- ages per class for training from 20 classes (denoted as “An- imal Face” in Tab. 2); alternatively, following [56, 73], we use all images of dog (389) and cat (160) classes (denoted as “dog face” and “cat face” in Tab. 2) for training. Moreover, we test our methods to more challenging off- manifold target tasks on DomainNet [45] Infograph and Cli- part (345 classes), and ImageNet sketch (1000 classes) [63] with as low as 2 training images per class. Setting. We study a class-and-instance conditional genera- tive transfer as in Sec. 3.1.2. Class-and-instance conditional prompts are particularly suitable for few-shot scenarios as there are only a limited number of training images. Baselines. GAN-based generative transfer learning meth- ods, e.g., MineGAN [64] and cGANTransfer [53], are used as baselines. Moreover, we compare to few-shot image gen- eration models, e.g., DiffAug [73] and LeCam GAN [56]. Evaluation. We report FIDs using 10k generated images, except for experiments on dog and cat faces, where we gen- erate 5kimages following [73]. For Places, ImageNet, and Animal Face, we use an entire training data ( i.e., 2500 for Places and ImageNet, 2000 for Animal Face, 389 and 160 for dog and cat faces, respectively) for the reference distri- bution. We sample10kimages for the reference distribution to compute FID for DomainNet and ImageNet sketch. Results. In Tab. 2, we report FIDs of our proposed method using prompts of S= 128. When conditioned on the class, our method improves FIDs upon existing generative transfer learning methods. When comparing with few-shot genera- tion methods on dog and cat face datasets, our method with a class condition slightly under-performs, likely due to that dataset having one class. When conditioned on instances, our models outperform all GAN-based few-shot generation models. We provide visualizations in Appendix B.2.1. We visualize generated images conditioned on the class by our models in Fig. 6. We show 2 (and only) training data for each class in red boxes. We observe that, though images in these datasets are highly artiﬁcial and their distributions are different from the source dataset, our method is able to synthesize images from respective target distributions well. Moreover, as clearly seen from Fig. 6, our models do more 7(a) DomainNet Clipart (2 shot; FID=22.4)  (b) DomainNet Infograph (2 shot; FID=20.6)  (c) ImageNet Sketch (2 shot; FID=14.4) Figure 6. Class conditional generation of few-shot transfer models. Images in red boxes are two training images of each class. than simply memorizing the training data. Data Efﬁciency. We conduct experiments with less training images to investigate the data efﬁciency. We train models on 5, 10, 50 and 100 images per class for ImageNet, Places and Animal Face datasets. We use a class-condition for im- age generation. The same number of images is used for the reference set to make FIDs comparable across settings. Results are in Fig. 7. Our method shows far superior data efﬁciency, achieving substantially lower FIDs with only 5 training images per class, to GAN-based transfer learning methods trained with 20 or 100 times more images per class. We ﬁnd that using long prompts is not favorable when the number of training images is too small ( e.g., less than 10 images per class for ImageNet and Places, 50 in total), as models start to overﬁt to a few images in the train set. When the total number of images is larger than 250, we ﬁnd that using a long prompt is still beneﬁcial. Enhancing Generation Diversity via Prompt Engineer- ing. As in Sec. 3.2 and Figs. 4b and 4c, our model offers a way to enhance generation diversity by composing prompts. We report quantitative metrics to support our claim. We conduct experiments on the dog and cat faces dataset using marquee header prompts with differentTcutoﬀ values. For the ﬁdelity metric, we compute the FID. To measure the diversity, we follow [42] and report a intra-cluster pairwise LPIPS distance, where we generate 5k samples and map them into one of training images.4 Results are shown in Fig. 8. Ideally, we expect a model with low FID and high intra-cluster LPIPS scores (e.g., yel- low star at top-left corner). When generating samples using a class-condition (red square), we generate diverse images, but with relatively poor ﬁdelity. On the other hand, when conditioned on data instances (green dot), we improve the FID by a large margin, but at the cost of reduced diversity. Instance to class Marquee header prompts (blue) allow to 4We use a pixel-wise L2 distance for computation efﬁciency instead of LPIPS distance in [42]. 5 10 50 100 500 # image/class 20 30 40 50 60FID MineGAN Prompt (S=1) Prompt (S=128) (a) ImageNet 5 10 50 100 500 # image/class 20 30 40 50 60 70 80 90 MineGAN cGANTr. Prompt (S=1) Prompt (S=128) (b) Places 5 10 50 100 # image/class 20 30 40 50 60 70 80 90 cGANTr. Prompt (S=1) Prompt (S=128) (c) Animal Face Figure 7. FIDs for models trained with varying numbers of images per class for class-conditional few-shot generative transfer. (a) Dog Faces  (b) Cat Faces Figure 8. Marquee header prompt shows clear tradeoff between ﬁdelity (FID) and diversity (LPIPS) when interpolating from in- stance to class (blue). It shows a better tradeoff when interpolating between instances (orange), achieving low FID and high LPIPS. control the generation diversity and ﬁdelity. Moreover, in- stance to instance Marquee header prompts, which interpo- lates an instance prompt to another randomly selected in- stance prompt, shows much better tradeoff between ﬁdelity and diversity. 8dataset: oxford_ﬂowers102, length: 1, embed: 32,  word: 0.848, token:0.851 Code Text (a) S = 1(NMI=0.848) dataset: oxford_ﬂowers102, length: 128,  embed: 32, word: 0.800, token:0.820  (b) S = 128(NMI=0.800) Figure 9. t-SNE plots of instance-conditioned prompt represen- tations on ﬂowers dataset. Points of the same color are from the same class. We also report normalized mutual information (NMI) score by clustering prompt representations using KMeans. 5. Analysis and Discussion With a successful demonstration of the power of prompt tuning for generative transfer learning, we further study to understand prompt representations (Sec. 5.1, Sec. 5.2) and conduct an ablation study regarding design choices of prompt token generator (Sec. 5.3) and transfer learning (Sec. 5.4). 5.1. What does the Prompt Learn? To understand what the prompt has learned, we study some properties of learned prompt representations. For this study, we train instance conditioned prompt models on ﬂowers dataset of VTAB, with S= 1and 128. Note that no class information is used for training in this experiment. We draw t-SNE plots [61] of prompts in Fig. 9. Here, we opt to use an output of an MLPC as a prompt representation instead of a token sequence (e.g., output of an MLP T) due to its low dimensionality. We see in Fig. 9a that points of the same color ( i.e., same class) are grouped together, im- plying that the prompt representations learn discriminative class information. While we see a similar trend in Fig. 9b, there are clusters crowded with points of various colors. We quantify our observation using a normalized mutual infor- mation (NMI) computed by clustering prompts. Clustering is more consistent with the ground-truth class labels with higher NMIs. The model with S= 1achieves 0.848 and the one with S= 128gets 0.800. Note that these are even better results than the number obtained using an embedding from ImageNet pretrained ResNet-50 [21] (NMI=0.734). 5.2. Adaptation-Diversity Trade-Off We study prompts with various lengths, but on a single image. We show generated images of models with different lengths in Fig. 10. With short prompts, the model produces diverse but less detailed images. On the other hand, a long prompt model generates images of a higher quality, more faithful to the training image, but less diverse. This implies 1 2 4 8 16 128 Figure 10. A single training image in red box and those generated by models using prompts of various lengths from 1 to 128. NAR # paramsSmallMediumLargeNaturalStruct.Spec. S=16 baseline1.81M 18.6 34.6 89.1 23.8 50.9 41.7F=1 0.68M 18.6 36.1 89.5 25.2 51.9 41.5F=4 0.95M 18.6 35.5 88.4 24.4 51.5 41.4F=16 2.02M 18.5 35.0 86.8 24.3 50.8 40.4 S=128 baseline10.4M 18.2 30.8 86.4 22.0 46.9 39.9F=1 0.76M 18.5 30.6 88.9 22.5 47.1 40.5F=4 1.30M 18.1 31.5 88.0 23.3 48.2 38.0F=16 3.39M 17.9 30.8 86.5 22.6 47.4 37.7 AR # paramsSmallMediumLargeNaturalStruct.Spec. S=16 baseline2.02M 30.5 41.9 82.7 28.5 61.9 41.7F=1 0.88M 34.5 43.3 83.9 32.3 62.9 42.9F=4 1.14M 31.9 42.3 82.7 29.9 62.0 42.0F=16 2.21M 31.2 41.9 82.6 28.9 61.9 41.6 S=256 baseline20.4M 25.7 32.7 71.6 23.7 52.1 35.9F=1 1.06M 32.3 33.5 70.5 29.0 49.1 36.4F=4 1.88M 31.2 41.9 82.6 28.9 61.9 41.6F=16 5.16M 26.6 32.6 69.9 24.5 48.9 34.6 Table 3. Ablation on prompt token generators for (top) NAR and (bottom) AR transformers on VTAB. We report FIDs averaged by different categorizations of tasks. that the short prompt learns concepts, while the long prompt learns ﬁne details of training data. This is in line with our results in Sec. 5.1 where short prompts learn more discrim- inative information than long prompts. In Fig. 11, we visualize images generated by models of Sec. 5.1. Compared to images in Fig. 11a whose model is trained with S= 1, we clearly see in Fig. 11b that the model trained with a long prompt generates images that are more consistent with training instances. 5.3. Ablation on Prompt Token Generators One of our technical novelties is the parameter-efﬁcient design of the prompt token generator as in Fig. 3b. We pro- vide in-depth study on different prompt token generators. Tab. 3 summarizes results. The key takeaway is that the performance, measured in FIDs, for models using prompts with the proposed factorization closely matches those us- ing the baseline, non-factorized prompts. This is particu- larly true for NAR transformers. On the other hand, AR transformers still prefers prompt generators with more pa- rameters. Nevertheless, we achieve on par results with the baseline using less than 30% of parameters. 9(a) Oxford Flowers, “Grape hyacinth” (S = 1)  (b) Oxford Flowers, “Grape hyacinth” (S = 128) Figure 11. Instance-conditioned generation. For each row, leftmost image in red box is a training image and next ﬁve images are gener- ated. When instance conditioned, generated images follow ﬁner-grained details of the reference training image, such as color, shape, or background, beyond class information. Adaptation and diversity could be further controlled by the prompt length. 101 102 103 Train epochs 20 30 40 50 60 70 80 90 100FID prompt scratch adapter finetune (a) VTAB small (<10k) 101 102 103 Train epochs 20 30 40 50 60 70 80 90 100FID prompt scratch adapter finetune (b) VTAB medium (<100k) 101 102 103 Train epochs 20 30 40 50 60 70 80 90 100FID prompt scratch adapter finetune (c) VTAB large (>100k) Figure 12. FID vs the number of train epochs for various learning methods for transformer-based sequence models. Knowledge transfer is essential for faster convergence when training data is small. 5.4. Beyond Prompt Tuning for Generative Transfer We have studied applying a prompt tuning to learn gen- erative vision transformers via knowledge transfer. We have seen promising results,e.g., excelling state-of-the-art GAN- based transfer learning methods at generative modeling. We also demonstrate the importance of knowledge transfer for fast and efﬁcient learning of generative models from small training data. Despite the success, prompt tuning is not the only method for learning transformer-based sequence mod- els. For the completeness, we conduct an extended study for various learning methods of generative vision transformers. To that end, we evaluate adapter tuning and ﬁne-tuning in addition to the prompt tuning and learning from scratch. Adapter tuning [25] introduces learnable adapter modules to each transformer block. Fine-tuning unfreezes pretrained weights and updates them. All models are trained using the same loss (e.g., masked visual token model loss [6] for NAR transformer). As we are interested in class-conditional gen- erative models, we also introduce class-conditional prompts of length 1 that are randomly initialized for adapter tuning and ﬁne-tuning. For experiments, we vary the number of training epochs from 10 to 3200,5 as training efﬁciency is one of the key dif- ferentiating factors across various learning strategies. For prompt tuning, we use 128 prompt tokens with a single fac- tor. For adapter tuning, we use 64 hidden units for adapter modules. We report the number of trainable parameters as- suming 100 classes, train time per step and generation time comparisons in Tab. 4. Prompt tuning shows the best pa- rameter and train time efﬁciency, where the number of train- able parameters is less than 0.5% of those of ﬁne-tuning and learning from scratch. On the other hand, due to the longer sequence, it takes more time for generation than those mod- els with a single class token. Adapter tuning, together with a tunable class-conditional prompt, turns out to be a method with a good balance, with relatively few trainable parame- ters and efﬁciency at both train and test time. Fig. 12 compares the generation performance in FID on VTAB. We see that models with a knowledge transfer con- verge faster than the ones without a transfer. For example, it requires almost 800 epochs for models learned from scratch to reach FIDs of the prompt tuning models trained for 10 epochs for tasks with a small data. Fine-tuning also adapts 5We limit the maximum number of training steps to 500K to ﬁnish model training within a reasonable time window. 10# params train / step generation Prompt tuning (S= 128) 0.76M 1× 1× Adapter tuning 5.43M 1.04× 0.84× Fine-tuning, Scratch 172M 1.67× 0.80× Table 4. Qualitative comparison (e.g., number of trainable param- eters, train and generation time) among various learning strategies based on NAR transformers. to new data distributions quickly, though it takes more time per step for model training. As in Fig. 12a, for tasks with a small training data, ﬁne-tuning shows the best FIDs. On the other hand, we ﬁnd that ﬁne-tuning behaves unstable on some datasets (e.g., smallnorb), and the performance di- verges as training goes, as in Fig. 12b. Complete FID results are in Tab. 7 of Appendix. To our surprise, learning from scratch performs well even for tasks with a small training data when given sufﬁcient compute resources and time. Finally, we’d like to note that there is no single method that wins against the rest as each method has its own advan- tage. For example, for applications where the small number of parameter is critical, prompt tuning should be preferred despite slightly worse generation quality. Also, prompt and adapter tuning are preferred when there are many datasets and tasks as transformer parameters are shared across tasks. 6. Related Work Transfer learning [43,55,66,75] is a method for improv- ing the performance of downstream tasks using knowledge from the source domain and task. It is shown to be partic- ularly effective when the amount of training data is limited for downstream tasks. Knowledge transfer of deep neural networks has been realized in various forms, such as linear probing [8, 23], ﬁne-tuning [31, 46], or adapter [25, 49, 50]. Recently, prompt tuning [34, 36, 37] has emerged as a pow- erful tool for transfer learning of transformer-based large language models in NLP. Since the introduction of Vision Transformer [13], such approach has been studied for vision tasks as well [1, 26]. While previous works have shown ef- fectiveness of prompt tuning for discriminative tasks (e.g., classiﬁcation), we apply the technique for image synthesis. Generative models have been extensively studied for im- age synthesis, including variational autoencoder [30,57,59], diffusion [11, 51] and autoregressive [44, 58, 62] models. A large volume of progress has been made around the genera- tive adversarial network (GAN) [18] thanks to its ability at synthesizing high-ﬁdelity images [2, 27, 28, 52]. As such, generative knowledge transfer has been studied to transfer knowledge of pretrained GAN models. TransferGAN [65], following a usual practice by ﬁne-tune on the target dataset, has demonstrated transferring knowledge from pretraining improves the performance when training with limited data. Freezing a few layers of discriminator [40] further improves while stabilizing the training process. MineGAN [64] intro- duces a miner, which projects random noise into the embed- ding space of the pretrained generator, and trains it with dis- criminator while ﬁxing generator parameters. cGANTrans- fer [53] makes explicit transfer of knowledge on classes of the source dataset to new classes. Albeit showing improve- ment, these methods still require careful training (e.g., early stopping) and have evaluated on a few datasets. In our work, we extensively test methods on a wide variety of visual do- mains (e.g., VTAB) and show improvement by a large mar- gin over existing GAN-based generative transfer methods. 7. Conclusion We present a method for learning image generation mod- els from diverse data distributions and varying amount of training data via knowledge transfer from the source model trained on a large dataset. A simple modiﬁcation on prompt token designs allows to learn a parameter and compute efﬁ- cient class and instance conditional image generation mod- els of autoregressive and non-autoregressive vision trans- formers. Comprehensive experimental results of image syn- thesis are provided across diverse visual domains, tasks, and the number of training images. In addition, we show how to use learned prompts for novel image synthesis in the form of marquee header prompts, which is particularly useful when synthesizing images using generative models learned from a few images. Acknowledgment We thank Brian Lester for helpful discussion on prompt tun- ing, Boqing Gong and David Salesin for their feedback on the manuscript. 11References [1] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Visual prompting: Modifying pixel space to adapt pre-trained models. arXiv preprint arXiv:2203.17274, 2022. 1, 3, 11 [2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural image synthesis. In International Conference on Learning Representations , 2018. 1, 6, 11 [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand- hini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020. 2 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural in- formation processing systems, 33:1877–1901, 2020. 3 [5] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal Drozdzal, and Adriana Romero Soriano. Instance- conditioned gan. Advances in Neural Information Process- ing Systems, 34:27517–27529, 2021. 4 [6] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. arXiv preprint arXiv:2202.04200, 2022. 1, 2, 3, 4, 6, 7, 10 [7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee- woo Jun, David Luan, and Ilya Sutskever. Generative pre- training from pixels. In International Conference on Ma- chine Learning, pages 1691–1703. PMLR, 2020. 2 [8] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classi- ﬁcation. In International Conference on Learning Represen- tations, 2018. 11 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 7 [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 2 [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Informa- tion Processing Systems, 34, 2021. 1, 11 [12] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image gen- eration via transformers. Advances in Neural Information Processing Systems, 34:19822–19835, 2021. 1, 2 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 11 [14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12873–12883, 2021. 1, 2, 3, 4, 6 [15] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. In EMNLP-IJCNLP, 2019. 2 [16] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter- national conference on computer vision , pages 1440–1448, 2015. 1 [17] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE con- ference on computer vision and pattern recognition , pages 580–587, 2014. 1 [18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 1, 11 [19] Jiatao Gu and Xiang Kong. Fully non-autoregressive neural machine translation: Tricks of the trade. In Findings of ACL- IJCNLP, 2021. 2, 3 [20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition , pages 9729–9738, 2020. 1 [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 1, 9 [22] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Rit- ter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. 4, 15 [23] Olivier Henaff. Data-efﬁcient image recognition with con- trastive predictive coding. In International conference on machine learning, pages 4182–4192. PMLR, 2020. 11 [24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilib- rium. Advances in neural information processing systems , 30, 2017. 6 [25] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019. 3, 10, 11, 15 [26] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi- sual prompt tuning. arXiv preprint arXiv:2203.12119, 2022. 1, 3, 4, 11 12[27] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, pages 4401–4410, 2019. 11 [28] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improv- ing the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8110–8119, 2020. 11 [29] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014. 15 [30] Diederik P Kingma and Max Welling. Auto-encoding varia- tional bayes. arXiv preprint arXiv:1312.6114, 2013. 11 [31] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In European conference on computer vision , pages 491–507. Springer, 2020. 1, 3, 11, 15 [32] Xiang Kong, Lu Jiang, Huiwen Chang, Han Zhang, Yuan Hao, Haifeng Gong, and Irfan Essa. Blt: Bidirectional lay- out transformer for controllable layout generation. arXiv preprint arXiv:2112.05112, 2021. 2, 3 [33] Xiang Kong, Zhisong Zhang, and Eduard Hovy. Incorpo- rating a local translation mechanism into non-autoregressive translation. arXiv preprint arXiv:2011.06132, 2020. 2 [34] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. In Proceed- ings of the 2021 Conference on Empirical Methods in Natu- ral Language Processing, pages 3045–3059, 2021. 1, 2, 3, 4, 11 [35] Jose Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Im- proved masked image generation with token-critic. InECCV, 2022. 2, 3 [36] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimiz- ing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 1, 3, 4, 11 [37] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be com- parable to ﬁne-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. 11 [38] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas- tic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 15 [39] Tomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan Cer- nock`y, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, volume 2, pages 1045–1048. Makuhari, 2010. 2 [40] Sangwoo Mo, Minsu Cho, and Jinwoo Shin. Freeze the dis- criminator: a simple baseline for ﬁne-tuning gans. arXiv preprint arXiv:2002.10964, 2020. 11 [41] Xing Nie, Bolin Ni, Jianlong Chang, Gaomeng Meng, Chun- lei Huo, Zhaoxiang Zhang, Shiming Xiang, Qi Tian, and Chunhong Pan. Pro-tuning: Uniﬁed prompt tuning for vi- sion tasks. arXiv preprint arXiv:2207.14381, 2022. 3 [42] Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A Efros, Yong Jae Lee, Eli Shechtman, and Richard Zhang. Few-shot image generation via cross-domain correspondence. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10743–10752, 2021. 1, 8 [43] Sinno Jialin Pan and Qiang Yang. A survey on transfer learn- ing. IEEE Transactions on knowledge and data engineering, 22(10):1345–1359, 2009. 11 [44] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im- age transformer. In International conference on machine learning, pages 4055–4064. PMLR, 2018. 11 [45] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF inter- national conference on computer vision , pages 1406–1415, 2019. 7 [46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learn- ing with a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. 11 [47] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors, ICML, 2021. 1, 2 [48] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener- ating diverse high-ﬁdelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. 2 [49] Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters.Ad- vances in neural information processing systems , 30, 2017. 11 [50] Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Efﬁcient parametrization of multi-domain deep neural net- works. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8119–8127, 2018. 11 [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684–10695, 2022. 11 [52] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan- xl: Scaling stylegan to large diverse datasets. arXiv preprint arXiv:2202.00273, 1, 2022. 11 [53] Mohamad Shahbazi, Zhiwu Huang, Danda Pani Paudel, Ajad Chhatkuli, and Luc Van Gool. Efﬁcient conditional gan transfer with knowledge propagation across classes. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12167–12176, 2021. 1, 2, 6, 7, 11, 15 [54] Zhangzhang Si and Song-Chun Zhu. Learning hybrid image templates (hit) by information projection.IEEE Transactions on pattern analysis and machine intelligence , 34(7):1354– 1367, 2011. 2, 7 [55] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, and Chunfang Liu. A survey on deep transfer learning. In International conference on artiﬁcial neural net- works, pages 270–279. Springer, 2018. 11 13[56] Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang. Regularizing generative adversarial networks under limited data. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 7921–7931, 2021. 1, 7 [57] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical vari- ational autoencoder. Advances in Neural Information Pro- cessing Systems, 33:19667–19679, 2020. 11 [58] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image genera- tion with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. 11 [59] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information pro- cessing systems, 30, 2017. 11 [60] A ¨aron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors, NeurIPS, 2017. 1, 2 [61] Laurens Van der Maaten and Geoffrey Hinton. Visualiz- ing data using t-sne. Journal of machine learning research, 9(11), 2008. 9 [62] Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning , pages 1747–1756. PMLR, 2016. 1, 11 [63] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power.Advances in Neural Information Pro- cessing Systems, 32, 2019. 7 [64] Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, and Joost van de Weijer. Minegan: effective knowledge transfer from gans to target domains with few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9332–9341, 2020. 1, 2, 6, 7, 11, 15 [65] Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, and Bogdan Raducanu. Transferring gans: generating images from limited data. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 218–234, 2018. 11 [66] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journal of Big data, 3(1):1–40, 2016. 11 [67] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N \\” uwa: Visual synthesis pre-training for neural visual world creation. arXiv preprint arXiv:2111.12417, 2021. 1, 2 [68] Ceyuan Yang, Yujun Shen, Zhiyi Zhang, Yinghao Xu, Jia- peng Zhu, Zhirong Wu, and Bolei Zhou. One-shot generative domain adaptation. arXiv preprint arXiv:2111.09876, 2021. 1 [69] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. arXiv preprint arXiv:2110.04627, 2021. 2 [70] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun- jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin- fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres- sive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 1, 2 [71] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo- longa, Andre Susano Pinto, Maxim Neumann, Alexey Doso- vitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019. 1, 2, 5 [72] Zhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li, Ming Ding, Jie Tang, Jingren Zhou, and Hongxia Yang. M6- ufc: Unifying multi-modal controls for conditional image synthesis. arXiv preprint arXiv:2105.14211, 2021. 2 [73] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efﬁcient gan training. Advances in Neural Information Processing Sys- tems, 33:7559–7570, 2020. 1, 7 [74] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor- ralba, and Aude Oliva. Learning deep features for scene recognition using places database. Advances in neural in- formation processing systems, 27, 2014. 2, 7 [75] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43–76, 2020. 11 141 import flax.linen as nn 2 import jax.numpy as jnp 3 4 class TokenGenerator(nn.Module): 5 n_token: int # Number of token (S) 6 n_class: int # Number of class (C) 7 n_factor: int # Number of factors (F) 8 d_embed: int # Embed dimension (P) 9 d_token: int # Token dimension (D) 10 11 @nn.compact 12 def __call__(self, cls_ids: jnp.ndarray): 13 MLP_p = nn.Embed(self.n_token, [self.d_embed, self.n_factor]) 14 MLP_c = nn.Embed(self.n_class, [self.d_embed, self.n_factor]) 15 MLP_t = nn.Dense(self.d_token) 16 MLP_f = nn.Embed(1, self.n_factor) 17 18 pos_ids = jnp.arange(self.n_token) 19 factor_ids = jnp.arange(1)[None, None, ...] 20 pos_embed = MLP_p(pos_ids[None, ...]) # 1 x S x P x F 21 cls_embed = MLP_c(cls_ids[..., None]) # B x 1 x P x F 22 fac_embed = MLP_f([None, None, ...]) # 1 x 1 x 1 x F 23 embed = (fac_embed * (pos_embed + cls_embed)).sum(-1) 24 return MLP_t(nn.LayerNorm(embed)) Figure 13. An example code for the token generator in Flax-ish [22] format. A. Pseudo-code for Token Generator In Fig. 13 we provide an example code that implements the prompt token generator in Flax [22] format. B. Comprehensive Experiment Description B.1. Visual Task Adaptation Benchmark (VTAB) B.1.1 Dataset Meta Information of Visual Task Adaptation Benchmark In Tab. 5 we provide a dataset meta information, including the number of class and the number of images in each data split, of VTAB. B.1.2 Hyperparameters We provide hyperparameters used in our experiments in Tab. 6. Note that most hyperparameters are shared across datasets, except the number of training epochs. We use Adam optimizer [29] with a cosine learning rate decay [38]. When learning models from scratch, we ﬁnd that learning rate warm-up is essential. To this end, we use a warm-up for the ﬁrst two epochs for AR models, and 80 train epochs for NAR transformers. B.1.3 Experimental Results We provide complete results in Tab. 7 for autoregressive transformers, non-autoregressive transformers as well as GAN- based generative model transfer learning methods including MineGAN [64] and cGANTransfer [53]. For AR and NAR transformers, we report FIDs for prompt tuning, learning from scratch, as well as different transfer learning techniques including adapter [25] and ﬁne-tuning [31]. B.1.4 Visualization of Generated Images We visualize images generated by the models trained on each of VTAB tasks from Fig. 14 to Fig. 29. 15Dataset # class train val test all Caltech-101 102 2754 306 6084 9144 CIFAR-100 100 45000 5000 10000 60000 SUN397 397 76128 10875 21750 108753 SVHN 10 65931 7326 26032 99289 Flowers102 102 1020 1020 6149 8189 Pet 37 2944 736 3669 7349 DTD 47 1880 1880 1880 5640 EuroSAT 10 16200 5400 5400 27000 Resisc45 45 18900 6300 6300 31500 Patch Camelyon 2 262144 32768 32768 327680 Diabetic Retinopathy 5 35126 10906 42670 88702 Kitti 4 6347 423 711 7481 Smallnorb (azimuth) 18 24300 12150 12150 48600 Smallnorb (elevation) 9 24300 12150 12150 48600 Dsprites (x position) 16 589824 73728 73728 737280 Dsprites (orientation) 16 589824 73728 73728 737280 Clevr (object distance) 6 63000 7000 15000 85000 Clevr (count) 8 63000 7000 15000 85000 DMLab 6 65550 22628 22735 110913 Mean 49.5 102851.2 15332.8 20416.0 138600.0 Table 5. Dataset meta information (e.g., number of images, number of class) for tasks in VTAB. AR AR AR AR NAR NAR NAR NAR scratch + Prompt + Adapter + Fine-tune scratch + Prompt + Adapter + Fine-tune Learning rate 0.0005 0.001 0.001 0.0005 0.0001 0.001 0.001 0.001 / 0.0001 Batch size 128 256 256 128 128 256 256 128 Weight decay 0.045 0 0 0.045 0.045 0 0 0.045 Warmup epochs 2 0 0 0 80 0 0 0 Table 6. Hyperparameter used for experiments. For NAR + Fine-tune, we use the learning rate of 0.001 for new model parameters ( e.g., prompt) while using 0.0001 for pretrained ones (e.g., transformer). The same hyperparameter is used across all datasets and scenarios. 16Models Caltech101CIFAR100SUN397SVHNFlower Pet DTD EuroSATResisc45PC DR Kitti MineGAN 102.4 82.6 77.5 144.7 132.1 130.1 87.4 111.5 81.0 170.3192.2117.9cGANTransfer 89.6 31.4 31.1 64.7 61.6 48.6 70.3 45.6 50.3 119.9149.8 48.9 NAR Scratch 72.7 24.2 9.2 44.4 57.2 70.3 66.1 39.5 32.0 48.3 25.6 33.8Scratch (3200 ep.)14.5 22.5 7.3 43.5 14.9 8.5 29.2 26.4 24.2 51.1 26.0 26.1P (S=1) 13.4 26.9 7.2 83.0 13.8 11.8 25.7 45.9 28.7 107.9 84.2 32.2P (S=16) 12.7 25.5 7.3 80.8 13.2 11.0 26.0 35.8 25.1 71.0 34.2 30.0P (S=128) 12.9 25.0 7.7 62.3 13.4 10.9 25.9 38.4 24.8 67.4 30.8 29.9P (S=128,F=16) 11.8 25.0 7.5 63.4 13.3 11.5 26.0 35.8 24.3 61.4 29.2 27.0 P†(S=16) 12.4 25.3 7.3 72.5 12.7 11.2 25.4 36.9 23.7 71.7 34.3 31.2 P†(S=128) 12.2 25.2 7.5 60.4 12.3 11.0 25.7 35.4 24.3 71.7 28.2 29.6Adapter 11.3 20.3 6.7 43.7 11.0 6.9 25.1 28.2 19.9 46.4 24.9 24.0Fine-tune 11.3 18.2 6.5 43.9 10.2 6.3 24.2 23.1 18.2 48.0 24.4 22.8 AR Scratch 76.1 27.1 13.5 31.2 56.1 52.5 92.7 19.4 29.5 32.9 37.0 31.6Scratch (3200 ep.)30.5 25.8 14.4 27.9 24.3 28.1 45.1 15.5 11.5 32.3 37.7 33.2P (S=1) 45.4 25.7 18.8 80.4 28.9 42.2 37.1 37.3 35.1 74.9 93.1 66.8P (S=16) 41.4 22.5 16.4 55.5 19.6 36.6 33.4 32.6 28.8 49.8 60.7 41.3P (S=256) 39.6 19.8 15.0 44.0 17.3 34.9 32.5 29.6 26.7 44.0 45.4 37.1P (S=256,F=16) 27.2 17.6 12.8 42.8 14.1 27.2 30.0 26.4 22.2 44.3 45.4 34.6 P†(S=16) 30.9 19.4 13.7 53.7 15.4 30.8 30.8 30.2 25.7 49.0 60.4 39.7 P†(S=256) 24.6 17.5 12.3 43.1 13.7 25.1 29.8 26.7 20.9 43.6 46.1 35.1Adapter 27.0 16.7 12.6 29.9 11.8 19.1 30.8 22.4 22.0 39.4 37.3 29.0Fine-tune 17.6 13.2 9.1 27.7 17.7 10.7 35.4 15.1 11.6 30.9 34.5 29.6 Models SNorbA SNorbB Dspr.A Dspr.B ClevrA ClevrB DMLabMean≤10K ≤100K ≥100K NaturalSpecial.Struct. MineGAN 160.4 161.1 252.7 285.1 212.1 225.6 152.4 151.5 114.0 145.6 236.0 108.1 138.7 195.9 cGANTransfer 93.3 90.5 133.7 165.4 109.4 115.0 98.8 85.1 63.8 80.0 139.7 56.8 91.4 106.9 NAR Scratch 31.4 32.9 87.5 89.0 12.5 13.3 20.6 42.7 60.0 26.0 75.0 49.2 36.4 40.1 Scratch (3200 ep.)29.4 30.5 90.1 88.3 13.7 13.5 19.6 30.5 18.6 23.3 76.5 20.1 31.9 38.9 P (S=1) 58.6 58.7 119.5 121.3 58.5 57.9 64.4 53.7 19.4 52.2 116.2 26.0 66.7 71.4 P (S=16) 46.1 42.8 98.7 98.8 27.3 28.2 43.4 39.9 18.6 36.1 89.5 25.2 41.5 51.9 P (S=128) 33.6 35.2 100.9 92.8 21.9 23.6 33.5 36.4 18.6 30.6 87.0 22.6 40.3 46.4 P (S=128,F=16) 36.0 36.1 98.7 99.3 25.6 24.1 32.0 36.2 17.9 30.8 86.5 22.6 37.7 47.4 P†(S=16) 44.1 44.7 96.5 99.0 26.0 27.1 38.9 39.0 18.6 34.6 89.1 23.8 41.7 50.9 P†(S=128) 34.6 38.4 92.2 95.4 24.7 27.5 32.9 36.3 18.2 30.8 86.4 22.0 39.9 46.9 Adapter 29.2 28.7 85.7 86.9 14.6 15.0 20.0 28.9 15.7 22.9 73.0 17.9 29.9 38.0 Fine-tune 67.2 51.3 86.5 88.0 20.6 19.7 23.4 32.3 15.0 28.8 74.1 17.2 28.4 47.4 AR Scratch 23.1 23.4 76.5 76.6 12.3 12.2 27.8 39.6 61.8 23.3 62.0 49.9 29.7 35.4 Scratch (3200 ep.)23.4 23.3 76.5 75.1 12.1 11.4 25.5 30.2 32.2 20.8 61.3 28.0 24.3 35.1 P (S=1) 62.2 62.0 215.9 214.0 90.6 91.6 69.0 73.2 44.1 60.5 168.3 39.8 60.1 109.0 P (S=16) 52.9 52.6 102.3 99.8 51.0 49.8 53.6 47.4 34.5 43.3 83.9 32.2 42.9 62.9 P (S=256) 42.4 42.3 83.7 83.7 29.5 28.9 45.2 39.0 32.3 33.5 70.5 29.0 36.4 49.1 P (S=256,F=16) 43.4 42.7 83.8 81.6 30.2 29.0 45.9 36.9 26.6 32.6 69.9 24.5 34.6 48.9 P†(S=16) 51.3 52.2 100.3 97.3 49.6 49.0 54.0 44.9 29.5 41.7 82.2 27.8 41.3 61.7 P†(S=256) 43.5 43.5 86.9 84.3 30.4 29.8 45.7 37.0 25.7 32.7 71.6 23.7 34.3 49.9 Adapter 36.0 36.3 77.8 77.9 15.5 14.9 29.6 30.8 23.5 24.8 65.1 21.1 30.3 39.6 Fine-tune 23.2 23.2 76.8 77.2 11.8 11.5 25.6 26.4 22.2 18.8 61.6 18.8 23.0 34.9 Table 7. FIDs on VTAB tasks tested with various models. We use the “all” set as a reference set for computing FIDs. Unless otherwise stated, all NAR models are trained for 200 epochs and AR models are trained for 400 epochs with the same hyperparameter settings speciﬁed in Tab. 6. “P” refers to the prompt tuning with the sequence length S and the number of factors F. “DTD”: Describable Tex- tures Dataset, “PC”: Patch Camelyon, “DR”: Diabetic Retinopathy, “SNorbA”: SmallNorb (azimuth), “SNorbB”: SmallNorb (elevation), “DsprA”: Dsprites (x position), “DsprB”: Dsprites (orientation), “ClevrA”: Clevr (object distance), “ClevrB”: Clevr (count). 17(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 14. Visualization of generated images with different models on Caltech101 of VTAB. 18(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 15. Visualization of generated images with different models on CIFAR100 of VTAB. 19(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 16. Visualization of generated images with different models on SUN397 of VTAB. 20(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 17. Visualization of generated images with different models on SVHN of VTAB. 21(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 18. Visualization of generated images with different models on Oxford Flowers102 of VTAB. 22(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 19. Visualization of generated images with different models on Oxford iiit Pet of VTAB. 23(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 20. Visualization of generated images with different models on DTD of VTAB. 24(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 21. Visualization of generated images with different models on EuroSAT of VTAB. 25(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 22. Visualization of generated images with different models on Resisc45 of VTAB. 26(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 23. Visualization of generated images with different models on Patch Camelyon of VTAB. 27(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 24. Visualization of generated images with different models on Diabetic Retinopathy of VTAB. 28(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 25. Visualization of generated images with different models on Kitti of VTAB. 29(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 26. Visualization of generated images with different models on Smallnorb of VTAB. 30(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 27. Visualization of generated images with different models on Dsprites of VTAB. 31(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 28. Visualization of generated images with different models on Clevr of VTAB. 32(a) MineGAN  (b) cGANTransfer (c) AR transformer with prompt tuning (S = 1)  (d) AR transformer with prompt tuning (S = 256, F = 16) (e) NAR transformer with prompt tuning (S = 1)  (f) NAR transformer with prompt tuning (S = 128) Figure 29. Visualization of generated images with different models on DMLab of VTAB. 33(a) Places, 5-shot, Left: real, Right: generation. (b) Places, 500-shot, All generation, without cherry-picking. Figure 30. Fewshot generation on places. B.2. Few-shot Generative Transfer B.2.1 Visualization of Generated Images 34(a) ImageNet, 5-shot, Left: real, Right: generation. (b) ImageNet, 500-shot, All generation, without cherry-picking. Figure 31. Fewshot generation on ImageNet. 35(a) Animal Face, 5-shot, Left: real, Right: generation. (b) Animal Face, 100-shot, All generation, without cherry-picking. Figure 32. Fewshot generation on Animal Face. 36",
      "meta_data": {
        "arxiv_id": "2210.00990v1",
        "authors": [
          "Kihyuk Sohn",
          "Yuan Hao",
          "José Lezama",
          "Luisa Polania",
          "Huiwen Chang",
          "Han Zhang",
          "Irfan Essa",
          "Lu Jiang"
        ],
        "published_date": "2022-10-03T14:56:05Z",
        "pdf_url": "https://arxiv.org/pdf/2210.00990v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This research introduces a novel framework for generative visual transfer learning tailored for vision transformers using prompt tuning. The key contributions include: (1) proposing a parameter-efficient prompt token generator design that effectively incorporates class and instance conditioning for controllable image synthesis; (2) developing a \"Marquee Header\" prompt engineering method to enhance generation diversity during the iterative decoding process; (3) conducting a large-scale empirical study on diverse visual domains (VTAB) and few-shot scenarios, demonstrating state-of-the-art image generation performance that significantly surpasses existing GAN-based transfer methods; and (4) providing the first substantial empirical evidence for the necessity of knowledge transfer in data and compute efficient generative image modeling using prompt tuning for image synthesis.",
        "methodology": "The framework utilizes state-of-the-art generative vision transformers (both Autoregressive (AR) like Taming Transformer and Non-Autoregressive (NAR) like MaskGIT) pretrained on a large dataset. Images are first quantized into discrete visual tokens by a Vector-Quantized (VQ) autoencoder. To adapt to new domains, prompt tuning is employed, where learnable prompt tokens are prepended to the visual token sequence, keeping the transformer parameters frozen. The novel prompt token generator design factorizes class/instance conditions and token positions using MLPs to significantly reduce the number of learnable parameters. For enhancing generation diversity, a \"Marquee Header\" prompt strategy is introduced, which interpolates learned prompt representations across multiple iterative decoding steps. Prompt parameters are optimized using gradient descent with standard AR or MaskGIT loss functions.",
        "experimental_setup": "The method was evaluated on a comprehensive Visual Task Adaptation Benchmark (VTAB) consisting of 19 visual recognition tasks from 16 diverse datasets (natural, structured, specialized). For few-shot generative transfer, datasets like Places, ImageNet, and Animal Face were used, along with challenging off-manifold tasks such as DomainNet (Infograph, Clipart) and ImageNet Sketch, with training data as low as 2 images per class. Source models were 256x256 ImageNet-pretrained Taming Transformer (AR) and MaskGIT (NAR). Baselines included GAN-based generative transfer methods (MineGAN, cGANTransfer), generative transformers trained from scratch, and few-shot generation models (DiffAug, LeCam GAN). The primary evaluation metric was Frechet Inception Distance (FID), computed from 20k or 10k generated images, and intra-cluster pairwise LPIPS distance for diversity.",
        "limitations": "The study identified several limitations: (1) using longer prompts, while improving fidelity, leads to an increased generation time (e.g., 25% for 128 tokens); (2) Autoregressive transformers generally require prompts with more learnable parameters (higher F factors) compared to Non-Autoregressive transformers, although still more efficient than non-factorized baselines; (3) long prompts are susceptible to overfitting when the number of training images is extremely small (e.g., less than 10 images per class for ImageNet and Places); (4) fine-tuning, despite its rapid adaptation, can exhibit instability on some datasets (e.g., smallnorb) with performance diverging over training; and (5) the paper acknowledges that no single learning method (prompt tuning, adapter, fine-tuning, scratch) is universally superior, with the optimal choice depending on specific application priorities such as parameter efficiency versus generation quality.",
        "future_research_directions": "The paper suggests two main directions for future research: (1) investigating various other formulations for the \"Marquee Header\" prompt to further explore and optimize the trade-off between generation diversity and fidelity; and (2) extending the proposed parameter-efficient factorization for the prompt token generator to incorporate the 'depth' position from deep visual prompts, aiming for even greater parameter reduction."
      }
    },
    {
      "title": "À-La-Carte Prompt Tuning (APT): Combining Distinct Data via Composable Prompting",
      "abstract": "We introduce \\`A-la-carte Prompt Tuning (APT), a transformer-based scheme to\ntune prompts on distinct data so that they can be arbitrarily composed at\ninference time. The individual prompts can be trained in isolation, possibly on\ndifferent devices, at different times, and on different distributions or\ndomains. Furthermore each prompt only contains information about the subset of\ndata it was exposed to during training. During inference, models can be\nassembled based on arbitrary selections of data sources, which we call\n\"\\`a-la-carte learning\". \\`A-la-carte learning enables constructing bespoke\nmodels specific to each user's individual access rights and preferences. We can\nadd or remove information from the model by simply adding or removing the\ncorresponding prompts without retraining from scratch. We demonstrate that\n\\`a-la-carte built models achieve accuracy within $5\\%$ of models trained on\nthe union of the respective sources, with comparable cost in terms of training\nand inference time. For the continual learning benchmarks Split CIFAR-100 and\nCORe50, we achieve state-of-the-art performance.",
      "full_text": "`A-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting Benjamin Bowman1,2* Alessandro Achille1 Luca Zancato1 Matthew Trager1 Pramuditha Perera1 Giovanni Paolini1 Stefano Soatto1 AWS AI Labs1 UCLA2 benbowman314@math.ucla.edu zancato@amazon.it {aachille,mttrager,pramudi,paoling,soattos}@amazon.com Abstract We introduce `A-la-carte Prompt Tuning (APT), a transformer-based scheme to tune prompts on distinct data so that they can be arbitrarily composed at inference time. The individual prompts can be trained in isolation, possibly on different devices, at different times, and on dif- ferent distributions or domains. Furthermore each prompt only contains information about the subset of data it was exposed to during training. During inference, models can be assembled based on arbitrary selections of data sources, which we call `a-la-carte learning. `A-la-carte learning enables constructing bespoke models speciﬁc to each user’s individual access rights and preferences. We can add or remove information from the model by simply adding or removing the corresponding prompts without retraining from scratch. We demonstrate that `a-la-carte built models achieve accuracy within 5% of models trained on the union of the respective sources, with comparable cost in terms of training and inference time. For the continual learning benchmarks Split CIFAR-100 and CORe50, we achieve state-of-the-art performance. 1. Introduction As large neural network models make their way into com- mercial applications, the basic paradigm of training them on a monolithic dataset leads to a number of challenges. First, as new data become available, updating the whole model can be prohibitively expensive. Even when training time is not an issue, some users may still require access and main- tenance of previous versions of the model to avoid disrup- tions of their downstream workﬂows. Second, owners of the training data may modify their sharing preferences at any time, leading to datasets that shrink over time (machine unlearning) or to different subsets of the training data being *Work done during an internship at AWS AI Labs. usable by different users (compartmentalization). Finally, the users themselves may want to use custom subsets of the data to better tailor their model to their use cases (model customization). These challenges are well known and addressed separately in different ﬁelds such as continual learning, forgetting, and model adaption. However, in order for a commercial sys- tem to be viable at scale, these issues have to be tackled concurrently. Ideally, one would have a large model that each user can run, trained using only data the speciﬁc user wants and has rights to, that can evolve without the need for ﬁne-tuning as new data becomes available, or as individ- ual data owners exercise their right to have their data erased (“the right to be forgotten”). We refer to the problem of building such a model as `a-la- carte learning since, depending on the data availability and the user, the service may need to select and use different data chunks from a menu of available training data. More speciﬁcally, let D= {D1,...,D n}be a variable collection of data sources (a data pool). In `a-la-carte learning a user at inference time can specify a subset S ⊂D of training data together with an input sample x to receive a personalized `a-la-carte output f(x,S) from the model f. Critically, the output f(x,S) must not depend on any data sourceDi /∈S. `A-la-carte learning can be na¨ıvely tackled in two ways. The service could pre-train one model for each possible sub- set of the data pool, and serve each user the most power- ful model they have rights to. While optimal from the user view-point, this requires a prohibitive exponential complex- ity O(2|D|) in both training time and storage. On the other extreme, the service could train a separate model on each data source individually and, at inference time, ensemble all models obtained from the sources in S. This requires only linear O(|D|) training time complexity to pre-train each model, but still has a signiﬁcant storage cost. Fur- thermore due to the ensembling inference time is signiﬁ- 1 arXiv:2302.07994v1  [cs.LG]  15 Feb 2023Figure 1. `A-la-carte Learning and APT. Given a pool of multiple data sources, the goal of `A-la-carte Learning is to allow the user to select – at inference time – an arbitrary subset S ⊂D of sources to use. The performance of the `a-la-carte model should be comparable to the performance of a model trained on S. (A) APT enables efﬁcient `A-la-carte Learning by converting each source into a prompt, and composing together the relevant prompts at inference time. (B) To perform inference, APT uses a modiﬁed attention mechanism that prevents the prompts from interfering with each other and ensembles the individual outputs to construct the ﬁnal prediction. cantly increased while also potentially suffering from lower performance than the ideal “paragon” model trained on the union of sources in S. The goal of `a-la-carte learning is to achieve performance as close as possible to the paragon without signiﬁcantly increasing inference or training time. To address these key issues, we propose`A-la-carte Prompt Tuning (APT) . APT leverages vision transformers and prompt tuning to solve the `a-la-carte learning problem. First, APT converts each dataset Di into a learned prompt pi, thus transforming the data pool into aprompt pool. Then at inference time, given a subset of sources S to use, APT retrieves all corresponding prompts and concatenates them together with the input. Surprisingly, we show that in most cases APT has performance comparable to the paragon of joint learning with all data in S. Moreover, since each prompt is trained on an individual dataset, information is naturally compartmentalized. Thanks to the small size of prompts and an efﬁcient forwarding method, APT is sig- niﬁcantly cheaper (in both storage and inference time) than ensembling models. Importantly however, we note that simply concatenating different prompts that were trained separately leads to de- structive interference in the attention block which corrupts the representations (see Table 2). To address this problem, we introduce a modiﬁed attention mechanism that elimi- nates such interference, while also signiﬁcantly reducing the inference time when multiple prompts are concatenated. A priori, this change comes with a small reduction in ex- pressive power and in the ability to capture synergistic in- formation between data sources. However, one of our main contributions is to show that the resulting drop in accuracy is generally modest, while providing far more valuable ben- eﬁts to scalability, maintainability, and privacy. We empirically demonstrate the advantage of APT-based`a- la-carte learning for forgetting and continual learning (both domain-incremental and class-incremental). We observe that in most cases the performance of APT is within 5% of the performance of the paragon at a fraction of the cost. We also show that APT outperforms all comparable base- lines with the advantage of computational scalability from the structured attention mechanism. Summary of our contributions. 1. We introduce the `A-la-carte Learning problem to ad- dress continual learning, machine unlearning, and model customization concurrently. 2. We propose APT, an efﬁcient method to address `A- la-carte Learning based on visual prompt tuning and a modiﬁed attention mechanism. 3. We demonstrate that for most tasks APT achieves ac- curacy within 5% of paragon performance even when each individual prompt has access to an order of mag- nitude less data 4. We show that APT with a simple prompt weighting mechanism achieves state-of-the-art performance on continual learning benchmarks Split CIFAR-100 and CORe50. 2. Related Work Prompt Tuning. Prompting originated from natural lan- guage processing by prepending “hard” language prompts to inputs to inform a pre-trained language model about the task to be solved [2,23]. It was then discovered that one can 2Concatenate Average APT 0% 100% 200% 300% Increase in error rel. to paragon Dataset Concatenate Average APTParagon MIT-67 84.6% 85.1% 86.2% 86.2% Cub-200 85.2% 84.6% 87.8% 86.6% Caltech-256 91.1% 87.9% 91.1% 91.7% Pets 93.8% 91.4% 93.1% 93.3% Aircrafts 56.5% 16.7% 61.1% 71.0% Flowers 84.5% 96.3% 99.3% 99.1% Stanford Cars 60.3% 26.1% 70.7% 81.2% Figure 2. Naive prompt composition vs. APT. We compare different methods of combining prompts. We split the training dataset into two equal sized shards then train prompts on each of the two shards in isolation. We then compare the test accuracies after combining the prompts using different methods. For the column “Concat” we concatenate the prompts without structured attention and average ensemble their predictions. For the column “Avg” we simply average the prompts and classiﬁer head as parameters and then take the single prediction. The column “APT” denotes our method. Numbers more than 10% below APT in each row are marked red; numbers more than 2% below APT are marked orange. The best method excluding the paragon in each row is marked in bold. optimize “soft” prompts in the embedding space in a dif- ferentiable fashion, with competitive performance to ﬁne- tuning [18, 21, 24]. This technique also proved useful when applied to vision transformers [15]. The idea of extending pre-trained transformers using prompt tokens with attention masking was introduced in [35]. We use the same attention masking scheme in our `a-la-carte learning implementation. The ensembling of soft and hard prompts was considered in [18] and [34] respectively. Continual Learning. Prompt tuning applied to the contin- ual learning problem has been considered in [5, 37, 38]. [5] augment a ﬁxed backbone with small task tokens that can be trained during episodes and added to the model incre- mentally. In [38] they query collections of prompts from a prompt pool on an instance-wise basis to be concate- nated at inference time. The query mechanism is supervised and consequently the compositionality of prompts is emer- gent from the supervision. By contrast, we select prompts from a pool on a per-user basis and achieve composability of prompts through structured attention. In [37] they ad- dress the domain incremental learning problem by training prompts independently on each domain and constructing a set of reference prototypes for the domain via K-means. At inference time, given an input xthey select the prompt according to the closest reference prototype to the embed- ding of the point x. In our APT Weight (APT-W) scheme (described in Sec. 6) rather than select a single prompt we weight the prompts according to the instance embedding’s distance to the closest prototype. Forgetting. Forgetting in deep networks [10, 11] is chal- lenging. [9] utilizes a ResNet-50 where they train a lin- earization of the network starting from a pre-trained check- point. Due to the linear parameterization, forgetting is much more tractable and they can get a bound on the mutual infor- mation after a certain number of forgetting steps. [14] offers forgetting for linear/logistic models, and [29] offer forget- ting techniques in the convex setting. [1] investigated train- ing distinct networks on separate shards of data. We run this same procedure to benchmark our APT approach. The novelty with the prompt tuning approach is that the memory overhead is minimal, and inference can be done at the cost of a single forward pass. 3. Preliminaries Vision Transformer. We use vision transformers [4] as our backbone architecture, due to both good accuracy on downstream tasks and ease of prompting. An image x ∈ RH×W×C is split into N patches x(1),...,x (N), which are represented as d-dimensional tokens z(i) = Ex(i) + e(i) pos ∈ Rd through a learned linear embedding E and a set of positional encodings {e(i) pos}N i=1. We add a special learn- able class token z(0) that is shared by all inputs. The in- put to the ﬁrst layer of the transformer is then given by z0 := [z(0),z(1),...,z (N)] which is the concatenation of the class token and the tokens corresponding to the image patches. Let Fℓ θ denote the ℓth attention layer of the trans- former, where θdenotes the parameters of the model. The output tokens of the ℓth layer are given by zℓ := Fℓ θ(zℓ−1). Let z(0) L be the output of the class token at the last trans- former layer. We use a linear head to output a probability distribution ˆyof the input’s label: ˆy:= softmax(headθ(z(0) L )) where headθ(x) = Wx + b is a learned fully connected layer. Visual Prompting. Like convolutional networks, pre- trained vision transformers can be adapted to new down- stream tasks by ﬁne-tuning their weights θ. However, 3prompting can also be used as an alternative adaptation mechanism for vision transformers [15, 35]. Let D be a supervised dataset for a downstream task. A new learnable prompt token p0 is attached to the transformer’s input, so that the ﬁnal output is given by [zL,pL] =FL θ ◦... ◦F1 θ([z0,p0]). To predict the downstream task label, the head of the pre- trained model is discarded to be replaced by a new head which is trained on the ﬁnal prompt token ˆy= softmax(head(pL)). Both p0 and head are trained on D, while the parameters θ of the pre-trained backbone are frozen. Notation. We denote with ℓ(ˆy,y) the cross entropy loss, and for a natural number k ∈N we let [k] := {1,...,k }. We consider a classiﬁcation task where Xis the input do- main and Yis the label space. 4. `A-la-carte Prompt Tuning Suppose we have a pre-trained backbone fθ and a pool of additional data sources D:= {D1,...,D n}. We focus in particular on the case where all sources in Dpertain to the same task and share the input and label spaceDi ⊂X×Y .1 Ideally we would like to ﬁne-tune the backbone using all data in Dby minimizing the loss: LD(θ) = ∑ (x,y)∈⋃D ℓ(y,fθ(x)). However, it is often the case (see Section 5) that the collec- tion of data sources Dchanges over time as data is added or removed. It may also be the case that different users of the model may want to use different subsets of the data to better cover their use cases (model customization) or may only have access rights to certain subsets of the data (com- partmentalization). `A-la-carte Learning. To remedy this, at inference time , given any subset I ⊂[n] we would like to be able to use a model that uses data exclusively from DI := ⋃ i∈I Di. A trivial option is to ﬁne-tune in advance the parametersθI on each possible subset I minimizing the loss LDI (θI) := ∑ (x,y)∈DI ℓ(f(x; θI),y) and, given I at inference, select the corresponding θI and use it to form the model f(x; θI). However, since there 1We do not however need to assume that all sources contain samples from all classes. The backbone fθ may be pre-trained on the same task as D (in which case D provides additional data to tune the model) or may be pre-trained on an unrelated proxy task (e.g., ImageNet or web-scale data). are 2n possible subsets I ⊂[n] it is prohibitively expen- sive to ﬁne-tune a separate model for each I, both from a compute-time and storage cost perspective. It would also require training 2n new models each time a source of data is added, which becomes infeasible quickly. Na¨ıve `A-la-carte Prompt Tuning. To reduce the compu- tational cost while satisfying all requirements of `A-la-carte Learning, we suggest an alternative strategy based on com- position of prompts trained on individual data sources. For each i∈[n] we train a prompt pi and classiﬁer head headi on the data Di using the loss function LDi(p(i),headi) := ∑ (x,y)∈Di ℓ(f(x; p(i)),y) where the dependence of f(x; p(i)) on headi above has been suppressed for ease of notation. Given a set of indices I = {i1,...,i |I|}we denote with p(I) = [p(i1),...,p (i|I|)] the concatenation of all prompt tokens corresponding to each data source in DI. The ﬁnal output of the transformer is given by [zL,p(I) L ] :=FL θ ◦... ◦F1 θ([z0,p(I)]) where θ are the frozen parameters of the backbone trans- former. Each output token p(i) L corresponding to a prompt p(i) can be used to generate a prediction ˆy(i) := softmax(headi(p(i) L )). The ﬁnal prediction is made by ensembling the predictions made by each individual prompt p(i) (see also Figure 1): ˆyI := 1 |I| ∑ i∈I ˆy(i). Since each prompt only contains information about its own source, the model output ˆyI depends only on the sources in DI. Moreover, after the initial cost O(|D|) to train each prompt pi, any subsetIof sources can be combined at infer- ence time with constant cost O(1). Hence, this procedure satisﬁes the requirements for `a-la-carte learning. However, in Figure 2 we see that the performance of this na¨ıve implementation of `a-la-carte prompt tuning by con- catenating prompts severely underperforms the paragon of using a single prompt trained from scratch on all the datasets in DI. The same is true for other composition mechanisms, such as averaging prompts. We hypothesise that this is due to the prompts, which were trained individ- ually, corrupting the representations at inference time when concatenated due to destructive interference in the attention mechanism of the transformer. Structured Attention. To remedy this we follow the tech- nique in [35]. First, we mask the attention so that the zℓ 40 5 10 15 20 num. of shards 0% 5% 10% 15% 20% 25% 30% 35%Increase in Error A Sharding 0 5 10 15 20 num. of forget requests 0% 10% 20% 30% 40% 50% B Forgetting 2 3 5 10 15 20 num. of shards 0% 10% 20% 30% 40% 50% 60% C Ensembling Gain Aircrafts Caltech-256 Cub-200 MIT-67 Oxford Flowers Pets Stanford Cars Figure 3. (A) Error increase of APT compared to paragon. We split a training set into a varying number of equal sized shards chosen uniformly at random. We then use APT to combine prompts learned individually on each shard, and measure the increase in error compared to the paragon of training on all data together. For most datasets, the performance of the APT is within a few percent of the paragon, even when the dataset is split in up to 20 parts. Aircrafts and Stanford Cars are the main exceptions, possibly due to the large domain shift between the backbone pretraining and those tasks. (B) Satisfying forgetting requests. We simulate a sequence of data removal requests starting from a pool of 20 sources and removing one source at the time. We report the increase in error compared to using the full data. We see that APT degrades gracefully as desired, while also ensuring perfect data removal.(C) Gain of using ensembles instead of individual prompts. We split a train set in a varying number of shards, and show the difference between the accuracy of APT prompt composition and the average accuracy of the individual prompts. For large number of shards, individual prompts don’t have enough information to classify accurately but APT can combine them to create a much stronger classiﬁer (with up to 60% better accuracy). tokens do not attend to the prompts, and the prompts do not attend each other (see Figure 4). This ensures the re- sult of forwarding each prompt p(i) through the network is unaffected by the presence of the other prompts. However, this reduces the power of the prompts to modify the forward pass of the network. To compensate, at each layer ℓof the transformer and for each prompt p(i) we add a set of dmem learnable memory tokens m(i) ℓ ∈Rdmem×d. These memory tokens can be attended by the prompts but cannot attend to anything. While a similar result could be obtained by us- ing longer prompts instead of using memory tokens, [35] notes that this solution gives comparable accuracy with a signiﬁcantly reduced inference time. Due to the structured attention, a single forward pass of the backbone transformer can be performed on z0 independent of the prompts. Sub- sequently at each layer l each prompt p(i) ℓ can perform cross attention to query itself and [zℓ,m(i) ℓ ]. While self- attention has quadratic complexity in the sequence length, this implementation has O(N2 + (N + dmem)|I|) com- plexity as opposed to O((N + |I|)2) complexity for self- attention without memory. Consistent with [35] in our im- plementation we set dmem = 5. Consequently N2 ≫ (N+dmem), and thus adding a prompt, and thus increasing |I|, only marginally increases inference time relative to the ﬁxed cost of a forward pass for the backbone transformer O(N2). By contrast classic model ensembling would have inference cost O(|I|N2) as one must do a forward pass through each model. Furthermore each prompt corresponds to 12 ×dmem + 1tokens, which amounts to a number of parameters less than .06% of the backbone model. Thus the memory overhead of storing the prompts is also marginal. zℓ p(1) ℓ p(2) ℓ p(3) ℓ m(1) ℓ m(2) ℓ m(3) ℓ zℓ \u0013 \u0017 \u0017 \u0017 \u0017 \u0017 \u0017 p(1) ℓ \u0013 \u0013 \u0017 \u0017 \u0013 \u0017 \u0017 p(2) ℓ \u0013 \u0017 \u0013 \u0017 \u0017 \u0013 \u0017 p(3) ℓ \u0013 \u0017 \u0017 \u0013 \u0017 \u0017 \u0013 Figure 4. Attention Masking Table. The rows correspond to queries and the columns correspond to keys. The cells marked with \u0013 denote where attention is performed and the cells marked \u0017 denote where attention is masked. `A-la-carte Prompt Tuning. Our ﬁnal proposal for efﬁcient `A-la-carte Learning, which we call `A-la-carte Prompt Tun- ing (APT), combines the composition of individual prompts with the structured attention mechanism. In Figure 2 we see that APT outperforms the na ¨ıve baselines in almost all cases, and importantly it not prone to the same catastrophic failures (e.g. on Aircrafts and Stanford Cars). Moreover its performance is close or better 2 than the paragon per- formance (training a prompt directly on the union of all datasets) on all datasets except Aircrafts and Stanford Cars. In the following, we explore particularly interesting appli- cations of `A-la-carte learning, and we empirically test the performance of APT in different settings. 2The results better than the paragon can be attributed to the regulariza- tion effect of ensembling prompts trained on different subsets of the data. 5Dataset No Sharding 2 Shards 3 Shards 5 Shards 10 Shards 15 Shards 20 Shards Head-only (in-domain) 90.8% 90.8% 90.8% 90.4% 90.1% 89.5% 88.5% APT (in-domain) 91.4% 91.5% 91.3% 91.4% 91.0% 90.6% 90.0% Head-only (out-of-domain) 59.7% 56.5% 53.5% 50.5% 45.0% 41.6% 40.5% APT (out-of-domain) 76.1% 65.9% 63.4% 57.9% 51.0% 46.8% 45.6% Table 1. Head-only ensembling vs. APT. We compare the performance of APT to ensembling classiﬁer heads (without prompts) trained on distinct shards chosen uniformly at random. We group the datasets MIT-67, Cub-200, Caltech-256, Pets, and Flowers as “in-domain” due to their alignment with ImageNet21k and group the datasets Aircrafts and Stanford Cars as “out-of-domain” due to their difference with the pretraining. We report the average accuracy for the datasets within each group. We see that APT consistently outperforms Head-only ensembling, and the difference is most pronounced for out-of-domain datasets. 5. Applications of `A-la-carte Learning Decentralized Learning. We may have datasets D1,...,D n stored across n different servers or devices. Each server can train a prompt pi on Di in isolation. At in- ference time, we can assemble the prompts p1,...,p n on a central server and perform inference usingp[n]. Each server can train their prompt pi without exposing or uploading their raw data to the central server. This is useful whenever it is not possible to efﬁciently aggregate the data across the different devices, or if the individual devices are not willing to expose their raw data. We note that in Federated learning one typically looks at a different setting where a single cen- tral model is trained but the gradients are computed locally and then shared. Since a single model is trained via gradi- ents aggregated across all the sources, this does not solve the `a-la-carte learning problem and does not allow forget- ting a source of data or ﬁrewalling a particular user from a source of data. Nevertheless, the two approaches are not mutually exclusive and we believe integrating them is an interesting avenue of research. Model Versioning. Different users may have different rights in terms of which datasets they are permitted to ac- cess. For each user A we can associate a set of indices I ⊂[n] based on which datasets they have rights to. Then the version of the model we offer to user Awould be given by f(x; θI). Aside from dataset rights, individuals may wish to add or drop data from the inﬂuence of a model simply for performance reasons. A dataset Di may be use- ful for user A but introduce performance degradations for user B. `A-la-carte learning allows us to include or not in- clude the prompt θi for different users. Furthermore since the prompts do not need to be trained at the same time, we can add prompts at later points in time to update the model according to new data. Forgetting. Forgetting a source Di is easy, as we simply need to delete its associated prompt pi. However, a ser- vice may periodically get requests to forget speciﬁc samples (x,y). Retraining a model from scratch each time a forget request is received can be prohibitively expensive. Further- more even if the economic cost of training is no issue, sat- isfying the forget request immediately requires suspending the service until the retraining has completed which can in- duce service delays. Following [1], we can partition our dataset Dinto disjoint “shards” of equal size chosen uni- formly at random so that D= ⋃ i∈[n] Di. Then anytime we receive a request to forget a speciﬁc data point (x,y) ∈D we only need to retrain the prompt pi corresponding to the shard Di that (x,y) belongs to. Furthermore the forget re- quest can be satisﬁed immediately without any downtime to the service as the service can drop the prompt pi from the model while it is being retrained and form predictions using the remaining prompts in the meantime. Continual Learning. We can let Di each correspond to a different training episode. Then in a continual learning setting at each episode i we train a prompt pi on Di and let our model after the speciﬁc training episode be f(x; pI) where I = {1,2,...,i }. 6. Experiments In all experiments we use a VIT-B/16 [4] pre-trained on ImageNet-21k. Unless explicitly stated otherwise, we use the pre-trained model vit base patch16 384 from the timm3 library in PyTorch [32]. Datasets. We evaluate APT on the datasets MIT-67 [33], Cub-200-2011 [36], FGVC-Aircrafts [28], Oxford Flow- ers [30], Caltech-256 [13], Oxford Pets [31], and Stan- ford Cars [16]. Based on the distance from the Ima- geNet21k pre-training, similarly to [19] we classify the datasets MIT-67, Cub-200-2011, Oxford Flowers, Caltech- 256, and Oxford Pets as “in-domain” datasets and classify the datasets FGVC-Aircrafts and Stanford Cars as “out- of-domain” datasets. To test APT on class incremental learning problem we use Split CIFAR-100 [17] (10 train- ing episodes, 10 classes per episode) and for domain incre- mental learning we use CORe50 (8 training domains, 3 test domains) [25, 26]. 3https : / / github . com / rwightman / pytorch - image - models 6Dataset No Sharding 2 Shards 3 Shards 5 Shards 10 Shards 15 Shards 20 Shards 50 Shards MIT-67 86.2% 86.2% 86.0% 87.3% 86.8% 86.3% 86.3% 84.2% Cub-200 86.6% 87.8% 87.2% 87.2% 86.5% 85.5% 83.9% 79.9% Caltech-256 91.7% 91.1% 90.8% 90.3% 89.7% 89.1% 88.7% 87.1% Pets 93.3% 93.1% 93.4% 93.3% 93.4% 93.5% 93.3% 92.3% Aircrafts 71.0% 61.1% 60.2% 56.3% 49.9% 46.6% 45.4% 36.9% Flowers 99.1% 99.3% 99.1% 98.8% 98.5% 98.6% 97.6% 97.7% Stanford Cars 81.2% 70.7% 66.6% 59.4% 52.1% 47.0% 45.8% 39.1% Average 87.0% 84.2% 83.3% 81.8% 79.6% 78.1% 77.3% 73.9% Table 2. Accuracy of shard ensembles. Accuracy of ensembling prompts trained on disjoint shards chosen uniformly at random. We see that for many datasets the performance of the ensemble is close to the paragon of prompt tuning on the entire dataset, despite each predictor of the dataset only seeing a fraction of the entire dataset. Dataset Finetuning Head-only Bias+Head Deep PT Deep Shared PT Shallow PT FT vs. PT gap MIT-67 87.1% 85.6% 87.2% 86.2% 86.5% 86.0% -0.9% Cub-200 88.4% 87.0% 89.4% 86.6% 86.4% 85.6% -1.8% Caltech-256 93.5% 90.4% 93.0% 91.7% 91.3% 90.4% -1.8% Pets 94.5% 92.2% 94.9% 93.3% 92.9% 91.6% -1.2% Aircrafts 75.6% 54.8% 75.6% 71.0% 68.2% 62.1% -4.6% Flowers 97.4% 98.8% 99.4% 99.1% 98.9% 98.5% 1.7% Stanford Cars 84.3% 64.5% 86.6% 81.2% 78.6% 69.6% -3.1% Avg 88.7% 81.9% 89.4% 87.0% 86.1% 83.4% -1.7% Table 3. Finetuning vs. Prompt Tuning. We compare different ﬁnetuning methods to prompt tuning. In the “Head-only” column only the linear classiﬁer head is trained. In “Bias+Head” the bias’s as well as the classiﬁer head are trained. “Deep PT” is prompt tuning with memory tokens at each layer. “Deep Shared PT” is prompt tuning where the memory tokens are shared across the layers. In “Shallow PT” a single prompt is tuned without memory tokens. “FT vs. PT Gap” reports the accuracy of Deep PT minus the accuracy of Finetuning. Comparison of model-tuning methods. Since our method is based on prompt-tuning, in Table 3 we measure how it compares to standard ﬁne-tuning. Consistent with [15], we see that on most datasets prompt tuning is competi- tive (within 2% accuracy) with ﬁnetuning and outperforms head-only tuning, especially on out-of-domain datasets. We also observe that per-layer memory tokens (Deep PT) have the best trade-off between accuracy and computational cost, motivating our design choice to use it. Decrease in performance due to sharding. Given a sharded dataset, we aim to establish whether composing per-shard prompts using APT achieves a comparable perfor- mance to training a prompt on all available data (paragon). Following [1], we split the training set into disjoint shards of equal size. The splitting is done by selecting samples uni- formly at random, hence the number of examples per class can slightly vary across shards and smaller shards may not have examples from all classes. We train prompts on each of the shards in isolation and then compose a model using APT. The test accuracies as we increase the number of splits are reported in Table 2. Figure 3 (A) shows the increase in test error of the APT method relative to the paragon. As expected, the accuracy of APT generally decreases as the number of splits increases. However, for many datasets the drop off in accuracy is surprisingly small: on the in- domain datasets for 10-20 shards the accuracy of APT is within 2-5% of the accuracy of the paragon of training on the entire dataset. The main exceptions are out-of-domain datasets, where we observe a steeper accuracy drop when splitting the dataset. We hypothesize that for out-of-domain dataset, synergistic information between datapoints of dif- ferent shards is more important for the training process. Importance of composing prompts. In Figure 3 (C) we plot the gap between the average individual prompt accu- racy and the accuracy of APT. We see that as the number of shards increases, the difference grows. This implies that while the performance of the ensemble may drop off slowly, that the performance of the individual predictors is deteri- orating. This demonstrates that on large and fragmented data pools, individual prompts do not have enough informa- tion to classify correctly, and aggregating their information through the APT composition mechansim is essential. Ablations. We perform a series of ablations to piece out the essential components of APT. To understand the effect of the attention masking, in Figure 2 we compare APT to the na¨ıve method of concatenating all prompts without struc- 7Method CIFAR-100 CORe50 APT 83.63 90.89 APT-W 85.21 91.14 L2P [8] 83.83 78.33 S-iPrompts [7] N/A 83.13 S-liPrompts [7] N/A 89.06 LwF [22] 60.69 75.45 EWC [6] 47.01 74.82 Table 4. Performance on Split CIFAR-100 and CORe50. Re- porting average accuracy on the test set. Numbers for the non- APT methods are reported in [7] or [8]. For fair comparison against [7, 8] we have changed the resolution of our VIT to 224 from 384. Since APT does not train with a memory buffer we compare against the memoryless versions of L2P and S-Prompts. tured attention. We see that naive concatenation performs almost uniformly worse than APT on average and has sig- niﬁcantly higher variance, failing with very low accuracies in some cases. To isolate the effect of prompt tuning on the success of APT, in Table 1 we compare our APT method to training a simple head-only classiﬁer on each shard. We see that APT uniformly outperforms its head-only counterpart, and that the difference is especially pronounced for out-of- domain datasets. Forgetting sources. In Figure 3 (B) we plot the increase in error of the APT method after a certain number of shards (and their corresponding prompts) are deleted. This simu- lates a setting where a service provider receives a sequence of forget requests and consequently must remove prompts from the model. We see that starting with 20 shards, even after removing 10 shards for most the datasets the decline in accuracy is approximately 5% or less despite utilizing half the data. Since training time is directly proportional to the number of samples in the training set, this implies that we can reduce the cost of retraining after a forget request by an order of magnitude with a negligible drop in accu- racy. Furthermore as shown in Figure 3 (B) we can handle a large number of forget requests sequentially without re- training before accuracy meaningfully declines. Moreover, since adding and removing sources are symmetric opera- tions for APT, the same plot can be interpreted in reverse as showing the performance of APT in incrementally learning from an increasing set of data sources. Class Incremental Learning. Oftentimes one wishes to add new classes to the model incrementally. In this section we explore class-incremental-learning (CIL) where at dif- ferent training episodes we have access to different classes. To evaluate APT in this setting, we use the Split CIFAR- 100 benchmark where the dataset is split into 10 disjoint sets of classes, with each subset containing 10 classes each. We train prompts on each subset in isolation. At inference time, we simply concatenate the class predictions from the individual prompts in line with our APT scheme. In Tab. 4 we report the results of APT in this setting. Out-of-the- box APT outperforms all the baselines and has a compara- ble performance to L2P [8]. We note that an advantage of L2P is the ability to dynamically select the prompts based on the test sample. Since prompts in APT are composi- tional by construction, we can easily implement a similar mechanism. Similarly to [37] we performK-means on each episode in the embedding space to extract reference proto- types for that episode (K = 20), then at inference time we weight each episode prompt based on the distance of the instance’s embedding from that episode’s prototypes. See details in Sec. A in the supplementary material for the ex- act weighting scheme. We denote this method in the ta- bles as APT-Weight (APT-W), and note that using this hard- coded weighting strategy – in contrast with L2P’s learned prompt selection mechanism – APT-W outperforms L2P. We note that this weighting scheme still satisﬁes the `a-la- carte learning requirement since the reference prototypes for each source are constructed independently of the other sources. Domain Incremental Learning. Oftentimes one encoun- ters data from different domains at different points in time. In the continual learning literature this setting is referred to as domain-incremental-learning (DIL). In this section we evaluate APT on domain incremental learning. In Tab. 4 we report the results of running APT on the CORe50 domain incremental learning benchmark. The CORe50 dataset con- tains data from 8 training domains and 3 test domains. By training prompts independently on each of the training do- mains, out-of-the-box APT outperforms all other methods on CORe50. Weighting the prompts in the APT-W scheme seems to give only a marginal increase (0.25%) in perfor- mance. 7. Conclusion We introduced the general problem of `A-la-carte Learn- ing and an efﬁcient solution to the problem using `A-la- carte Prompt Tuning (APT). We demonstrate that models constructed `a la carte are competitive with models trained on the union of the respective sources, with added ben- eﬁts for privacy and customization. Furthermore APT achieves state-of-the-art performance for both class incre- mental learning and domain incremental learning with ad- ditional beneﬁts for privacy and customization. While APT offers one solution to the `A-la-carte Learning problem, we emphasize that this problem is more general and deserves further study in order to develop competitive machine learn- ing methods that respect users’ data usage and privacy rights. 8References [1] Lucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP) , pages 141–159, 2021. 3, 6, 7, 11 [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural in- formation processing systems, 33:1877–1901, 2020. 2 [3] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In H. Larochelle, M. Ranzato, R. Had- sell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18613– 18624. Curran Associates, Inc., 2020. 11 [4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representa- tions, 2021. 3, 6 [5] Arthur Douillard, Alexandre Ram ´e, Guillaume Couairon, and Matthieu Cord. Dytox: Transformers for continual learning with dynamic token expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9285–9295, June 2022. 3 [6] Kirkpatrick et al. Overcoming catastrophic forgetting in neu- ral networks. PNAS, 114, 12 2016. 8 [7] Yabin Wang et al. S-prompts learning with pre-trained trans- formers: An occam’s razor for domain incremental learning. In NeurIPS, 2022. 8 [8] Zifeng Wang et al. Learning to prompt for continual learning. In CVPR, 2022. 8 [9] Aditya Golatkar, Alessandro Achille, Avinash Ravichan- dran, Marzia Polito, and Stefano Soatto. Mixed-privacy for- getting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 792–801, June 2021. 3 [10] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2020. 3 [11] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Forgetting outside the box: Scrubbing deep networks of information accessible from input-output observations. In European Conference on Computer Vision, pages 383–398. Springer, 2020. 3 [12] Priya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noord- huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large mini- batch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 11 [13] Grifﬁn, Holub, and Perona. Caltech 256, Apr 2022. 6, 12 [14] Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. Approximate data deletion from machine learn- ing models. In AISTATS, pages 2008–2016, 2021. 3 [15] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi- sual prompt tuning. In Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIII , page 709–727, Berlin, Heidel- berg, 2022. Springer-Verlag. 3, 4, 7 [16] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia, 2013. 6, 12 [17] Alex Krizhevsky. Learning multiple layers of features from tiny images. pages 32–33, 2009. 6, 12 [18] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. In Proceed- ings of the 2021 Conference on Empirical Methods in Nat- ural Language Processing , pages 3045–3059, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. 3 [19] Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Rethink- ing the hyperparameters for ﬁne-tuning. In International Conference on Learning Representations, 2020. 6 [20] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Got- mare, Shaﬁq Joty, Caiming Xiong, and Steven Hoi. Align be- fore fuse: Vision and language representation learning with momentum distillation. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neu- ral Information Processing Systems, 2021. 12 [21] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 4582–4597. Association for Computational Linguis- tics, 2021. 3 [22] Zhizhong Li and Derek Hoiem. Learning without forgetting. In ECCV, 2016. 8 [23] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi- roaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natu- ral language processing. ACM Comput. Surv., aug 2022. Just Accepted. 2 [24] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to ﬁne-tuning across scales and tasks. In Pro- ceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 61–68, Dublin, Ireland, May 2022. Association for Compu- tational Linguistics. 3 [25] Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. In Sergey Levine, Vincent Vanhoucke, and Ken Goldberg, ed- itors, Proceedings of the 1st Annual Conference on Robot 9Learning, volume 78 of Proceedings of Machine Learning Research, pages 17–26. PMLR, 13–15 Nov 2017. 6, 12 [26] Vincenzo Lomonaco, Davide Maltoni, and Lorenzo Pel- legrini. Fine-grained continual learning. ArXiv, abs/1907.03799, 2019. 6, 12 [27] Ilya Loshchilov and Frank Hutter. Decoupled weight de- cay regularization. In International Conference on Learning Representations, 2019. 11 [28] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of aircraft. Technical re- port, 2013. 6, 12 [29] Seth Neel, Aaron Roth, and Saeed Shariﬁ-Malvajerdi. Descent-to-delete: Gradient-based methods for machine un- learning. In Vitaly Feldman, Katrina Ligett, and Sivan Sabato, editors, Proceedings of the 32nd International Con- ference on Algorithmic Learning Theory, volume 132 ofPro- ceedings of Machine Learning Research , pages 931–962. PMLR, 16–19 Mar 2021. 3 [30] Maria-Elena Nilsback and Andrew Zisserman. A visual vo- cabulary for ﬂower classiﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages 1447–1454, 2006. 6, 12 [31] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. InIEEE Conference on Com- puter Vision and Pattern Recognition, 2012. 6, 12 [32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ¨opf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imper- ative Style, High-Performance Deep Learning Library. Cur- ran Associates Inc., Red Hook, NY , USA, 2019. 6 [33] Ariadna Quattoni and Antonio Torralba. Recognizing indoor scenes. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 413–420, 2009. 6, 12 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th Interna- tional Conference on Machine Learning, volume 139 ofPro- ceedings of Machine Learning Research, pages 8748–8763. PMLR, 18–24 Jul 2021. 3 [35] Mark Sandler, Andrey Zhmoginov, Max Vladymyrov, and Andrew Jackson. Fine-tuning image transformers using learnable memory. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR) , pages 12155–12164, June 2022. 3, 4, 5, 11 [36] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. 6, 12 [37] Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. S-prompts learning with pre-trained transformers: An occam’s razor for domain incremental learning. In Alice H. Oh, Alekh Agar- wal, Danielle Belgrave, and Kyunghyun Cho, editors, Ad- vances in Neural Information Processing Systems , 2022. 3, 8 [38] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pﬁster. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 139–149, 2022. 3, 11 [39] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. In International Conference on Learning Representa- tions, 2018. 11 10Supplementary Material A. Details of APT Weight (APT-W) In this section we describe the details of the APT Weight (APT-W) scheme. Let D= {D1,...,D n}be a collection of sources. Consistent with APT for each source Di we train a prompt p(i) and a classiﬁer head headi using only the data in Di. Then, differing with classical APT, for each source Di we perform K-means (K = 20) in the embed- ding space to construct a set of prototypes µ(i) 1 ,...,µ (i) K . More concretely for each (x,y) ∈Di we forward the in- put x through the transformer to get the ﬁnal embedding sequence [zL(x)]. We use the class token embeddings z(0) L (x) as the vectors to perform the K-means algorithm on. Speciﬁcally we perform K-means on the set {z(0) L (x) : (x,y) ∈Di} to construct the prototypesµ(i) 1 ,...,µ (i) K . At inference time, the basic forward pass for APT Weight is the same as APT. Given an instancexand a setI = {i1,...,i |I|}⊂ [n] we let p(I) = [p(i1),...,p (i|I|)] be the concatenation of all prompt tokens corresponding to each data source in DI. The ﬁnal output of the transformer is given by [zL,p(I) L ] :=FL θ ◦... ◦F1 θ([z0,p(I)]) where the structured attention is applied as usual. Each out- put token p(i) L corresponding to a prompt p(i) is used to gen- erate logits ˆy(i) := headi(p(i) L ). In contrast to APT, APT Weight will apply a weighting to the logits ˆy(i) based on the distance of the embedding of the instance xto the prototypes µ(i) 1 ,...,µ (i) K . Speciﬁcally, for each index in i∈I we compute di = min k∈[K] ∥z(0) L −µ(i) k ∥2. Let us denote d = (di1 ,...,d i|I| ). Then we construct a weight vector w:= softmax(−βd) where βis the inverse temperature which in our experiments we set to β = 0.1. We then form the weighted logits [wi1 ·ˆy(i1),wi2 ·ˆy(i2),...,w i|I| ·ˆy(i|I|)]. For class incremental learning problems, these weighted logits are the ﬁnal logits used for prediction. For domain in- cremental learning problems, the logits are average pooled to form the ﬁnal logits ˆy= 1 |I| ∑ i∈I wi ·ˆy(i). B. Hyperparameters Consistent with [38] for the continual learning experiments on Split CIFAR-100 and CORe50 we train for 5 epochs. All methods in Table 3 are trained for 150 epochs. For all other experiments the paragon method (trained on the en- tire dataset) is trained for 150 epochs whereas the prompts in the APT method are trained for 80 epochs on their respec- tive sources. We emphasize that the paragon is never trained for fewer epochs than the APT method to remain a true “up- per bound”. For the paragon prompt tuning numbers we do not use structured attention. Consistent with [35] we use 5 memory tokens at each layer for deep prompting. For the prompt tuning of the APT method structured attention is ap- plied during both train and inference time. In all cases we optimize using the AdamW algorithm [27] with the weight decay parameter set to 0.02. We use linear warmup cosine annealing with start learning rate 1e−5, minimum learning rate 1e−6, and one warmup epoch. The base learning rates for ﬁnetuning, head-only ﬁnetuning, and prompt tuning are 1e−5, 5e−1, and 1e−1 respectively. We did not do any hy- perparameter sweep over learning rates. The one exception is for the “Bias+Head” column in Table 3 we did a sweep over learning rates to arrive at the learning rate5e−3. How- ever, we note that this column is merely for comparison and does not concern our speciﬁc method. We use a batch size of 8 and follow the convention presented in [12] of rescal- ing the learning rate by the effective batch size (batch size x devices x nodes) divided by 256. We perform data augmentation following standard practice in training ViTs and include RandAugment [3] with N=2 and M=10. However we note that we did not use Mixup [39] which is known to be a reliable way of increasing perfor- mance. C. Dataset Details In Table 5 we report detailed statistics for the datasets used as well as links to download the datasets. D. Additional Ablations Average ensembling vs. majority vote. In our APT method we chose to aggregate the individual predictions of the prompts by average ensembling. Another common en- sembling method is to perform majority vote. Consistent with [1] we ﬁnd that average ensembling outperforms ma- jority vote. In Table 6 we report the performance gap of average ensembling over majority vote for the sharding ex- periment. We see that excluding one exceptional case, aver- age ensembling uniformly outperforms majority vote for all datasets and all numbers of shards. The performance gain on average is in the range 1.5-3.8%. Pretraining. To investigate how APT performs when the 11Table 5. Dataset sample/class counts. We list the number of training images, test images, and classes for each of the datasets. We also provide a link to download the data. Dataset Training Images Testing Images # Classes URL MIT-67 [33] 5360 1340 67 https://web.mit.edu/torralba/www/indoor.html CUB-200 [36] 5994 5794 200 https://www.vision.caltech.edu/datasets/cub_200_2011/ Caltech-256 [13] 15418 15189 257 https://authors.library.caltech.edu/7694/ Oxford Pets [31] 3680 3669 37 https://www.robots.ox.ac.uk/˜vgg/data/pets/ FGVC-Aircrafts [28] 6667 3333 100 https://www.robots.ox.ac.uk/˜vgg/data/fgvc-aircraft/ Oxford Flowers [30] 2040 6149 102 https://www.robots.ox.ac.uk/˜vgg/data/flowers/102/ Stanford Cars [16] 8144 8041 196 https://ai.stanford.edu/˜jkrause/cars/car_dataset.html CIFAR-100 [17] 50,000 10,000 100 https://www.cs.toronto.edu/˜kriz/cifar.html CORe50 [25, 26] 119,894 44,972 50 https://vlomonaco.github.io/core50/ Dataset 2 Shards 3 Shards 5 Shards 10 Shards 15 Shards 20 Shards 50 Shards MIT-67 3.1% 0.9% 0.6% 0.7% 0.6% 0.9% 0.5% Cub-200 3.3% 1.1% 1.5% 0.8% 1.0% 0.8% 1.3% Caltech-256 3.0% 1.4% 0.8% 0.2% 0.6% 0.4% 0.2% Pets 1.4% 0.2% 0.3% 0.0% -0.1% 0.2% 0.3% Aircrafts 6.2% 5.2% 5.0% 3.8% 3.2% 3.7% 3.0% Flowers 0.7% 4.2% 0.2% 0.6% 0.7% 1.3% 2.8% Stanford Cars 9.0% 7.6% 5.6% 5.5% 4.9% 5.1% 4.7% Average 3.81% 2.94% 2.0% 1.66% 1.56% 1.77% 1.83% Table 6. Average vs. majority vote. We report the accuracy of average ensembling minus the accuracy of majority vote. We observe that average ensembling uniformly outperforms majority vote. backbone transformer has a different pretraining, instead of using ImageNet21k we experiment with loading the VIT- B/16 from the visual encoder of the multimodal model AL- BEF [20]. In Table 7 we report the accuracies of APT ap- plied to this checkpoint. We see that the performance of APT for the ALBEF visual encoder decays more quickly as the number of shards increases relative to the ImageNet21k numbers reported in Table 2. For example for the visual encoder of ALBEF, for 10 shards only the datasets MIT- 67, Caltech-256, and Pets are within5% performance of the paragon, whereas by contrast for the ImageNet21k check- point all datasets except for Aircrafts and Stanford Cars are within 5% performance of paragon even when the number of shards is twice as large, namely 20. Thus we conclude that the pretraining of the backbone transformer is highly pertinent for the performance of APT. This is sensible as due to the structured attention the APT prompts do not mod- ify the internal representations of the backbone, and thus are unable to provide compensation whenever the backbone representations are deﬁcient. Finetuning. While inference and storage for the APT method is less costly than ensembling ﬁnetuned models, it is worthwhile to ask how the two compare in terms of classiﬁcation accuracy. In Table 8 we report the accura- cies for the sharding experiment using ﬁnetuning instead of APT. Speciﬁcally we ﬁnetune separate models on each shard which are then ensembled at inference time. By com- paring the results in Table 2 to the results in Table 8, we see that APT uniformly outperforms ﬁnetuning in terms of clas- siﬁcation accuracy, and the gap becomes most pronounced as the number of shards increases. Speciﬁcally, for 20 and 50 shards APT has average accuracy of 77.3% and 73.9% respectively compared to 41.5% and 25.6% for ﬁnetuning. We believe this is due to ﬁnetuning being more susceptible to overﬁtting when there are fewer data in contrast to APT which uses a ﬁxed backbone and thus has a stronger induc- tive bias. 12Dataset No Sharding 2 Shards 3 Shards 5 Shards 10 Shards 15 Shards 20 Shards 50 Shards MIT-67 89.1% 87.5% 88.4% 88.9% 89.0% 88.4% 88.4% 87.3% Cub-200 78.8% 71.6% 66.9% 57.1% 54.9% 48.5% 46.2% 39.6% Caltech-256 91.4% 88.8% 89.2% 88.7% 88.9% 88.6% 87.8% 86.2% Pets 91.1% 89.9% 88.2% 87.0% 86.3% 83.1% 81.0% 66.1% Aircrafts 72.6% 60.5% 54.4% 51.1% 40.2% 38.7% 35.9% 30.7% Flowers 93.4% 85.1% 83.1% 81.7% 80.7% 78.1% 76.2% 67.8% Stanford Cars 83.3% 78.2% 76.5% 70.7% 63.7% 59.5% 55.9% 43.6% Average 85.67% 80.23% 78.1% 75.03% 71.96% 69.27% 67.34% 60.19% Table 7. Sharding from ALBEF pretraining. We report the accuracies for the sharding experiment using the ALBEF checkpoint. Dataset No Sharding 2 Shards 3 Shards 5 Shards 10 Shards 15 Shards 20 Shards 50 Shards MIT-67 87.1% 86.1% 83.8% 81.9% 74.4% 69.9% 68.8% 44.9% Cub-200 88.4% 81.8% 76.4% 70.9% 54.4% 42.5% 32.5% 5.9% Caltech-256 93.5% 90.3% 87.8% 85.8% 81.0% 78.2% 74.1% 52.0% Pets 94.5% 93.6% 92.6% 91.2% 89.7% 84.2% 81.6% 55.8% Aircrafts 75.6% 51.1% 44.5% 36.2% 24.1% 23.0% 19.2% 12.3% Flowers 97.4% 75.3% 56.1% 39.8% 15.6% 11.1% 2.2% 2.0% Stanford Cars 84.3% 53.3% 39.4% 28.2% 19.2% 16.2% 11.8% 6.4% Average 88.69% 75.93% 68.66% 62.00% 51.20% 46.44% 41.46% 25.61% Table 8. Sharding using ﬁnetuning. We report the accuracy for the sharding experiment when using ﬁnetuning. 13",
      "meta_data": {
        "arxiv_id": "2302.07994v1",
        "authors": [
          "Benjamin Bowman",
          "Alessandro Achille",
          "Luca Zancato",
          "Matthew Trager",
          "Pramuditha Perera",
          "Giovanni Paolini",
          "Stefano Soatto"
        ],
        "published_date": "2023-02-15T23:51:09Z",
        "pdf_url": "https://arxiv.org/pdf/2302.07994v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces `A-la-carte Prompt Tuning (APT), a novel transformer-based scheme that enables tuning prompts on distinct data sources and composing them arbitrarily at inference time. This addresses challenges in large neural networks such as costly updates, dynamic data access rights (machine unlearning, compartmentalization), and user-specific model customization. APT allows for `a-la-carte learning, where models can be dynamically assembled based on user-selected data sources without retraining from scratch. The method achieves accuracy within 5% of models trained on the union of data sources, with comparable training and inference costs. It also sets state-of-the-art performance on continual learning benchmarks like Split CIFAR-100 and CORe50, and offers benefits for privacy and maintainability through natural compartmentalization of information within prompts.",
        "methodology": "APT leverages vision transformers (specifically VIT-B/16) and prompt tuning. Each individual data source Di is converted into a learned prompt pi and a classifier head headi, trained in isolation. At inference, for a given subset of data sources S, APT retrieves and concatenates their corresponding prompts p(I) with the input. The core innovation is a modified attention mechanism, referred to as structured attention, which prevents destructive interference between separately trained prompts. This mechanism masks attention so that zℓ tokens do not attend to prompts, and prompts do not attend to each other. To compensate for reduced expressive power, `dmem` learnable memory tokens are added at each layer, which prompts can attend to. Final predictions are made by ensembling (averaging) the individual predictions from each prompt's classifier head. For continual learning tasks, an extension called APT-Weight (APT-W) is introduced, which uses K-means to extract reference prototypes from each episode's embedding space and then weights each prompt's logits at inference time based on the distance of the input instance's embedding to these prototypes.",
        "experimental_setup": "The experimental setup utilized a VIT-B/16 backbone pre-trained on ImageNet-21k, specifically the `vit_base_patch16_384` model from the timm library. Evaluation was conducted on a range of fine-grained image classification datasets including MIT-67, Cub-200-2011, FGVC-Aircrafts, Oxford Flowers, Caltech-256, Oxford Pets, and Stanford Cars, categorized as 'in-domain' or 'out-of-domain' relative to ImageNet21k pre-training. For continual learning, Split CIFAR-100 (class incremental) and CORe50 (domain incremental) benchmarks were used. APT's performance was compared against a 'paragon' model (a single prompt trained on the union of all data), naive prompt composition methods (concatenation, averaging), head-only ensembling, and state-of-the-art continual learning baselines (L2P, S-iPrompts, S-liPrompts, LwF, EWC). Training parameters included AdamW optimizer with a weight decay of 0.02, linear warmup cosine annealing learning rate schedule, a batch size of 8, and RandAugment for data augmentation. Ablations were performed to study the impact of structured attention, prompt tuning versus head-only training, and different pre-training checkpoints (e.g., ALBEF visual encoder).",
        "limitations": "The modified structured attention mechanism, while crucial for scalability and avoiding destructive interference, inherently entails a small reduction in expressive power and the ability to capture synergistic information between data sources. This limitation manifests as a steeper accuracy drop for 'out-of-domain' datasets (e.g., Aircrafts, Stanford Cars) when the dataset is highly sharded, suggesting that synergistic information might be more critical for these tasks. Additionally, the performance of APT is highly dependent on the quality of the backbone transformer's pre-training, as the prompts do not modify the backbone's internal representations; a deficient backbone pre-training (e.g., using ALBEF visual encoder) leads to a more rapid decay in performance as the number of data shards increases.",
        "future_research_directions": "The paper suggests integrating `A-la-carte Learning with Federated Learning as an interesting avenue for future research. It also emphasizes that `A-la-carte Learning is a general problem that warrants further study to develop competitive machine learning methods that explicitly respect user data usage and privacy rights."
      }
    }
  ],
  "research_session": {
    "hypothesis": {
      "open_problems": "LoRA distributes the same rank to every weight matrix. AdaLoRA fixes this with iterative SVD‐based pruning but: (1) adds ~10–15 % training time, (2) introduces many scheduler hyper-parameters, (3) is fragile to importance-score estimation errors. We need a cheaper, hyper-parameter-light way to let more important layers use more of the limited LoRA capacity.",
      "method": "Importance–Weighted LoRA (IW-LoRA)\nMinimal change: keep standard LoRA training but add a single extra L2 term whose coefficient is different for every weight matrix.\n1. Warm-up (calibration) phase: run 100–200 forward/backward passes with LoRA parameters frozen (or very small lr) and record the running average of per-matrix gradient sensitivity s_k = E[||∂L/∂W_k||_F].\n2. Define importance weight w_k = s_k / (Σ_j s_j).\n3. During actual fine-tuning (LoRA parameters trainable) minimise\n   L_total = L_task  +  α · Σ_k (1 – w_k)^β · ||Δ_k||_F² ,\n   where Δ_k is the LoRA update for matrix k, α≃1e-4 and β=1 (can be fixed).\n   Layers with high w_k receive almost no penalty and can use the rank budget freely, while unimportant layers are softly shrunk toward zero; the fixed global rank therefore re-allocates itself automatically without pruning or SVD.",
      "experimental_setup": "Backbone: DeBERTaV3-base.\nDatasets: GLUE (MNLI, SST-2, RTE).\nBaselines: (a) Full fine-tune, (b) LoRA (rank=8 everywhere), (c) AdaLoRA (rank budget equivalent to rank 8).\nIW-LoRA uses the same fixed rank=8 but the new loss term.\nMetrics: dev accuracy (MNLI), F1 (RTE), and average score.\nTraining: 3 epochs, lr 2e-4, batch 32. α=1e-4, β=1 fixed for all runs.\nHardware: single V100 GPU.",
      "experimental_code": "# key excerpt – add IW-LoRA regularisation\ndef collect_sensitivity(model, data_loader, steps=200):\n    sens = {n:0. for n,p in model.named_parameters() if 'lora_A' in n}\n    for i, batch in enumerate(data_loader):\n        if i==steps: break\n        loss = model(**batch).loss\n        loss.backward()\n        for n,p in model.named_parameters():\n            if n in sens:\n                sens[n] += p.grad.detach().pow(2).sum()\n        model.zero_grad()\n    # normalise\n    total = sum(sens.values())\n    for n in sens: sens[n] = sens[n]/total\n    return sens\n\nsens = collect_sensitivity(model, train_loader)\n\nalpha, beta = 1e-4, 1.0\n\n# training loop\nfor batch in train_loader:\n    outputs = model(**batch)\n    loss = outputs.loss\n    reg = 0.\n    for n,p in model.named_parameters():\n        if 'lora_A' in n or 'lora_B' in n:  # treat A and B together\n            key = n.replace('lora_A','lora_A') if 'lora_A' in n else n.replace('lora_B','lora_A')\n            reg += (1-sens[key])**beta * p.pow(2).sum()\n    loss = loss + alpha*reg\n    loss.backward()\n    optimizer.step(); optimizer.zero_grad()",
      "expected_result": "IW-LoRA should match AdaLoRA and outperform vanilla LoRA while training in roughly the same time as LoRA (≈10-15 % faster than AdaLoRA). On GLUE we expect:\nMNLI accuracy: LoRA 84.6, AdaLoRA 85.3, IW-LoRA ≈85.1.\nSST-2 accuracy: LoRA 93.2, AdaLoRA 93.8, IW-LoRA ≈93.7.\nRTE F1: LoRA 72.0, AdaLoRA 73.4, IW-LoRA ≈73.2.\nAverage gain of ~0.6 points over vanilla LoRA with negligible extra compute.",
      "expected_conclusion": "A single importance-weighted L2 term lets the model allocate its fixed low-rank capacity where it matters, approaching AdaLoRA’s accuracy without its computational overhead or complex pruning schedule. The change is only a few lines of code and one new global hyper-parameter (α), making the technique easy to adopt in any LoRA-based fine-tuning pipeline."
    },
    "iterations": [
      {
        "method": "Importance–Weighted LoRA (IW-LoRA)\nMinimal change: keep standard LoRA training but add a single extra L2 term whose coefficient is different for every weight matrix.\n1. Warm-up (calibration) phase: run 100–200 forward/backward passes with LoRA parameters frozen (or very small lr) and record the running average of per-matrix gradient sensitivity s_k = E[||∂L/∂W_k||_F].\n2. Define importance weight w_k = s_k / (Σ_j s_j).\n3. During actual fine-tuning (LoRA parameters trainable) minimise\n   L_total = L_task  +  α · Σ_k (1 – w_k)^β · ||Δ_k||_F² ,\n   where Δ_k is the LoRA update for matrix k, α≃1e-4 and β=1 (can be fixed).\n   Layers with high w_k receive almost no penalty and can use the rank budget freely, while unimportant layers are softly shrunk toward zero; the fixed global rank therefore re-allocates itself automatically without pruning or SVD.",
        "experimental_design": {
          "experiment_summary": "The experiment evaluates the new Importance-Weighted LoRA (IW-LoRA) regularisation against the standard LoRA adaptation on a strong encoder model.  \n1. We fine-tune a single backbone (DeBERTaV3-base, ≈86 M parameters) on the GLUE MNLI natural-language-understanding benchmark.  \n2. A short, 150-step warm-up pass is run with LoRA parameters frozen to collect per-matrix gradient sensitivities s_k.  \n3. Sensitivities are normalised to importance weights w_k and stored.  \n4. Full fine-tuning then proceeds for three epochs. The ordinary task loss is augmented with the importance-weighted L2 term α·Σ_k(1-w_k)^β‖Δ_k‖² (α searched, β fixed to 1).  \n5. We compare final validation accuracy to a vanilla LoRA baseline with identical rank=8 adapters.  \n6. All runs use the same optimisation settings (AdamW, three epochs, learning-rate schedule) and are executed on a single NVIDIA A100 (the available environment far exceeds the memory footprint).  \nThe workflow therefore isolates the effect of the proposed regulariser while keeping every other variable identical.",
          "evaluation_metrics": [
            "Accuracy"
          ],
          "proposed_method": "Importance-Weighted LoRA (IW-LoRA) is a drop-in regularisation for LoRA fine-tuning that redistributes a fixed low-rank budget toward layers that matter most.  \n• Theory: For each trainable weight matrix W_k equipped with a rank-r LoRA update Δ_k = A_kB_k, we approximate its task importance with the Frobenius norm of the gradient ‖∂L/∂W_k‖_F averaged over a small calibration run.  \n• Calibration: Freeze LoRA parameters, run 100–200 forward/backward passes, accumulate s_k = E[‖∂L/∂W_k‖_F].  \n• Importance weights: w_k = s_k / Σ_j s_j.  \n• Training objective: L_total = L_task + α Σ_k (1−w_k)^β ‖Δ_k‖_F² with global α (searched) and β=1. High-importance layers (large w_k) receive almost no penalty; low-importance layers are softly shrunk, effectively donating rank capacity to useful layers without explicit pruning or SVD.  \n• Implementation: only ~15 lines added to a standard LoRA training loop—store sensitivities, then, during each training step, compute the weighted L2 regulariser over all LoRA parameters and add it to the loss before back-propagation.  \n• Hyper-parameter economy: only α needs tuning; β can stay at 1 for all tasks.  \n• Computational cost: negligible—no iterative SVD, schedulers, or additional optimiser steps; calibration adds <2 % wall-time.",
          "comparative_methods": [
            "LoRA (rank = 8, no regularisation)"
          ],
          "models_to_use": [
            "DeBERTaV3-base (≈86 M)"
          ],
          "datasets_to_use": [
            "GLUE-MNLI"
          ],
          "hyperparameters_to_search": {
            "learning_rate": "1e-5-5e-4",
            "alpha_IW_LoRA": "1e-5-1e-3",
            "batch_size": "16,32,64"
          },
          "external_resources": {
            "hugging_face": {
              "models": [
                {
                  "id": "microsoft/deberta-v3-base",
                  "author": "microsoft",
                  "sha": "8ccc9b6f36199bec6961081d44eb72fb3f7353f3",
                  "created_at": "2022-03-02T23:29:05+00:00",
                  "last_modified": "2022-09-22T12:34:19+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 2518675,
                  "likes": 372,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "config.json"
                    },
                    {
                      "rfilename": "pytorch_model.bin"
                    },
                    {
                      "rfilename": "rust_model.ot"
                    },
                    {
                      "rfilename": "spm.model"
                    },
                    {
                      "rfilename": "tf_model.h5"
                    },
                    {
                      "rfilename": "tokenizer_config.json"
                    }
                  ],
                  "card_data": {
                    "license": "mit",
                    "language": "en",
                    "tags": [
                      "deberta",
                      "deberta-v3",
                      "fill-mask"
                    ],
                    "datasets": [],
                    "task_categories": [],
                    "size_categories": [],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "transformers",
                    "pytorch",
                    "tf",
                    "rust",
                    "deberta-v2",
                    "deberta",
                    "deberta-v3",
                    "fill-mask",
                    "en",
                    "arxiv:2006.03654",
                    "arxiv:2111.09543",
                    "license:mit",
                    "endpoints_compatible",
                    "region:us"
                  ],
                  "pipeline_tag": "fill-mask",
                  "library_name": "transformers",
                  "readme": "---\nlanguage: en\ntags: \n  - deberta\n  - deberta-v3\n  - fill-mask\nthumbnail: https://huggingface.co/front/thumbnails/microsoft.png\nlicense: mit\n---\n\n## DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. \n\nIn [DeBERTa V3](https://arxiv.org/abs/2111.09543), we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our [paper](https://arxiv.org/abs/2111.09543).\n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more implementation details and updates.\n\nThe DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters  with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.\n\n\n#### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 2.0 and MNLI tasks.\n\n| Model             |Vocabulary(K)|Backbone #Params(M)| SQuAD 2.0(F1/EM) | MNLI-m/mm(ACC)|\n|-------------------|----------|-------------------|-----------|----------|\n| RoBERTa-base      |50     |86                 | 83.7/80.5 | 87.6/-   |\n| XLNet-base        |32     |92                 | -/80.2    | 86.8/-   |\n| ELECTRA-base      |30    |86                  | -/80.5    | 88.8/    |\n| DeBERTa-base      |50     |100                |  86.2/83.1| 88.8/88.5|\n| DeBERTa-v3-base   |128|86                       | **88.4/85.4** | **90.6/90.7**|\n| DeBERTa-v3-base + SiFT |128|86                | -/- | 91.0/-|\n\nWe present the dev results on SQuAD 1.1/2.0 and MNLI tasks.\n\n#### Fine-tuning with HF transformers\n\n```bash\n#!/bin/bash\n\ncd transformers/examples/pytorch/text-classification/\n\npip install datasets\nexport TASK_NAME=mnli\n\noutput_dir=\"ds_results\"\n\nnum_gpus=8\n\nbatch_size=8\n\npython -m torch.distributed.launch --nproc_per_node=${num_gpus} \\\n  run_glue.py \\\n  --model_name_or_path microsoft/deberta-v3-base \\\n  --task_name $TASK_NAME \\\n  --do_train \\\n  --do_eval \\\n  --evaluation_strategy steps \\\n  --max_seq_length 256 \\\n  --warmup_steps 500 \\\n  --per_device_train_batch_size ${batch_size} \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir $output_dir \\\n  --overwrite_output_dir \\\n  --logging_steps 1000 \\\n  --logging_dir $output_dir\n\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following papers:\n\n``` latex\n@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n```\n",
                  "extracted_code": ""
                }
              ],
              "datasets": [
                {
                  "id": "Neuronovo/neuronovo-utc-data-glue-mnli",
                  "author": "Neuronovo",
                  "sha": "a1fbcdc0845b1793f066ceea222f1a60cc263a5f",
                  "created_at": "2024-01-25T19:46:58+00:00",
                  "last_modified": "2024-01-25T21:01:39+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 56,
                  "likes": 0,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "data/test-00000-of-00001.parquet"
                    },
                    {
                      "rfilename": "data/train-00000-of-00001.parquet"
                    },
                    {
                      "rfilename": "data/validation-00000-of-00001.parquet"
                    }
                  ],
                  "card_data": {
                    "language": [],
                    "tags": [],
                    "datasets": [],
                    "task_categories": [],
                    "size_categories": [],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "size_categories:1M<n<10M",
                    "format:parquet",
                    "modality:tabular",
                    "modality:text",
                    "library:datasets",
                    "library:pandas",
                    "library:mlcroissant",
                    "library:polars",
                    "region:us"
                  ],
                  "readme": "---\ndataset_info:\n  features:\n  - name: x\n    dtype: string\n  - name: y\n    dtype: int64\n  - name: label_id\n    dtype: int64\n  - name: text\n    dtype: string\n  - name: id\n    dtype: int64\n  splits:\n  - name: train\n    num_bytes: 492690051\n    num_examples: 1119828\n  - name: validation\n    num_bytes: 25718605\n    num_examples: 58278\n  - name: test\n    num_bytes: 26234868\n    num_examples: 58941\n  download_size: 144048422\n  dataset_size: 544643524\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: data/train-*\n  - split: validation\n    path: data/validation-*\n  - split: test\n    path: data/test-*\n---\n"
                }
              ]
            }
          }
        },
        "experiment_runs": [
          {
            "run_id": "proposed-DeBERTaV3-base-86-M--GLUE-MNLI",
            "method_name": "proposed",
            "model_name": "DeBERTaV3-base (≈86 M)",
            "dataset_name": "GLUE-MNLI"
          },
          {
            "run_id": "comparative-1-DeBERTaV3-base-86-M--GLUE-MNLI",
            "method_name": "comparative-1",
            "model_name": "DeBERTaV3-base (≈86 M)",
            "dataset_name": "GLUE-MNLI"
          }
        ]
      }
    ]
  }
}