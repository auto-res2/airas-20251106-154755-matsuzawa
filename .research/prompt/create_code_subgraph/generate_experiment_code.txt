
LLM Name: o3-2025-04-16
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/runs/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/runs/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path} mode=full

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} mode=trial
```
- `run`: Experiment run_id (matching a run_id from config/runs/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `mode`: Execution mode (required parameter)
  * `mode=trial`: Lightweight execution for validation (epochs=1, batches limited to 1-2, wandb.mode=disabled, optuna.n_trials=0)
  * `mode=full`: Full experiment execution (wandb.mode=online, full epochs, full Optuna trials)
  * **Code must automatically configure based on mode (e.g., `if cfg.mode == "trial": cfg.wandb.mode = "disabled"; cfg.optuna.n_trials = 0` elif `cfg.mode == "full": cfg.wandb.mode = "online"`)**

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- `run_ids`: JSON string list of run IDs to evaluate (e.g., '["run-1-proposed-bert-glue", "run-2-baseline-bert-glue"]')
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * **Optuna Integration**: If using Optuna for hyperparameter search, DO NOT log intermediate trial results to WandB - only train once with the best hyperparameters after optimization completes and log that final run
  * **Log ALL metrics to WandB comprehensively**:
    - Use `wandb.log()` at each training step/batch/epoch with ALL relevant metrics
    - Log as frequently as possible (per-batch or per-epoch) to capture training dynamics
    - Use CONSISTENT metric names across train.py and evaluate.py (e.g., if train.py logs "train_acc", evaluate.py MUST use run.history(keys=["train_acc",...]))
  * **Save final/best metrics to WandB summary**:
    - Use `wandb.summary["key"] = value` for final results
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse command line arguments:
    - `results_dir`: Output directory path
    - `run_ids`: JSON string list of run IDs (parse with `json.loads(args.run_ids)`)
  * Load WandB config from `config/config.yaml` (in repository root)
  * **Retrieve comprehensive experimental data from WandB API** for specified run_ids:
    ```python
    import json
    api = wandb.Api()
    run_ids = json.loads(args.run_ids)  # Parse JSON string to list
    for run_id in run_ids:
        run = api.run(f"{entity}/{project}/{run_id}")
        history = run.history()  # pandas DataFrame with ALL time-series metrics (train_loss, val_acc, etc.)
        summary = run.summary._json_dict  # Final/best metrics (best_val_acc, final_test_acc, etc.)
        config = dict(run.config)  # Run configuration (hyperparameters, model settings, etc.)
    ```
  * **STEP 1: Per-Run Processing** (for each run_id):
    - Export **comprehensive** run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
    - Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
    - Each run should have its own subdirectory with its metrics and figures
  * **STEP 2: Aggregated Analysis** (after processing all runs):
    - Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
    - Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
    - Generate comparison figures to: `{results_dir}/comparison/`:
      * Cross-run comparison charts (bar charts, box plots)
      * Performance metrics tables
      * Statistical significance tests
  * **Figure Generation Guidelines**:
    - Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
    - Use GLOBALLY UNIQUE image filenames to prevent collisions across different runs and directories**:
      * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
      * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
  * Print all generated file paths to stdout (both per-run and comparison)

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- **Mode handling**: Automatically configure based on `cfg.mode`:
  * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1, etc.
  * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 251106-test
    mode: online  # Automatically set to "disabled" in trial_mode
  ```
- `WANDB_API_KEY` environment variable is automatically available for authentication

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Mode-Based Behavior**: Code must automatically configure based on `cfg.mode` ("trial" vs "full")
   - `mode=trial`: Set `cfg.wandb.mode="disabled"`, `cfg.optuna.n_trials=0`, epochs=1, limited batches
   - `mode=full`: Set `cfg.wandb.mode="online"`, use full configuration
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: Several mandatory requirements are violated:

1. Trial-mode computational budget is not respected.  When cfg.mode == "trial" the training loop is shortened, but the IW-LoRA warm-up still executes cfg.model.importance_weighted_regularisation.sensitivity_warmup_steps (150 forward/backward passes).  This contradicts requirement 9, which demands only 1–2 batches in trial mode.

2. The parameter freeze_lora_during_warmup defined in the run config is ignored; LoRA weights remain trainable during the warm-up despite the flag.  This means the implementation is not fully faithful to the configuration (criterion 1 – complete implementation).

3. results_dir passed from the CLI is never created/used in train.py, so artefacts other than WandB (e.g. checkpoints) cannot be stored there (minor, but violates criterion 2 on automatic configuration handling).

Because of issue (1) the code will exceed CI/fast-validation limits and therefore is not ready for immediate execution.  Please add a fast path that skips or drastically shortens the warm-up when mode=="trial", honour freeze_lora_during_warmup, and ensure results_dir is created/used in train.py.
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "# config/config.yaml\n# -----------------------------------------------------------------------------\n# Global Hydra configuration. The *run* group will be overridden from CLI via\n# `run=\u003crun_id\u003e` where `\u003crun_id\u003e` is a filename inside `config/runs/`.\n# -----------------------------------------------------------------------------\n\ndefaults:\n  - run: null   # overwritten from CLI \u2013 DO NOT rename this line\n\n# -----------------------------------------------------------------------------\n# WandB \u2013 overridden automatically depending on mode (trial/full)\n# -----------------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 251106-test\n  mode: online\n\n# Runtime mode (trial / full)\nmode: full\n\n# Where to place outputs (overridden from CI)\nresults_dir: ./results\n\n# Placeholder groups \u2013 will be populated by run-specific configs\ntraining: {}\noptuna: {}\nmodel: {}\ndataset: {}\nrun: {}\n", "evaluate_py": "#!/usr/bin/env python\n\"\"\"src/evaluate.py\nIndependent evaluation \u0026 visualisation script. Fetches completed WandB runs,\nexports metrics, generates per-run and aggregated figures (learning curves,\nconfusion matrices, comparison charts) and performs basic statistical tests.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom scipy.stats import ttest_ind\nfrom tqdm.auto import tqdm\n\n# -----------------------------------------------------------------------------\n# Helper functions\n# -----------------------------------------------------------------------------\n\ndef save_json(obj: dict, path: Path) -\u003e None:\n    path.write_text(json.dumps(obj, indent=2))\n\n\ndef plot_learning_curve(history: pd.DataFrame, run_id: str, out_dir: Path) -\u003e Path:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    if \"train_loss\" in history:\n        ax.plot(history[\"_step\"], history[\"train_loss\"], label=\"train_loss\")\n    if \"validation_accuracy\" in history:\n        ax.plot(history[\"_step\"], history[\"validation_accuracy\"], label=\"val_acc\")\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Metric value\")\n    ax.set_title(f\"Learning curves \u2013 {run_id}\")\n    ax.legend()\n    fig.tight_layout()\n    path = out_dir / f\"{run_id}_learning_curve.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef plot_confusion_matrix(conf: np.ndarray, run_id: str, out_dir: Path) -\u003e Path:\n    fig, ax = plt.subplots(figsize=(4, 4))\n    sns.heatmap(conf, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"True\")\n    ax.set_title(f\"Confusion matrix \u2013 {run_id}\")\n    fig.tight_layout()\n    path = out_dir / f\"{run_id}_confusion_matrix.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef bar_chart(agg: pd.DataFrame, out_dir: Path) -\u003e Path:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.barplot(data=agg, x=\"run_id\", y=\"best_val_acc\", hue=\"method\", ax=ax)\n    for i, v in enumerate(agg.best_val_acc):\n        ax.text(i, v + 0.002, f\"{v:.3f}\", ha=\"center\")\n    ax.set_ylabel(\"Best validation accuracy\")\n    ax.set_xlabel(\"\")\n    ax.set_ylim(0, agg.best_val_acc.max() * 1.05)\n    fig.tight_layout()\n    path = out_dir / \"comparison_accuracy_bar_chart.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef box_plot(values: Dict[str, List[float]], out_dir: Path) -\u003e Path:\n    df = pd.DataFrame({\"run_id\": np.repeat(list(values.keys()), [len(v) for v in values.values()]),\n                       \"val_acc\": np.concatenate(list(values.values()))})\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.boxplot(data=df, x=\"run_id\", y=\"val_acc\", ax=ax)\n    fig.tight_layout()\n    path = out_dir / \"comparison_val_accuracy_boxplot.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n# -----------------------------------------------------------------------------\n# Main entry\n# -----------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Evaluation \u0026 visualisation \u2013 WandB fetcher\")\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory where outputs will be saved\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON string list of WandB run IDs\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir).expanduser().absolute()\n    results_dir.mkdir(parents=True, exist_ok=True)\n    run_ids: List[str] = json.loads(args.run_ids)\n\n    # --------------------------------------------------\n    # Load WandB entity/project from config/config.yaml\n    # --------------------------------------------------\n    import yaml\n    cfg_path = Path(__file__).resolve().parent.parent / \"config\" / \"config.yaml\"\n    with cfg_path.open() as f:\n        global_cfg = yaml.safe_load(f)\n    entity = global_cfg[\"wandb\"][\"entity\"]\n    project = global_cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    aggregated_metrics: List[dict] = []\n    epoch_accuracy_values: Dict[str, List[float]] = {}\n    generated_files: List[str] = []\n\n    # --------------------------------------------------\n    # Per-run processing\n    # --------------------------------------------------\n    for rid in tqdm(run_ids, desc=\"Processing runs\"):\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        history = run.history(samples=100000)  # large enough\n        summary = run.summary._json_dict\n        config = dict(run.config)\n\n        run_dir = results_dir / rid\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save complete metrics \u0026 config\n        save_json({\"history\": history.to_dict(orient=\"list\"),\n                   \"summary\": summary,\n                   \"config\": config}, run_dir / \"metrics.json\")\n\n        # Learning curve figure\n        generated_files.append(str(plot_learning_curve(history, rid, run_dir)))\n\n        # Confusion matrix (if available)\n        conf_key = next((k for k in summary if k.startswith(\"confusion_matrix\")), None)\n        if conf_key:\n            conf_array = np.array(summary[conf_key])\n            generated_files.append(str(plot_confusion_matrix(conf_array, rid, run_dir)))\n\n        # Aggregated metrics collection\n        best_val_acc = summary.get(\"best_val_acc\", None)\n        aggregated_metrics.append({\"run_id\": rid,\n                                   \"best_val_acc\": best_val_acc,\n                                   \"method\": config.get(\"method\", \"unknown\")})\n\n        # Epoch-wise accuracy list for statistical test\n        if \"validation_accuracy\" in history:\n            epoch_acc = history.dropna(subset=[\"validation_accuracy\"]).groupby(\"epoch\")[\"validation_accuracy\"].max().tolist()\n            epoch_accuracy_values[rid] = epoch_acc\n\n    # --------------------------------------------------\n    # Aggregated analysis\n    # --------------------------------------------------\n    comp_dir = results_dir / \"comparison\"\n    comp_dir.mkdir(exist_ok=True)\n\n    agg_df = pd.DataFrame(aggregated_metrics)\n    save_json(aggregated_metrics, comp_dir / \"aggregated_metrics.json\")\n    agg_df.to_csv(comp_dir / \"aggregated_metrics.csv\", index=False)\n\n    # Improvement rates w.r.t. baseline (first run)\n    baseline_val = agg_df.best_val_acc.iloc[0]\n    agg_df[\"improvement_rate\"] = (agg_df.best_val_acc - baseline_val) / baseline_val\n    agg_df.to_csv(comp_dir / \"aggregated_with_improvement.csv\", index=False)\n\n    # Significance tests \u2013 pairwise t-test on epoch-wise accuracies\n    significance_results = {}\n    run_pairs = [(run_ids[i], run_ids[j]) for i in range(len(run_ids)) for j in range(i + 1, len(run_ids))]\n    for a, b in run_pairs:\n        if a in epoch_accuracy_values and b in epoch_accuracy_values:\n            t_stat, p_val = ttest_ind(epoch_accuracy_values[a], epoch_accuracy_values[b], equal_var=False)\n            significance_results[f\"{a} vs {b}\"] = {\"t_stat\": float(t_stat), \"p_val\": float(p_val)}\n    save_json(significance_results, comp_dir / \"significance_tests.json\")\n\n    # Comparison figures\n    generated_files.append(str(bar_chart(agg_df, comp_dir)))\n    generated_files.append(str(box_plot(epoch_accuracy_values, comp_dir)))\n\n    # --------------------------------------------------\n    # Print generated file paths\n    # --------------------------------------------------\n    print(\"Generated files:\")\n    for p in generated_files:\n        print(p)\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "#!/usr/bin/env python\n\"\"\"src/main.py\nJob orchestrator \u2013 reads Hydra config once then launches the actual training\ncode (src.train) as a subprocess so that the training run has its own Hydra\noutput directory.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport shlex\nimport subprocess\nimport sys\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\ndef _get_run_cfg(cfg: DictConfig):\n    if \"run\" in cfg and cfg.run:\n        return cfg.run\n    if \"runs\" in cfg and cfg.runs:\n        return cfg.runs\n    raise KeyError(\"Cannot locate run configuration \u2013 check Hydra group name.\")\n\n\n@hydra.main(config_path=\"../config\", version_base=None)\ndef main(cfg: DictConfig):\n    # Adjust mode-specific settings early so that subprocess inherits the right overrides\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    run_cfg = _get_run_cfg(cfg)\n\n    cmd = [\n        sys.executable, \"-u\", \"-m\", \"src.train\",\n        f\"run={run_cfg.run_id}\",  # CLI override for Hydra in the child process\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n    cmd_str = \" \".join(shlex.quote(c) for c in cmd)\n    print(\"Launching training subprocess:\\n\", cmd_str)\n    env = os.environ.copy()\n    subprocess.run(cmd, check=True, env=env)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "#!/usr/bin/env python\n\"\"\"src/model.py\nModular model builder (currently wraps HuggingFace + LoRA).\"\"\"\nfrom __future__ import annotations\n\nfrom transformers import AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model\n\nCACHE_DIR = \".cache/\"\n\n\ndef build_model(cfg):\n    num_labels = 3  # GLUE-MNLI default, override as needed\n    base = AutoModelForSequenceClassification.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR,\n                                                              num_labels=num_labels)\n    lora_cfg = LoraConfig(r=cfg.model.adapter.rank,\n                          lora_alpha=cfg.model.adapter.lora_alpha,\n                          target_modules=[\"query_proj\", \"value_proj\"],\n                          bias=\"none\")\n    return get_peft_model(base, lora_cfg)\n", "preprocess_py": "#!/usr/bin/env python\n\"\"\"src/preprocess.py\nStandalone dataset loader \u0026 tokenizer helpers (mirrors logic used in training).\nUseful for quick interactive checks.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\nfrom omegaconf import DictConfig\n\nCACHE_DIR = \".cache/\"\n\n\ndef build_loaders(cfg: DictConfig):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR)\n    dataset = load_dataset(cfg.dataset.name, cfg.dataset.get(\"subset\", None), cache_dir=CACHE_DIR)\n\n    task = cfg.dataset.get(\"subset\", \"mnli\")\n    s1, s2 = {\"mnli\": (\"premise\", \"hypothesis\"), \"rte\": (\"sentence1\", \"sentence2\"), \"sst2\": (\"sentence\", None)}.get(task, (\"premise\", \"hypothesis\"))\n\n    def tok(example):\n        args = (example[s1],) if s2 is None else (example[s1], example[s2])\n        return tokenizer(*args, truncation=True, padding=\"max_length\", max_length=cfg.dataset.max_seq_length)\n\n    dataset = dataset.map(tok, batched=True, load_from_cache_file=True)\n    keep_cols = {\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"}\n    for split in dataset.keys():\n        dataset[split] = dataset[split].remove_columns([c for c in dataset[split].column_names if c not in keep_cols])\n\n    collator = DataCollatorWithPadding(tokenizer)\n    def loader(split, bs, shuf):\n        return DataLoader(dataset[split], batch_size=bs, shuffle=shuf, collate_fn=collator)\n\n    return loader(\"train\", cfg.training.batch_size, True), loader(\"validation\", cfg.training.batch_size, False)\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"iwlora-glue\"\nversion = \"0.1.0\"\nrequires-python = \"\u003e=3.9\"\ndescription = \"Importance-Weighted LoRA experiments on GLUE\"\ndependencies = [\n  \"torch\u003e=2.1\",\n  \"transformers\u003e=4.37\",\n  \"datasets\u003e=2.14\",\n  \"peft\u003e=0.6.2\",\n  \"scikit-learn\u003e=1.3\",\n  \"hydra-core\u003e=1.3\",\n  \"wandb\u003e=0.16\",\n  \"optuna\u003e=3.5\",\n  \"seaborn\u003e=0.13\",\n  \"matplotlib\u003e=3.8\",\n  \"pandas\u003e=2.1\",\n  \"scipy\u003e=1.11\",\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "train_py": "#!/usr/bin/env python\n\"\"\"src/train.py\nSingle-run trainer. Implements Importance-Weighted LoRA (IW-LoRA) with full\nWandB \u0026 Optuna integration. No placeholders \u2013 full, production-ready code.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport hydra\nimport numpy as np\nimport optuna\nimport torch\nimport torch.nn.functional as F  # noqa: F401 (sometimes used in derived methods)\nimport wandb\nfrom datasets import load_dataset\nfrom omegaconf import DictConfig, OmegaConf\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n                          DataCollatorWithPadding, get_linear_schedule_with_warmup)\n\nfrom peft import LoraConfig, get_peft_model\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Helper utilities\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int) -\u003e None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_device() -\u003e torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef get_run_cfg(cfg: DictConfig) -\u003e DictConfig:  # handles run / runs mismatch\n    if \"run\" in cfg and cfg.run:\n        return cfg.run\n    if \"runs\" in cfg and cfg.runs:\n        return cfg.runs\n    raise KeyError(\"Neither cfg.run nor cfg.runs is present \u2013 check Hydra group name.\")\n\n# -----------------------------------------------------------------------------\n# Data pipeline (tokenisation + Dataloaders)\n# -----------------------------------------------------------------------------\n\ndef prepare_dataloaders(cfg: DictConfig) -\u003e Tuple[AutoTokenizer, DataLoader, DataLoader]:\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR)\n\n    dataset = load_dataset(cfg.dataset.name, cfg.dataset.get(\"subset\", None), cache_dir=CACHE_DIR)\n\n    task = cfg.dataset.get(\"subset\", \"mnli\")\n    sentence1_key, sentence2_key = {\n        \"mnli\": (\"premise\", \"hypothesis\"),\n        \"rte\": (\"sentence1\", \"sentence2\"),\n        \"sst2\": (\"sentence\", None),\n    }.get(task, (\"premise\", \"hypothesis\"))\n\n    def tokenize_fn(ex):\n        args = (ex[sentence1_key],) if sentence2_key is None else (ex[sentence1_key], ex[sentence2_key])\n        return tokenizer(*args,\n                         truncation=cfg.dataset.preprocessing.truncation,\n                         padding=cfg.dataset.preprocessing.padding,\n                         max_length=cfg.dataset.max_seq_length)\n\n    dataset = dataset.map(tokenize_fn, batched=True, load_from_cache_file=True)\n    keep_cols = {\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"}\n    for split in dataset.keys():\n        remove_cols = [c for c in dataset[split].column_names if c not in keep_cols]\n        dataset[split] = dataset[split].remove_columns(remove_cols)\n\n    data_collator = DataCollatorWithPadding(tokenizer)\n\n    def make_loader(name: str, shuffle: bool, bs: int):\n        return DataLoader(dataset[name], batch_size=bs, shuffle=shuffle, collate_fn=data_collator)\n\n    train_loader = make_loader(cfg.dataset.splits.train, True, cfg.training.batch_size)\n    val_loader = make_loader(cfg.dataset.splits.validation, False, cfg.training.batch_size)\n    return tokenizer, train_loader, val_loader\n\n# -----------------------------------------------------------------------------\n# Sensitivity collection for IW-LoRA\n# -----------------------------------------------------------------------------\n\ndef collect_sensitivity(model: torch.nn.Module, loader: DataLoader, steps: int) -\u003e Dict[str, float]:\n    device = next(model.parameters()).device\n    sens: Dict[str, float] = {n: 0.0 for n, _ in model.named_parameters() if \"lora_A\" in n}\n    model.train()\n    iterator = iter(loader)\n    pbar = tqdm(total=steps, desc=\"IW-LoRA warm-up\", leave=False)\n    for _ in range(steps):\n        try:\n            batch = next(iterator)\n        except StopIteration:\n            iterator = iter(loader)\n            batch = next(iterator)\n        batch = {k: v.to(device) for k, v in batch.items()}\n        loss = model(**batch).loss\n        loss.backward()\n        for n, p in model.named_parameters():\n            if n in sens and p.grad is not None:\n                sens[n] += p.grad.detach().pow(2).sum().item()\n        model.zero_grad(set_to_none=True)\n        pbar.update(1)\n    pbar.close()\n    total = sum(sens.values()) or 1e-12\n    for k in sens:\n        sens[k] /= total\n    return sens\n\n# -----------------------------------------------------------------------------\n# Evaluation helper returning accuracy + confusion matrix\n# -----------------------------------------------------------------------------\n\ndef evaluate(model: torch.nn.Module, loader: DataLoader) -\u003e Tuple[float, np.ndarray]:\n    device = next(model.parameters()).device\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            logits = outputs.logits\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            labels.extend(batch[\"label\"].cpu().tolist())\n    acc = accuracy_score(labels, preds)\n    conf = confusion_matrix(labels, preds)\n    model.train()\n    return acc, conf\n\n# -----------------------------------------------------------------------------\n# Optuna objective (lightweight single-epoch run)\n# -----------------------------------------------------------------------------\n\ndef optuna_objective(trial: optuna.Trial, base_cfg: DictConfig) -\u003e float:\n    cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True, deepcopy=True))\n    # suggestions\n    lr = trial.suggest_float(\"learning_rate\", cfg.optuna.search_space.learning_rate.low,\n                             cfg.optuna.search_space.learning_rate.high, log=True)\n    bs = trial.suggest_categorical(\"batch_size\", cfg.optuna.search_space.batch_size.choices)\n    cfg.training.learning_rate = lr\n    cfg.training.batch_size = bs\n    if cfg.model.get(\"importance_weighted_regularisation\", {}).get(\"enabled\", False):\n        cfg.model.importance_weighted_regularisation.reg_alpha = trial.suggest_float(\n            \"iw_reg_alpha\", cfg.optuna.search_space.iw_reg_alpha.low,\n            cfg.optuna.search_space.iw_reg_alpha.high, log=True)\n\n    # prepare data\n    _, train_loader, val_loader = prepare_dataloaders(cfg)\n\n    # model\n    num_labels = 3  # MNLI\n    base = AutoModelForSequenceClassification.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR,\n                                                              num_labels=num_labels)\n    lora_cfg = LoraConfig(r=cfg.model.adapter.rank,\n                          lora_alpha=cfg.model.adapter.lora_alpha,\n                          target_modules=[\"query_proj\", \"value_proj\"],\n                          bias=\"none\")\n    model = get_peft_model(base, lora_cfg).to(get_device())\n\n    optim = torch.optim.AdamW(model.parameters(), lr=cfg.training.learning_rate,\n                              weight_decay=cfg.training.weight_decay)\n    scheduler = get_linear_schedule_with_warmup(optim, 0, len(train_loader))\n\n    # train 1 epoch (or up to 200 steps) \u2013 AMP off for simplicity\n    model.train()\n    for step, batch in enumerate(train_loader):\n        batch = {k: v.to(get_device()) for k, v in batch.items()}\n        optim.zero_grad(set_to_none=True)\n        loss = model(**batch).loss\n        loss.backward()\n        optim.step(); scheduler.step()\n        if step \u003e 200:\n            break\n    val_acc, _ = evaluate(model, val_loader)\n    return val_acc\n\n# -----------------------------------------------------------------------------\n# Training entry-point \u2013 full experiment\n# -----------------------------------------------------------------------------\n@hydra.main(config_path=\"../config\", version_base=None)\ndef main(cfg: DictConfig) -\u003e None:  # pylint: disable=too-many-locals,too-many-statements\n    os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"true\")\n    set_seed(cfg.training.get(\"seed\", 42))\n\n    # --------------------------------------------------------------\n    # Robust handling for trial / full mode\n    # --------------------------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.log_every_n_steps = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"cfg.mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    run_cfg = get_run_cfg(cfg)\n\n    # --------------------------------------------------------------\n    # Hyper-parameter search\n    # --------------------------------------------------------------\n    if cfg.optuna.get(\"n_trials\", 0) \u003e 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(lambda t: optuna_objective(t, cfg), n_trials=cfg.optuna.n_trials)\n        for k, v in study.best_params.items():\n            if k == \"learning_rate\":\n                cfg.training.learning_rate = v\n            elif k == \"batch_size\":\n                cfg.training.batch_size = v\n            elif k == \"iw_reg_alpha\":\n                cfg.model.importance_weighted_regularisation.reg_alpha = v\n\n    # --------------------------------------------------------------\n    # Initialise WandB (optional)\n    # --------------------------------------------------------------\n    if cfg.wandb.mode != \"disabled\":\n        wandb.init(entity=cfg.wandb.entity,\n                   project=cfg.wandb.project,\n                   id=run_cfg.run_id,\n                   config=OmegaConf.to_container(cfg, resolve=True),\n                   resume=\"allow\",\n                   mode=cfg.wandb.mode)\n        print(\"WandB URL:\", wandb.run.get_url())\n\n    # --------------------------------------------------------------\n    # Data \u0026 model\n    # --------------------------------------------------------------\n    tokenizer, train_loader, val_loader = prepare_dataloaders(cfg)\n\n    num_labels = 3  # adjust per task if needed\n    base_model = AutoModelForSequenceClassification.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR,\n                                                                    num_labels=num_labels)\n    lora_cfg = LoraConfig(r=cfg.model.adapter.rank,\n                          lora_alpha=cfg.model.adapter.lora_alpha,\n                          target_modules=[\"query_proj\", \"value_proj\"],\n                          bias=\"none\")\n    model = get_peft_model(base_model, lora_cfg).to(get_device())\n\n    # --------------------------------------------------------------\n    # IW-LoRA importance weights\n    # --------------------------------------------------------------\n    importance_weights: Dict[str, float] = {}\n    if cfg.model.importance_weighted_regularisation.enabled:\n        importance_weights = collect_sensitivity(model, train_loader,\n                                                 cfg.model.importance_weighted_regularisation.sensitivity_warmup_steps)\n\n    # --------------------------------------------------------------\n    # Optimiser, scheduler, scaler\n    # --------------------------------------------------------------\n    total_steps = (len(train_loader) * cfg.training.epochs) // cfg.training.gradient_accumulation_steps\n    optimiser = torch.optim.AdamW(model.parameters(), lr=cfg.training.learning_rate,\n                                  weight_decay=cfg.training.weight_decay)\n    scheduler = get_linear_schedule_with_warmup(optimiser, cfg.training.warmup_steps, total_steps)\n    scaler = torch.cuda.amp.GradScaler(enabled=cfg.training.fp16 and torch.cuda.is_available())\n\n    # --------------------------------------------------------------\n    # Training loop\n    # --------------------------------------------------------------\n    best_val_acc = 0.0\n    global_step = 0\n    device = get_device()\n    for epoch in range(cfg.training.epochs):\n        model.train()\n        epoch_loss = 0.0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.training.epochs}\")\n        for step, batch in enumerate(pbar):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with torch.cuda.amp.autocast(enabled=cfg.training.fp16 and torch.cuda.is_available()):\n                outputs = model(**batch)\n                task_loss = outputs.loss\n                # IW-LoRA regularisation\n                if cfg.model.importance_weighted_regularisation.enabled:\n                    reg = 0.0\n                    beta = cfg.model.importance_weighted_regularisation.beta\n                    alpha_reg = cfg.model.importance_weighted_regularisation.reg_alpha\n                    for name, param in model.named_parameters():\n                        if \"lora_A\" in name or \"lora_B\" in name:\n                            key = name.replace(\"lora_B\", \"lora_A\")\n                            w_k = importance_weights.get(key, 0.0)\n                            reg += ((1 - w_k) ** beta) * param.pow(2).sum()\n                    loss = task_loss + alpha_reg * reg\n                else:\n                    loss = task_loss\n            scaler.scale(loss / cfg.training.gradient_accumulation_steps).backward()\n            if (step + 1) % cfg.training.gradient_accumulation_steps == 0:\n                scaler.step(optimiser)\n                scaler.update()\n                optimiser.zero_grad(set_to_none=True)\n                scheduler.step()\n                global_step += 1\n\n            epoch_loss += loss.item()\n            if cfg.wandb.mode != \"disabled\" and global_step % cfg.training.log_every_n_steps == 0:\n                wandb.log({\"train_loss\": loss.item(),\n                           \"lr\": scheduler.get_last_lr()[0],\n                           \"global_step\": global_step}, step=global_step)\n\n            if cfg.mode == \"trial\" and step \u003e 1:  # ultra-fast CI mode\n                break\n        pbar.close()\n\n        # ---------------- Validation ----------------\n        val_acc, conf = evaluate(model, val_loader)\n        best_val_acc = max(best_val_acc, val_acc)\n        if cfg.wandb.mode != \"disabled\":\n            wandb.log({\"validation_accuracy\": val_acc, \"epoch\": epoch + 1}, step=global_step)\n            wandb.summary[f\"confusion_matrix_epoch_{epoch+1}\"] = conf.tolist()\n        print(f\"Epoch {epoch+1} finished \u2013 val acc {val_acc:.4f}\")\n        if cfg.mode == \"trial\":\n            break\n\n    # --------------------------------------------------------------\n    # Wrap-up\n    # --------------------------------------------------------------\n    if cfg.wandb.mode != \"disabled\":\n        wandb.summary[\"best_val_acc\"] = best_val_acc\n        wandb.finish()\n    print(\"Training completed. Best validation acc:\", best_val_acc)\n\nif __name__ == \"__main__\":\n    main()\n"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Hypothesis
open_problems='LoRA distributes the same rank to every weight matrix. AdaLoRA fixes this with iterative SVD‐based pruning but: (1) adds ~10–15 % training time, (2) introduces many scheduler hyper-parameters, (3) is fragile to importance-score estimation errors. We need a cheaper, hyper-parameter-light way to let more important layers use more of the limited LoRA capacity.' method='Importance–Weighted LoRA (IW-LoRA)\nMinimal change: keep standard LoRA training but add a single extra L2 term whose coefficient is different for every weight matrix.\n1. Warm-up (calibration) phase: run 100–200 forward/backward passes with LoRA parameters frozen (or very small lr) and record the running average of per-matrix gradient sensitivity s_k = E[||∂L/∂W_k||_F].\n2. Define importance weight w_k = s_k / (Σ_j s_j).\n3. During actual fine-tuning (LoRA parameters trainable) minimise\n   L_total = L_task  +  α · Σ_k (1 – w_k)^β · ||Δ_k||_F² ,\n   where Δ_k is the LoRA update for matrix k, α≃1e-4 and β=1 (can be fixed).\n   Layers with high w_k receive almost no penalty and can use the rank budget freely, while unimportant layers are softly shrunk toward zero; the fixed global rank therefore re-allocates itself automatically without pruning or SVD.' experimental_setup='Backbone: DeBERTaV3-base.\nDatasets: GLUE (MNLI, SST-2, RTE).\nBaselines: (a) Full fine-tune, (b) LoRA (rank=8 everywhere), (c) AdaLoRA (rank budget equivalent to rank 8).\nIW-LoRA uses the same fixed rank=8 but the new loss term.\nMetrics: dev accuracy (MNLI), F1 (RTE), and average score.\nTraining: 3 epochs, lr 2e-4, batch 32. α=1e-4, β=1 fixed for all runs.\nHardware: single V100 GPU.' experimental_code="# key excerpt – add IW-LoRA regularisation\ndef collect_sensitivity(model, data_loader, steps=200):\n    sens = {n:0. for n,p in model.named_parameters() if 'lora_A' in n}\n    for i, batch in enumerate(data_loader):\n        if i==steps: break\n        loss = model(**batch).loss\n        loss.backward()\n        for n,p in model.named_parameters():\n            if n in sens:\n                sens[n] += p.grad.detach().pow(2).sum()\n        model.zero_grad()\n    # normalise\n    total = sum(sens.values())\n    for n in sens: sens[n] = sens[n]/total\n    return sens\n\nsens = collect_sensitivity(model, train_loader)\n\nalpha, beta = 1e-4, 1.0\n\n# training loop\nfor batch in train_loader:\n    outputs = model(**batch)\n    loss = outputs.loss\n    reg = 0.\n    for n,p in model.named_parameters():\n        if 'lora_A' in n or 'lora_B' in n:  # treat A and B together\n            key = n.replace('lora_A','lora_A') if 'lora_A' in n else n.replace('lora_B','lora_A')\n            reg += (1-sens[key])**beta * p.pow(2).sum()\n    loss = loss + alpha*reg\n    loss.backward()\n    optimizer.step(); optimizer.zero_grad()" expected_result='IW-LoRA should match AdaLoRA and outperform vanilla LoRA while training in roughly the same time as LoRA (≈10-15 % faster than AdaLoRA). On GLUE we expect:\nMNLI accuracy: LoRA 84.6, AdaLoRA 85.3, IW-LoRA ≈85.1.\nSST-2 accuracy: LoRA 93.2, AdaLoRA 93.8, IW-LoRA ≈93.7.\nRTE F1: LoRA 72.0, AdaLoRA 73.4, IW-LoRA ≈73.2.\nAverage gain of ~0.6 points over vanilla LoRA with negligible extra compute.' expected_conclusion='A single importance-weighted L2 term lets the model allocate its fixed low-rank capacity where it matters, approaching AdaLoRA’s accuracy without its computational overhead or complex pruning schedule. The change is only a few lines of code and one new global hyper-parameter (α), making the technique easy to adopt in any LoRA-based fine-tuning pipeline.'

# Current Research Method
Importance–Weighted LoRA (IW-LoRA)
Minimal change: keep standard LoRA training but add a single extra L2 term whose coefficient is different for every weight matrix.
1. Warm-up (calibration) phase: run 100–200 forward/backward passes with LoRA parameters frozen (or very small lr) and record the running average of per-matrix gradient sensitivity s_k = E[||∂L/∂W_k||_F].
2. Define importance weight w_k = s_k / (Σ_j s_j).
3. During actual fine-tuning (LoRA parameters trainable) minimise
   L_total = L_task  +  α · Σ_k (1 – w_k)^β · ||Δ_k||_F² ,
   where Δ_k is the LoRA update for matrix k, α≃1e-4 and β=1 (can be fixed).
   Layers with high w_k receive almost no penalty and can use the rank budget freely, while unimportant layers are softly shrunk toward zero; the fixed global rank therefore re-allocates itself automatically without pruning or SVD.

# Experimental Design
- Summary: The experiment evaluates the new Importance-Weighted LoRA (IW-LoRA) regularisation against the standard LoRA adaptation on a strong encoder model.  
1. We fine-tune a single backbone (DeBERTaV3-base, ≈86 M parameters) on the GLUE MNLI natural-language-understanding benchmark.  
2. A short, 150-step warm-up pass is run with LoRA parameters frozen to collect per-matrix gradient sensitivities s_k.  
3. Sensitivities are normalised to importance weights w_k and stored.  
4. Full fine-tuning then proceeds for three epochs. The ordinary task loss is augmented with the importance-weighted L2 term α·Σ_k(1-w_k)^β‖Δ_k‖² (α searched, β fixed to 1).  
5. We compare final validation accuracy to a vanilla LoRA baseline with identical rank=8 adapters.  
6. All runs use the same optimisation settings (AdamW, three epochs, learning-rate schedule) and are executed on a single NVIDIA A100 (the available environment far exceeds the memory footprint).  
The workflow therefore isolates the effect of the proposed regulariser while keeping every other variable identical.
- Evaluation metrics: ['Accuracy']

# Experiment Runs

- Run ID: proposed-DeBERTaV3-base-86-M--GLUE-MNLI
  Method: proposed
  Model: DeBERTaV3-base (≈86 M)
  Dataset: GLUE-MNLI
  Config File: config/runs/proposed-DeBERTaV3-base-86-M--GLUE-MNLI.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-DeBERTaV3-base-86-M--GLUE-MNLI
method: proposed
model:
  name: microsoft/deberta-v3-base
  adapter:
    type: lora
    rank: 8           # same global rank as baseline
    lora_alpha: 16    # conventional LoRA scaling factor
  importance_weighted_regularisation:
    enabled: true
    reg_alpha: 1e-4   # default value; tuned by Optuna under key ``iw_reg_alpha``
    beta: 1.0
    sensitivity_warmup_steps: 150
    sensitivity_buffer_size: 1024  # mini-batches stored for running average
    freeze_lora_during_warmup: true

dataset:
  name: glue
  subset: mnli
  splits:
    train: train
    validation: validation_matched
  max_seq_length: 256
  preprocessing:
    truncation: true
    padding: max_length

training:
  epochs: 3
  batch_size: 32          # default; can be overridden by Optuna
  learning_rate: 2.0e-4   # default; tuned by Optuna
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler: linear
  warmup_steps: 500
  gradient_accumulation_steps: 1
  fp16: true
  seed: 42
  log_every_n_steps: 50
  save_strategy: epoch
  evaluation_strategy: epoch

optuna:
  direction: maximize                 # maximise validation accuracy
  metric_name: validation_accuracy
  n_trials: 30
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    iw_reg_alpha:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-1-DeBERTaV3-base-86-M--GLUE-MNLI
  Method: comparative-1
  Model: DeBERTaV3-base (≈86 M)
  Dataset: GLUE-MNLI
  Config File: config/runs/comparative-1-DeBERTaV3-base-86-M--GLUE-MNLI.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-DeBERTaV3-base-86-M--GLUE-MNLI
method: comparative-1
model:
  name: microsoft/deberta-v3-base
  adapter:
    type: lora
    rank: 8
    lora_alpha: 16
  importance_weighted_regularisation:
    enabled: false     # vanilla LoRA, no extra regulariser

dataset:
  name: glue
  subset: mnli
  splits:
    train: train
    validation: validation_matched
  max_seq_length: 256
  preprocessing:
    truncation: true
    padding: max_length

training:
  epochs: 3
  batch_size: 32          # default; can be overridden by Optuna
  learning_rate: 2.0e-4   # default; tuned by Optuna
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler: linear
  warmup_steps: 500
  gradient_accumulation_steps: 1
  fp16: true
  seed: 42
  log_every_n_steps: 50
  save_strategy: epoch
  evaluation_strategy: epoch

optuna:
  direction: maximize                 # maximise validation accuracy
  metric_name: validation_accuracy
  n_trials: 30
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices: [16, 32, 64]

  ```
  


# External Resources (Use these for implementation)

**HuggingFace Models:**

- ID: microsoft/deberta-v3-base



**HuggingFace Datasets:**

- ID: Neuronovo/neuronovo-utc-data-glue-mnli




Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "#!/usr/bin/env python\n\"\"\"src/train.py\nSingle-run trainer for Importance-Weighted LoRA experiments.\nThis version fixes three issues reported by CI validation:\n  1.  Trial-mode computational budget is now respected during IW-LoRA\n      sensitivity warm-up (≤ 2 batches).\n  2.  The configuration flag `freeze_lora_during_warmup` is honoured –\n      LoRA parameters have `requires_grad=False` during the warm-up and\n      are restored afterwards.\n  3.  The `results_dir` CLI argument is now created and populated with\n      run-specific artefacts (resolved config, importance weights, final\n      checkpoint).  This allows downstream workflows to pick up the\n      artefacts without relying on WandB alone.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport hydra\nimport numpy as np\nimport optuna\nimport torch\nimport torch.nn.functional as F  # noqa: F401 – kept for possible ablations\nimport wandb\nfrom datasets import load_dataset\nfrom omegaconf import DictConfig, OmegaConf\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n                          DataCollatorWithPadding, get_linear_schedule_with_warmup)\n\nfrom peft import LoraConfig, get_peft_model\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Utility helpers\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_device() -> torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef get_run_cfg(cfg: DictConfig) -> DictConfig:\n    \"\"\"Return the `run` sub-config irrespective of how the Hydra group was\n    named (run / runs).\n    \"\"\"\n    if \"run\" in cfg and cfg.run:\n        return cfg.run\n    if \"runs\" in cfg and cfg.runs:\n        return cfg.runs\n    raise KeyError(\"Cannot locate run configuration – check Hydra group name.\")\n\n# -----------------------------------------------------------------------------\n# Data pipeline\n# -----------------------------------------------------------------------------\n\ndef prepare_dataloaders(cfg: DictConfig) -> Tuple[AutoTokenizer, DataLoader, DataLoader]:\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR)\n\n    dataset = load_dataset(cfg.dataset.name, cfg.dataset.get(\"subset\", None), cache_dir=CACHE_DIR)\n\n    task = cfg.dataset.get(\"subset\", \"mnli\")\n    s1_key, s2_key = {\n        \"mnli\": (\"premise\", \"hypothesis\"),\n        \"rte\": (\"sentence1\", \"sentence2\"),\n        \"sst2\": (\"sentence\", None),\n    }.get(task, (\"premise\", \"hypothesis\"))\n\n    def tok(example):\n        args = (example[s1_key],) if s2_key is None else (example[s1_key], example[s2_key])\n        return tokenizer(*args,\n                         truncation=cfg.dataset.preprocessing.truncation,\n                         padding=cfg.dataset.preprocessing.padding,\n                         max_length=cfg.dataset.max_seq_length)\n\n    dataset = dataset.map(tok, batched=True, load_from_cache_file=True)\n\n    keep_cols = {\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"}\n    for split in dataset.keys():\n        remove = [c for c in dataset[split].column_names if c not in keep_cols]\n        if remove:\n            dataset[split] = dataset[split].remove_columns(remove)\n\n    collator = DataCollatorWithPadding(tokenizer)\n\n    def make_loader(name: str, shuffle: bool, bs: int):\n        return DataLoader(dataset[name], batch_size=bs, shuffle=shuffle, collate_fn=collator)\n\n    train_loader = make_loader(cfg.dataset.splits.train, True, cfg.training.batch_size)\n    val_loader = make_loader(cfg.dataset.splits.validation, False, cfg.training.batch_size)\n    return tokenizer, train_loader, val_loader\n\n# -----------------------------------------------------------------------------\n# IW-LoRA sensitivity warm-up\n# -----------------------------------------------------------------------------\n\ndef collect_sensitivity(model: torch.nn.Module,\n                        loader: DataLoader,\n                        steps: int,\n                        freeze_lora: bool) -> Dict[str, float]:\n    \"\"\"Accumulate squared gradients for LoRA matrices over *steps* batches.\n\n    If *freeze_lora* is True, LoRA parameters are temporarily marked\n    `requires_grad=False` so they remain frozen during warm-up, as\n    requested in the recent validation feedback.\n    \"\"\"\n    device = next(model.parameters()).device\n\n    # ------------------------------------------------------------------\n    # Temporarily freeze LoRA parameters (if requested)\n    # ------------------------------------------------------------------\n    orig_requires_grad: Dict[str, bool] = {}\n    if freeze_lora:\n        for n, p in model.named_parameters():\n            if \"lora_\" in n:\n                orig_requires_grad[n] = p.requires_grad\n                p.requires_grad = False\n\n    # Collect gradient norms\n    sens: Dict[str, float] = {n: 0.0 for n, _ in model.named_parameters() if \"lora_A\" in n}\n    model.train()\n    iterator = iter(loader)\n    pbar = tqdm(total=steps, desc=\"IW-LoRA warm-up\", leave=False)\n\n    for _ in range(steps):\n        try:\n            batch = next(iterator)\n        except StopIteration:  # wrap-around if dataset shorter than *steps*\n            iterator = iter(loader)\n            batch = next(iterator)\n        batch = {k: v.to(device) for k, v in batch.items()}\n        loss = model(**batch).loss\n        loss.backward()\n        for n, p in model.named_parameters():\n            if n in sens and p.grad is not None:\n                sens[n] += p.grad.detach().pow(2).sum().item()\n        model.zero_grad(set_to_none=True)\n        pbar.update(1)\n    pbar.close()\n\n    # Restore original requires_grad flags\n    for n, req in orig_requires_grad.items():\n        for name, p in model.named_parameters():  # iterate again to avoid key lookup on param objects\n            if name == n:\n                p.requires_grad = req\n                break\n\n    total = sum(sens.values()) or 1e-12\n    for k in sens:\n        sens[k] /= total\n    return sens\n\n# -----------------------------------------------------------------------------\n# Validation helper\n# -----------------------------------------------------------------------------\n\ndef evaluate(model: torch.nn.Module, loader: DataLoader) -> Tuple[float, np.ndarray]:\n    device = next(model.parameters()).device\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(**batch).logits\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            labels.extend(batch[\"label\"].cpu().tolist())\n    acc = accuracy_score(labels, preds)\n    conf = confusion_matrix(labels, preds)\n    model.train()\n    return acc, conf\n\n# -----------------------------------------------------------------------------\n# Optuna objective (unchanged)\n# -----------------------------------------------------------------------------\n\ndef optuna_objective(trial: optuna.Trial, base_cfg: DictConfig) -> float:  # noqa: C901 – complex but fine\n    cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True, deepcopy=True))\n\n    # Hyper-param suggestions\n    lr = trial.suggest_float(\"learning_rate\", cfg.optuna.search_space.learning_rate.low,\n                             cfg.optuna.search_space.learning_rate.high, log=True)\n    bs = trial.suggest_categorical(\"batch_size\", cfg.optuna.search_space.batch_size.choices)\n    cfg.training.learning_rate = lr\n    cfg.training.batch_size = bs\n\n    if cfg.model.get(\"importance_weighted_regularisation\", {}).get(\"enabled\", False):\n        cfg.model.importance_weighted_regularisation.reg_alpha = trial.suggest_float(\n            \"iw_reg_alpha\", cfg.optuna.search_space.iw_reg_alpha.low,\n            cfg.optuna.search_space.iw_reg_alpha.high, log=True)\n\n    # Data\n    _, train_loader, val_loader = prepare_dataloaders(cfg)\n\n    # Model\n    num_labels = 3\n    backbone = AutoModelForSequenceClassification.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR,\n                                                                  num_labels=num_labels)\n    lora_cfg = LoraConfig(r=cfg.model.adapter.rank,\n                          lora_alpha=cfg.model.adapter.lora_alpha,\n                          target_modules=[\"query_proj\", \"value_proj\"],\n                          bias=\"none\")\n    model = get_peft_model(backbone, lora_cfg).to(get_device())\n\n    optim = torch.optim.AdamW(model.parameters(), lr=cfg.training.learning_rate,\n                              weight_decay=cfg.training.weight_decay)\n    scheduler = get_linear_schedule_with_warmup(optim, 0, len(train_loader))\n\n    # Single-epoch lightweight run (≤ 200 steps)\n    model.train()\n    for step, batch in enumerate(train_loader):\n        batch = {k: v.to(get_device()) for k, v in batch.items()}\n        optim.zero_grad(set_to_none=True)\n        loss = model(**batch).loss\n        loss.backward()\n        optim.step(); scheduler.step()\n        if step >= 200:\n            break\n\n    val_acc, _ = evaluate(model, val_loader)\n    return val_acc\n\n# -----------------------------------------------------------------------------\n# Main training routine\n# -----------------------------------------------------------------------------\n@hydra.main(config_path=\"../config\", version_base=None)\ndef main(cfg: DictConfig) -> None:  # noqa: C901 – complex but fine\n    os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"true\")\n    set_seed(cfg.training.get(\"seed\", 42))\n\n    # --------------------------------------------------------------\n    # Mode-specific overrides (trial / full)\n    # --------------------------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.log_every_n_steps = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"cfg.mode must be 'trial' or 'full'\")\n\n    run_cfg = get_run_cfg(cfg)\n\n    # --------------------------------------------------------------\n    # Prepare results directory early so that artefacts are captured\n    # --------------------------------------------------------------\n    results_root = Path(cfg.results_dir).expanduser().absolute() / run_cfg.run_id\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    # Save resolved configuration for reproducibility\n    OmegaConf.save(cfg, results_root / \"config_resolved.yaml\")\n\n    # --------------------------------------------------------------\n    # Hyper-parameter optimisation (Optuna)\n    # --------------------------------------------------------------\n    if cfg.optuna.get(\"n_trials\", 0) > 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(lambda t: optuna_objective(t, cfg), n_trials=cfg.optuna.n_trials)\n        for k, v in study.best_params.items():\n            if k == \"learning_rate\":\n                cfg.training.learning_rate = v\n            elif k == \"batch_size\":\n                cfg.training.batch_size = v\n            elif k == \"iw_reg_alpha\":\n                cfg.model.importance_weighted_regularisation.reg_alpha = v\n\n    # --------------------------------------------------------------\n    # WandB initialisation (if enabled)\n    # --------------------------------------------------------------\n    use_wandb = cfg.wandb.mode != \"disabled\"\n    if use_wandb:\n        wandb.init(entity=cfg.wandb.entity,\n                   project=cfg.wandb.project,\n                   id=run_cfg.run_id,\n                   config=OmegaConf.to_container(cfg, resolve=True),\n                   resume=\"allow\",\n                   mode=cfg.wandb.mode)\n        print(\"WandB URL:\", wandb.run.get_url())\n\n    # --------------------------------------------------------------\n    # Data & model\n    # --------------------------------------------------------------\n    _, train_loader, val_loader = prepare_dataloaders(cfg)\n\n    num_labels = 3  # MNLI fixed here; adjust for other tasks if needed\n    backbone = AutoModelForSequenceClassification.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR,\n                                                                  num_labels=num_labels)\n    lora_cfg = LoraConfig(r=cfg.model.adapter.rank,\n                          lora_alpha=cfg.model.adapter.lora_alpha,\n                          target_modules=[\"query_proj\", \"value_proj\"],\n                          bias=\"none\")\n    model = get_peft_model(backbone, lora_cfg).to(get_device())\n\n    # --------------------------------------------------------------\n    # IW-LoRA sensitivity warm-up\n    # --------------------------------------------------------------\n    importance_weights: Dict[str, float] = {}\n    if cfg.model.importance_weighted_regularisation.enabled:\n        warmup_steps = cfg.model.importance_weighted_regularisation.sensitivity_warmup_steps\n        if cfg.mode == \"trial\":\n            warmup_steps = min(2, warmup_steps)  # keep CI fast\n        importance_weights = collect_sensitivity(\n            model,\n            train_loader,\n            steps=warmup_steps,\n            freeze_lora=cfg.model.importance_weighted_regularisation.get(\"freeze_lora_during_warmup\", False),\n        )\n\n        # Store weights for transparency\n        (results_root / \"importance_weights.json\").write_text(json.dumps(importance_weights, indent=2))\n\n    # --------------------------------------------------------------\n    # Optimiser / scheduler / AMP scaler\n    # --------------------------------------------------------------\n    total_steps = (len(train_loader) * cfg.training.epochs) // cfg.training.gradient_accumulation_steps\n    optimiser = torch.optim.AdamW(model.parameters(), lr=cfg.training.learning_rate,\n                                  weight_decay=cfg.training.weight_decay)\n    scheduler = get_linear_schedule_with_warmup(optimiser, cfg.training.warmup_steps, total_steps)\n    scaler = torch.cuda.amp.GradScaler(enabled=cfg.training.fp16 and torch.cuda.is_available())\n\n    # --------------------------------------------------------------\n    # Training loop\n    # --------------------------------------------------------------\n    best_val_acc = 0.0\n    global_step = 0\n    device = get_device()\n\n    for epoch in range(cfg.training.epochs):\n        model.train()\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.training.epochs}\")\n        for step, batch in enumerate(pbar):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with torch.cuda.amp.autocast(enabled=cfg.training.fp16 and torch.cuda.is_available()):\n                outputs = model(**batch)\n                task_loss = outputs.loss\n                if cfg.model.importance_weighted_regularisation.enabled:\n                    beta = cfg.model.importance_weighted_regularisation.beta\n                    alpha_reg = cfg.model.importance_weighted_regularisation.reg_alpha\n                    reg = 0.0\n                    for name, param in model.named_parameters():\n                        if \"lora_A\" in name or \"lora_B\" in name:\n                            key = name.replace(\"lora_B\", \"lora_A\")\n                            w_k = importance_weights.get(key, 0.0)\n                            reg += ((1 - w_k) ** beta) * param.pow(2).sum()\n                    loss = task_loss + alpha_reg * reg\n                else:\n                    loss = task_loss\n\n            scaler.scale(loss / cfg.training.gradient_accumulation_steps).backward()\n            if (step + 1) % cfg.training.gradient_accumulation_steps == 0:\n                scaler.step(optimiser)\n                scaler.update()\n                optimiser.zero_grad(set_to_none=True)\n                scheduler.step()\n                global_step += 1\n\n            # Logging\n            if use_wandb and global_step % cfg.training.log_every_n_steps == 0:\n                wandb.log({\n                    \"train_loss\": loss.item(),\n                    \"lr\": scheduler.get_last_lr()[0],\n                    \"global_step\": global_step,\n                }, step=global_step)\n\n            # Ultra-light trial mode – at most two optimisation steps\n            if cfg.mode == \"trial\" and step >= 1:\n                break\n        pbar.close()\n\n        # ---------------- Validation ----------------\n        val_acc, conf = evaluate(model, val_loader)\n        best_val_acc = max(best_val_acc, val_acc)\n        if use_wandb:\n            wandb.log({\"validation_accuracy\": val_acc, \"epoch\": epoch + 1}, step=global_step)\n            wandb.summary[f\"confusion_matrix_epoch_{epoch+1}\"] = conf.tolist()\n        print(f\"Epoch {epoch+1} complete – val acc {val_acc:.4f}\")\n\n        if cfg.mode == \"trial\":  # only one epoch in trial\n            break\n\n    # --------------------------------------------------------------\n    # Save final checkpoint locally (optional)\n    # --------------------------------------------------------------\n    ckpt_path = results_root / \"pytorch_model.bin\"\n    torch.save(model.state_dict(), ckpt_path)\n\n    # --------------------------------------------------------------\n    # Wrap-up & WandB summary\n    # --------------------------------------------------------------\n    if use_wandb:\n        wandb.summary[\"best_val_acc\"] = best_val_acc\n        wandb.finish()\n\n    print(\"Training finished. Best validation accuracy:\", best_val_acc)\n    print(\"Local artefacts saved to:\", results_root)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "#!/usr/bin/env python\n\"\"\"src/evaluate.py\n(unchanged from the previous submission – included here for completeness)\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom scipy.stats import ttest_ind\nfrom tqdm.auto import tqdm\n\n\ndef save_json(obj: dict, path: Path) -> None:\n    path.write_text(json.dumps(obj, indent=2))\n\n\ndef plot_learning_curve(history: pd.DataFrame, run_id: str, out_dir: Path) -> Path:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    if \"train_loss\" in history:\n        ax.plot(history[\"_step\"], history[\"train_loss\"], label=\"train_loss\")\n    if \"validation_accuracy\" in history:\n        ax.plot(history[\"_step\"], history[\"validation_accuracy\"], label=\"val_acc\")\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Metric value\")\n    ax.set_title(f\"Learning curves – {run_id}\")\n    ax.legend()\n    fig.tight_layout()\n    path = out_dir / f\"{run_id}_learning_curve.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef plot_confusion_matrix(conf: np.ndarray, run_id: str, out_dir: Path) -> Path:\n    fig, ax = plt.subplots(figsize=(4, 4))\n    sns.heatmap(conf, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"True\")\n    ax.set_title(f\"Confusion matrix – {run_id}\")\n    fig.tight_layout()\n    path = out_dir / f\"{run_id}_confusion_matrix.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef bar_chart(agg: pd.DataFrame, out_dir: Path) -> Path:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.barplot(data=agg, x=\"run_id\", y=\"best_val_acc\", hue=\"method\", ax=ax)\n    for i, v in enumerate(agg.best_val_acc):\n        ax.text(i, v + 0.002, f\"{v:.3f}\", ha=\"center\")\n    ax.set_ylabel(\"Best validation accuracy\")\n    ax.set_xlabel(\"\")\n    ax.set_ylim(0, agg.best_val_acc.max() * 1.05)\n    fig.tight_layout()\n    path = out_dir / \"comparison_accuracy_bar_chart.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef box_plot(values: Dict[str, List[float]], out_dir: Path) -> Path:\n    df = pd.DataFrame({\"run_id\": np.repeat(list(values.keys()), [len(v) for v in values.values()]),\n                       \"val_acc\": np.concatenate(list(values.values()))})\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.boxplot(data=df, x=\"run_id\", y=\"val_acc\", ax=ax)\n    fig.tight_layout()\n    path = out_dir / \"comparison_val_accuracy_boxplot.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Evaluation & visualisation – WandB fetcher\")\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory where outputs will be saved\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON string list of WandB run IDs\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir).expanduser().absolute()\n    results_dir.mkdir(parents=True, exist_ok=True)\n    run_ids: List[str] = json.loads(args.run_ids)\n\n    # Load global WandB settings\n    import yaml\n    cfg_path = Path(__file__).resolve().parent.parent / \"config\" / \"config.yaml\"\n    with cfg_path.open() as f:\n        global_cfg = yaml.safe_load(f)\n    entity = global_cfg[\"wandb\"][\"entity\"]\n    project = global_cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    aggregated_metrics: List[dict] = []\n    epoch_accuracy_values: Dict[str, List[float]] = {}\n    generated_files: List[str] = []\n\n    # ---------------- Per-run processing ----------------\n    for rid in tqdm(run_ids, desc=\"Processing runs\"):\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        history = run.history(samples=100000)\n        summary = run.summary._json_dict\n        config = dict(run.config)\n\n        run_dir = results_dir / rid\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save metrics & config\n        save_json({\"history\": history.to_dict(orient=\"list\"),\n                   \"summary\": summary,\n                   \"config\": config}, run_dir / \"metrics.json\")\n\n        # Figures\n        generated_files.append(str(plot_learning_curve(history, rid, run_dir)))\n        conf_key = next((k for k in summary if k.startswith(\"confusion_matrix_epoch\")), None)\n        if conf_key:\n            conf_arr = np.array(summary[conf_key])\n            generated_files.append(str(plot_confusion_matrix(conf_arr, rid, run_dir)))\n\n        # Aggregation helpers\n        best_val_acc = summary.get(\"best_val_acc\", None)\n        aggregated_metrics.append({\"run_id\": rid,\n                                   \"best_val_acc\": best_val_acc,\n                                   \"method\": config.get(\"method\", \"unknown\")})\n\n        if \"validation_accuracy\" in history:\n            epoch_vals = history.dropna(subset=[\"validation_accuracy\"]).groupby(\"epoch\")[\"validation_accuracy\"].max().tolist()\n            epoch_accuracy_values[rid] = epoch_vals\n\n    # ---------------- Aggregated analysis ----------------\n    comp_dir = results_dir / \"comparison\"\n    comp_dir.mkdir(exist_ok=True)\n\n    agg_df = pd.DataFrame(aggregated_metrics)\n    save_json(aggregated_metrics, comp_dir / \"aggregated_metrics.json\")\n    agg_df.to_csv(comp_dir / \"aggregated_metrics.csv\", index=False)\n\n    baseline_val = agg_df.best_val_acc.iloc[0]\n    agg_df[\"improvement_rate\"] = (agg_df.best_val_acc - baseline_val) / baseline_val\n    agg_df.to_csv(comp_dir / \"aggregated_with_improvement.csv\", index=False)\n\n    significance_results = {}\n    run_pairs = [(run_ids[i], run_ids[j]) for i in range(len(run_ids)) for j in range(i + 1, len(run_ids))]\n    for a, b in run_pairs:\n        if a in epoch_accuracy_values and b in epoch_accuracy_values:\n            t_stat, p_val = ttest_ind(epoch_accuracy_values[a], epoch_accuracy_values[b], equal_var=False)\n            significance_results[f\"{a} vs {b}\"] = {\"t_stat\": float(t_stat), \"p_val\": float(p_val)}\n    save_json(significance_results, comp_dir / \"significance_tests.json\")\n\n    generated_files.append(str(bar_chart(agg_df, comp_dir)))\n    generated_files.append(str(box_plot(epoch_accuracy_values, comp_dir)))\n\n    # Print generated file paths so CI can fetch them if needed\n    print(\"Generated files:\")\n    for p in generated_files:\n        print(p)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "#!/usr/bin/env python\n\"\"\"src/preprocess.py – utility to build DataLoaders interactively.\nUnchanged from previous version (kept for completeness).\"\"\"\nfrom __future__ import annotations\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\nfrom omegaconf import DictConfig\n\nCACHE_DIR = \".cache/\"\n\n\ndef build_loaders(cfg: DictConfig):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR)\n    dataset = load_dataset(cfg.dataset.name, cfg.dataset.get(\"subset\", None), cache_dir=CACHE_DIR)\n\n    task = cfg.dataset.get(\"subset\", \"mnli\")\n    s1, s2 = {\"mnli\": (\"premise\", \"hypothesis\"), \"rte\": (\"sentence1\", \"sentence2\"), \"sst2\": (\"sentence\", None)}.get(task, (\"premise\", \"hypothesis\"))\n\n    def tok(example):\n        args = (example[s1],) if s2 is None else (example[s1], example[s2])\n        return tokenizer(*args, truncation=True, padding=\"max_length\", max_length=cfg.dataset.max_seq_length)\n\n    dataset = dataset.map(tok, batched=True, load_from_cache_file=True)\n    keep_cols = {\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"}\n    for split in dataset.keys():\n        dataset[split] = dataset[split].remove_columns([c for c in dataset[split].column_names if c not in keep_cols])\n\n    collator = DataCollatorWithPadding(tokenizer)\n\n    def loader(split, bs, shuf):\n        return DataLoader(dataset[split], batch_size=bs, shuffle=shuf, collate_fn=collator)\n\n    return loader(\"train\", cfg.training.batch_size, True), loader(\"validation\", cfg.training.batch_size, False)\n",
    "model_py": "#!/usr/bin/env python\n\"\"\"src/model.py – minimal builder (unchanged).\"\"\"\nfrom __future__ import annotations\n\nfrom transformers import AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model\n\nCACHE_DIR = \".cache/\"\n\n\ndef build_model(cfg):\n    num_labels = 3  # GLUE-MNLI\n    backbone = AutoModelForSequenceClassification.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR,\n                                                                  num_labels=num_labels)\n    lora_cfg = LoraConfig(r=cfg.model.adapter.rank,\n                          lora_alpha=cfg.model.adapter.lora_alpha,\n                          target_modules=[\"query_proj\", \"value_proj\"],\n                          bias=\"none\")\n    return get_peft_model(backbone, lora_cfg)\n",
    "main_py": "#!/usr/bin/env python\n\"\"\"src/main.py\nOrchestrator (unchanged except for minor comments).\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport shlex\nimport subprocess\nimport sys\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(config_path=\"../config\", version_base=None)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n\n    # Determine run sub-config\n    run_cfg = cfg.run if \"run\" in cfg and cfg.run else cfg.runs\n\n    cmd = [\n        sys.executable, \"-u\", \"-m\", \"src.train\",\n        f\"run={run_cfg.run_id}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n    print(\"Launching training subprocess:\\n\", \" \".join(shlex.quote(c) for c in cmd))\n    env = os.environ.copy()\n    subprocess.run(cmd, check=True, env=env)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"iwlora-glue\"\nversion = \"0.1.0\"\nrequires-python = \">=3.9\"\ndescription = \"Importance-Weighted LoRA experiments on GLUE\"\ndependencies = [\n  \"torch>=2.1\",\n  \"transformers>=4.37\",\n  \"datasets>=2.14\",\n  \"peft>=0.6.2\",\n  \"scikit-learn>=1.3\",\n  \"hydra-core>=1.3\",\n  \"wandb>=0.16\",\n  \"optuna>=3.5\",\n  \"seaborn>=0.13\",\n  \"matplotlib>=3.8\",\n  \"pandas>=2.1\",\n  \"scipy>=1.11\",\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n",
    "config_yaml": "# config/config.yaml\n# -----------------------------------------------------------------------------\n# Global Hydra configuration.\n# -----------------------------------------------------------------------------\n\ndefaults:\n  - run: null   # overridden from CLI – do not change\n\nwandb:\n  entity: gengaru617-personal\n  project: 251106-test\n  mode: online  # switched to \"disabled\" automatically in trial\n\n# Mode (set by CLI) – required so that Hydra is aware of the key\nmode: full\n\n# Where to place local artefacts (overridden from CI)\nresults_dir: ./results\n\n# Placeholder groups – populated by run-specific configs\ntraining: {}\noptuna: {}\nmodel: {}\ndataset: {}\nrun: {}\n"
}
