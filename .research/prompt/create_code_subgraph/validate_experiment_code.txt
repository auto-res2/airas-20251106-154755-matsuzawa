
LLM Name: o3-2025-04-16
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/runs/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training (full): `uv run python -u -m src.main run={run_id} results_dir={path} mode=full`
     * Training (trial): `uv run python -u -m src.main run={run_id} results_dir={path} mode=trial`
     * Evaluation: `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'` (independent execution)
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics comprehensively:
     * Use `wandb.log()` at each training step/batch/epoch with ALL relevant time-series metrics
     * Log as frequently as possible (per-batch or per-epoch) to capture complete training dynamics
     * Use `wandb.summary["key"] = value` to save final/best metrics (best_val_acc, final_test_acc, best_epoch, etc.)
     * Metric names in train.py's wandb.log() MUST exactly match the keys used in evaluate.py's run.history()
   - Optuna Integration: If using Optuna, DO NOT log intermediate trial results to WandB - only log the final run with best hyperparameters
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"` before any WandB operations
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` before any WandB operations
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)
   - `WANDB_API_KEY` environment variable is available for authentication

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/runs/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
   - Accepts `run_ids` parameter as JSON string list (parse with `json.loads(args.run_ids)`)
   - main.py DOES NOT call evaluate.py
   - evaluate.py loads WandB config from `config/config.yaml` (in repository root)
   - evaluate.py retrieves comprehensive data from WandB API:
     * Use `wandb.Api()` to get run data: `run = api.run(f"{entity}/{project}/{run_id}")`
     * Retrieve: `history = run.history()`, `summary = run.summary._json_dict`, `config = dict(run.config)`
   - **STEP 1: Per-Run Processing** (for each run_id):
     * Export comprehensive run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
     * Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
     * Each run should have its own subdirectory with its metrics and figures
   - **STEP 2: Aggregated Analysis** (after processing all runs):
     * Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
     * Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
     * Generate comparison figures to: `{results_dir}/comparison/`
     * Cross-run comparison charts (bar charts, box plots)
     * Performance metrics tables
     * Statistical significance tests
   - Proper figure quality: legends, annotations, tight_layout
   - Follows GLOBALLY UNIQUE naming convention to prevent collisions:
     * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
     * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Mode-Based Implementation**:
   - `mode` parameter controls experiment behavior (required parameter)
   - When `cfg.mode == "trial"`:
     * Properly reduces computational load: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
     * Automatically sets `cfg.wandb.mode = "disabled"`
     * Purpose: Fast validation that code runs without errors
   - When `cfg.mode == "full"`:
     * Automatically sets `cfg.wandb.mode = "online"`
     * Uses full configuration (full epochs, full Optuna trials, etc.)

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Hypothesis
open_problems='LoRA distributes the same rank to every weight matrix. AdaLoRA fixes this with iterative SVD‐based pruning but: (1) adds ~10–15 % training time, (2) introduces many scheduler hyper-parameters, (3) is fragile to importance-score estimation errors. We need a cheaper, hyper-parameter-light way to let more important layers use more of the limited LoRA capacity.' method='Importance–Weighted LoRA (IW-LoRA)\nMinimal change: keep standard LoRA training but add a single extra L2 term whose coefficient is different for every weight matrix.\n1. Warm-up (calibration) phase: run 100–200 forward/backward passes with LoRA parameters frozen (or very small lr) and record the running average of per-matrix gradient sensitivity s_k = E[||∂L/∂W_k||_F].\n2. Define importance weight w_k = s_k / (Σ_j s_j).\n3. During actual fine-tuning (LoRA parameters trainable) minimise\n   L_total = L_task  +  α · Σ_k (1 – w_k)^β · ||Δ_k||_F² ,\n   where Δ_k is the LoRA update for matrix k, α≃1e-4 and β=1 (can be fixed).\n   Layers with high w_k receive almost no penalty and can use the rank budget freely, while unimportant layers are softly shrunk toward zero; the fixed global rank therefore re-allocates itself automatically without pruning or SVD.' experimental_setup='Backbone: DeBERTaV3-base.\nDatasets: GLUE (MNLI, SST-2, RTE).\nBaselines: (a) Full fine-tune, (b) LoRA (rank=8 everywhere), (c) AdaLoRA (rank budget equivalent to rank 8).\nIW-LoRA uses the same fixed rank=8 but the new loss term.\nMetrics: dev accuracy (MNLI), F1 (RTE), and average score.\nTraining: 3 epochs, lr 2e-4, batch 32. α=1e-4, β=1 fixed for all runs.\nHardware: single V100 GPU.' experimental_code="# key excerpt – add IW-LoRA regularisation\ndef collect_sensitivity(model, data_loader, steps=200):\n    sens = {n:0. for n,p in model.named_parameters() if 'lora_A' in n}\n    for i, batch in enumerate(data_loader):\n        if i==steps: break\n        loss = model(**batch).loss\n        loss.backward()\n        for n,p in model.named_parameters():\n            if n in sens:\n                sens[n] += p.grad.detach().pow(2).sum()\n        model.zero_grad()\n    # normalise\n    total = sum(sens.values())\n    for n in sens: sens[n] = sens[n]/total\n    return sens\n\nsens = collect_sensitivity(model, train_loader)\n\nalpha, beta = 1e-4, 1.0\n\n# training loop\nfor batch in train_loader:\n    outputs = model(**batch)\n    loss = outputs.loss\n    reg = 0.\n    for n,p in model.named_parameters():\n        if 'lora_A' in n or 'lora_B' in n:  # treat A and B together\n            key = n.replace('lora_A','lora_A') if 'lora_A' in n else n.replace('lora_B','lora_A')\n            reg += (1-sens[key])**beta * p.pow(2).sum()\n    loss = loss + alpha*reg\n    loss.backward()\n    optimizer.step(); optimizer.zero_grad()" expected_result='IW-LoRA should match AdaLoRA and outperform vanilla LoRA while training in roughly the same time as LoRA (≈10-15 % faster than AdaLoRA). On GLUE we expect:\nMNLI accuracy: LoRA 84.6, AdaLoRA 85.3, IW-LoRA ≈85.1.\nSST-2 accuracy: LoRA 93.2, AdaLoRA 93.8, IW-LoRA ≈93.7.\nRTE F1: LoRA 72.0, AdaLoRA 73.4, IW-LoRA ≈73.2.\nAverage gain of ~0.6 points over vanilla LoRA with negligible extra compute.' expected_conclusion='A single importance-weighted L2 term lets the model allocate its fixed low-rank capacity where it matters, approaching AdaLoRA’s accuracy without its computational overhead or complex pruning schedule. The change is only a few lines of code and one new global hyper-parameter (α), making the technique easy to adopt in any LoRA-based fine-tuning pipeline.'

# Current Research Method
Importance–Weighted LoRA (IW-LoRA)
Minimal change: keep standard LoRA training but add a single extra L2 term whose coefficient is different for every weight matrix.
1. Warm-up (calibration) phase: run 100–200 forward/backward passes with LoRA parameters frozen (or very small lr) and record the running average of per-matrix gradient sensitivity s_k = E[||∂L/∂W_k||_F].
2. Define importance weight w_k = s_k / (Σ_j s_j).
3. During actual fine-tuning (LoRA parameters trainable) minimise
   L_total = L_task  +  α · Σ_k (1 – w_k)^β · ||Δ_k||_F² ,
   where Δ_k is the LoRA update for matrix k, α≃1e-4 and β=1 (can be fixed).
   Layers with high w_k receive almost no penalty and can use the rank budget freely, while unimportant layers are softly shrunk toward zero; the fixed global rank therefore re-allocates itself automatically without pruning or SVD.

# Experimental Design
- Strategy: The experiment evaluates the new Importance-Weighted LoRA (IW-LoRA) regularisation against the standard LoRA adaptation on a strong encoder model.  
1. We fine-tune a single backbone (DeBERTaV3-base, ≈86 M parameters) on the GLUE MNLI natural-language-understanding benchmark.  
2. A short, 150-step warm-up pass is run with LoRA parameters frozen to collect per-matrix gradient sensitivities s_k.  
3. Sensitivities are normalised to importance weights w_k and stored.  
4. Full fine-tuning then proceeds for three epochs. The ordinary task loss is augmented with the importance-weighted L2 term α·Σ_k(1-w_k)^β‖Δ_k‖² (α searched, β fixed to 1).  
5. We compare final validation accuracy to a vanilla LoRA baseline with identical rank=8 adapters.  
6. All runs use the same optimisation settings (AdamW, three epochs, learning-rate schedule) and are executed on a single NVIDIA A100 (the available environment far exceeds the memory footprint).  
The workflow therefore isolates the effect of the proposed regulariser while keeping every other variable identical.
- Proposed Method: Importance-Weighted LoRA (IW-LoRA) is a drop-in regularisation for LoRA fine-tuning that redistributes a fixed low-rank budget toward layers that matter most.  
• Theory: For each trainable weight matrix W_k equipped with a rank-r LoRA update Δ_k = A_kB_k, we approximate its task importance with the Frobenius norm of the gradient ‖∂L/∂W_k‖_F averaged over a small calibration run.  
• Calibration: Freeze LoRA parameters, run 100–200 forward/backward passes, accumulate s_k = E[‖∂L/∂W_k‖_F].  
• Importance weights: w_k = s_k / Σ_j s_j.  
• Training objective: L_total = L_task + α Σ_k (1−w_k)^β ‖Δ_k‖_F² with global α (searched) and β=1. High-importance layers (large w_k) receive almost no penalty; low-importance layers are softly shrunk, effectively donating rank capacity to useful layers without explicit pruning or SVD.  
• Implementation: only ~15 lines added to a standard LoRA training loop—store sensitivities, then, during each training step, compute the weighted L2 regulariser over all LoRA parameters and add it to the loss before back-propagation.  
• Hyper-parameter economy: only α needs tuning; β can stay at 1 for all tasks.  
• Computational cost: negligible—no iterative SVD, schedulers, or additional optimiser steps; calibration adds <2 % wall-time.
- Evaluation Metrics: ['Accuracy']

# Experiment Runs

- Run ID: proposed-DeBERTaV3-base-86-M--GLUE-MNLI
  Method: proposed
  Model: DeBERTaV3-base (≈86 M)
  Dataset: GLUE-MNLI
  
  Config Content:
  ```yaml
  run_id: proposed-DeBERTaV3-base-86-M--GLUE-MNLI
method: proposed
model:
  name: microsoft/deberta-v3-base
  adapter:
    type: lora
    rank: 8           # same global rank as baseline
    lora_alpha: 16    # conventional LoRA scaling factor
  importance_weighted_regularisation:
    enabled: true
    reg_alpha: 1e-4   # default value; tuned by Optuna under key ``iw_reg_alpha``
    beta: 1.0
    sensitivity_warmup_steps: 150
    sensitivity_buffer_size: 1024  # mini-batches stored for running average
    freeze_lora_during_warmup: true

dataset:
  name: glue
  subset: mnli
  splits:
    train: train
    validation: validation_matched
  max_seq_length: 256
  preprocessing:
    truncation: true
    padding: max_length

training:
  epochs: 3
  batch_size: 32          # default; can be overridden by Optuna
  learning_rate: 2.0e-4   # default; tuned by Optuna
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler: linear
  warmup_steps: 500
  gradient_accumulation_steps: 1
  fp16: true
  seed: 42
  log_every_n_steps: 50
  save_strategy: epoch
  evaluation_strategy: epoch

optuna:
  direction: maximize                 # maximise validation accuracy
  metric_name: validation_accuracy
  n_trials: 30
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    iw_reg_alpha:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-1-DeBERTaV3-base-86-M--GLUE-MNLI
  Method: comparative-1
  Model: DeBERTaV3-base (≈86 M)
  Dataset: GLUE-MNLI
  
  Config Content:
  ```yaml
  run_id: comparative-1-DeBERTaV3-base-86-M--GLUE-MNLI
method: comparative-1
model:
  name: microsoft/deberta-v3-base
  adapter:
    type: lora
    rank: 8
    lora_alpha: 16
  importance_weighted_regularisation:
    enabled: false     # vanilla LoRA, no extra regulariser

dataset:
  name: glue
  subset: mnli
  splits:
    train: train
    validation: validation_matched
  max_seq_length: 256
  preprocessing:
    truncation: true
    padding: max_length

training:
  epochs: 3
  batch_size: 32          # default; can be overridden by Optuna
  learning_rate: 2.0e-4   # default; tuned by Optuna
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler: linear
  warmup_steps: 500
  gradient_accumulation_steps: 1
  fp16: true
  seed: 42
  log_every_n_steps: 50
  save_strategy: epoch
  evaluation_strategy: epoch

optuna:
  direction: maximize                 # maximise validation accuracy
  metric_name: validation_accuracy
  n_trials: 30
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    batch_size:
      type: categorical
      choices: [16, 32, 64]

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "# config/config.yaml\n# -----------------------------------------------------------------------------\n# Global Hydra configuration. The *run* group will be overridden from CLI via\n# `run=\u003crun_id\u003e` where `\u003crun_id\u003e` is a filename inside `config/runs/`.\n# -----------------------------------------------------------------------------\n\ndefaults:\n  - run: null   # overwritten from CLI \u2013 DO NOT rename this line\n\n# -----------------------------------------------------------------------------\n# WandB \u2013 overridden automatically depending on mode (trial/full)\n# -----------------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 251106-test\n  mode: online\n\n# Runtime mode (trial / full)\nmode: full\n\n# Where to place outputs (overridden from CI)\nresults_dir: ./results\n\n# Placeholder groups \u2013 will be populated by run-specific configs\ntraining: {}\noptuna: {}\nmodel: {}\ndataset: {}\nrun: {}\n", "evaluate_py": "#!/usr/bin/env python\n\"\"\"src/evaluate.py\nIndependent evaluation \u0026 visualisation script. Fetches completed WandB runs,\nexports metrics, generates per-run and aggregated figures (learning curves,\nconfusion matrices, comparison charts) and performs basic statistical tests.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom scipy.stats import ttest_ind\nfrom tqdm.auto import tqdm\n\n# -----------------------------------------------------------------------------\n# Helper functions\n# -----------------------------------------------------------------------------\n\ndef save_json(obj: dict, path: Path) -\u003e None:\n    path.write_text(json.dumps(obj, indent=2))\n\n\ndef plot_learning_curve(history: pd.DataFrame, run_id: str, out_dir: Path) -\u003e Path:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    if \"train_loss\" in history:\n        ax.plot(history[\"_step\"], history[\"train_loss\"], label=\"train_loss\")\n    if \"validation_accuracy\" in history:\n        ax.plot(history[\"_step\"], history[\"validation_accuracy\"], label=\"val_acc\")\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Metric value\")\n    ax.set_title(f\"Learning curves \u2013 {run_id}\")\n    ax.legend()\n    fig.tight_layout()\n    path = out_dir / f\"{run_id}_learning_curve.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef plot_confusion_matrix(conf: np.ndarray, run_id: str, out_dir: Path) -\u003e Path:\n    fig, ax = plt.subplots(figsize=(4, 4))\n    sns.heatmap(conf, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"True\")\n    ax.set_title(f\"Confusion matrix \u2013 {run_id}\")\n    fig.tight_layout()\n    path = out_dir / f\"{run_id}_confusion_matrix.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef bar_chart(agg: pd.DataFrame, out_dir: Path) -\u003e Path:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.barplot(data=agg, x=\"run_id\", y=\"best_val_acc\", hue=\"method\", ax=ax)\n    for i, v in enumerate(agg.best_val_acc):\n        ax.text(i, v + 0.002, f\"{v:.3f}\", ha=\"center\")\n    ax.set_ylabel(\"Best validation accuracy\")\n    ax.set_xlabel(\"\")\n    ax.set_ylim(0, agg.best_val_acc.max() * 1.05)\n    fig.tight_layout()\n    path = out_dir / \"comparison_accuracy_bar_chart.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef box_plot(values: Dict[str, List[float]], out_dir: Path) -\u003e Path:\n    df = pd.DataFrame({\"run_id\": np.repeat(list(values.keys()), [len(v) for v in values.values()]),\n                       \"val_acc\": np.concatenate(list(values.values()))})\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.boxplot(data=df, x=\"run_id\", y=\"val_acc\", ax=ax)\n    fig.tight_layout()\n    path = out_dir / \"comparison_val_accuracy_boxplot.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n# -----------------------------------------------------------------------------\n# Main entry\n# -----------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Evaluation \u0026 visualisation \u2013 WandB fetcher\")\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory where outputs will be saved\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON string list of WandB run IDs\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir).expanduser().absolute()\n    results_dir.mkdir(parents=True, exist_ok=True)\n    run_ids: List[str] = json.loads(args.run_ids)\n\n    # --------------------------------------------------\n    # Load WandB entity/project from config/config.yaml\n    # --------------------------------------------------\n    import yaml\n    cfg_path = Path(__file__).resolve().parent.parent / \"config\" / \"config.yaml\"\n    with cfg_path.open() as f:\n        global_cfg = yaml.safe_load(f)\n    entity = global_cfg[\"wandb\"][\"entity\"]\n    project = global_cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    aggregated_metrics: List[dict] = []\n    epoch_accuracy_values: Dict[str, List[float]] = {}\n    generated_files: List[str] = []\n\n    # --------------------------------------------------\n    # Per-run processing\n    # --------------------------------------------------\n    for rid in tqdm(run_ids, desc=\"Processing runs\"):\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        history = run.history(samples=100000)  # large enough\n        summary = run.summary._json_dict\n        config = dict(run.config)\n\n        run_dir = results_dir / rid\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save complete metrics \u0026 config\n        save_json({\"history\": history.to_dict(orient=\"list\"),\n                   \"summary\": summary,\n                   \"config\": config}, run_dir / \"metrics.json\")\n\n        # Learning curve figure\n        generated_files.append(str(plot_learning_curve(history, rid, run_dir)))\n\n        # Confusion matrix (if available)\n        conf_key = next((k for k in summary if k.startswith(\"confusion_matrix\")), None)\n        if conf_key:\n            conf_array = np.array(summary[conf_key])\n            generated_files.append(str(plot_confusion_matrix(conf_array, rid, run_dir)))\n\n        # Aggregated metrics collection\n        best_val_acc = summary.get(\"best_val_acc\", None)\n        aggregated_metrics.append({\"run_id\": rid,\n                                   \"best_val_acc\": best_val_acc,\n                                   \"method\": config.get(\"method\", \"unknown\")})\n\n        # Epoch-wise accuracy list for statistical test\n        if \"validation_accuracy\" in history:\n            epoch_acc = history.dropna(subset=[\"validation_accuracy\"]).groupby(\"epoch\")[\"validation_accuracy\"].max().tolist()\n            epoch_accuracy_values[rid] = epoch_acc\n\n    # --------------------------------------------------\n    # Aggregated analysis\n    # --------------------------------------------------\n    comp_dir = results_dir / \"comparison\"\n    comp_dir.mkdir(exist_ok=True)\n\n    agg_df = pd.DataFrame(aggregated_metrics)\n    save_json(aggregated_metrics, comp_dir / \"aggregated_metrics.json\")\n    agg_df.to_csv(comp_dir / \"aggregated_metrics.csv\", index=False)\n\n    # Improvement rates w.r.t. baseline (first run)\n    baseline_val = agg_df.best_val_acc.iloc[0]\n    agg_df[\"improvement_rate\"] = (agg_df.best_val_acc - baseline_val) / baseline_val\n    agg_df.to_csv(comp_dir / \"aggregated_with_improvement.csv\", index=False)\n\n    # Significance tests \u2013 pairwise t-test on epoch-wise accuracies\n    significance_results = {}\n    run_pairs = [(run_ids[i], run_ids[j]) for i in range(len(run_ids)) for j in range(i + 1, len(run_ids))]\n    for a, b in run_pairs:\n        if a in epoch_accuracy_values and b in epoch_accuracy_values:\n            t_stat, p_val = ttest_ind(epoch_accuracy_values[a], epoch_accuracy_values[b], equal_var=False)\n            significance_results[f\"{a} vs {b}\"] = {\"t_stat\": float(t_stat), \"p_val\": float(p_val)}\n    save_json(significance_results, comp_dir / \"significance_tests.json\")\n\n    # Comparison figures\n    generated_files.append(str(bar_chart(agg_df, comp_dir)))\n    generated_files.append(str(box_plot(epoch_accuracy_values, comp_dir)))\n\n    # --------------------------------------------------\n    # Print generated file paths\n    # --------------------------------------------------\n    print(\"Generated files:\")\n    for p in generated_files:\n        print(p)\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "#!/usr/bin/env python\n\"\"\"src/main.py\nJob orchestrator \u2013 reads Hydra config once then launches the actual training\ncode (src.train) as a subprocess so that the training run has its own Hydra\noutput directory.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport shlex\nimport subprocess\nimport sys\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\ndef _get_run_cfg(cfg: DictConfig):\n    if \"run\" in cfg and cfg.run:\n        return cfg.run\n    if \"runs\" in cfg and cfg.runs:\n        return cfg.runs\n    raise KeyError(\"Cannot locate run configuration \u2013 check Hydra group name.\")\n\n\n@hydra.main(config_path=\"../config\", version_base=None)\ndef main(cfg: DictConfig):\n    # Adjust mode-specific settings early so that subprocess inherits the right overrides\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    run_cfg = _get_run_cfg(cfg)\n\n    cmd = [\n        sys.executable, \"-u\", \"-m\", \"src.train\",\n        f\"run={run_cfg.run_id}\",  # CLI override for Hydra in the child process\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n    cmd_str = \" \".join(shlex.quote(c) for c in cmd)\n    print(\"Launching training subprocess:\\n\", cmd_str)\n    env = os.environ.copy()\n    subprocess.run(cmd, check=True, env=env)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "#!/usr/bin/env python\n\"\"\"src/model.py\nModular model builder (currently wraps HuggingFace + LoRA).\"\"\"\nfrom __future__ import annotations\n\nfrom transformers import AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model\n\nCACHE_DIR = \".cache/\"\n\n\ndef build_model(cfg):\n    num_labels = 3  # GLUE-MNLI default, override as needed\n    base = AutoModelForSequenceClassification.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR,\n                                                              num_labels=num_labels)\n    lora_cfg = LoraConfig(r=cfg.model.adapter.rank,\n                          lora_alpha=cfg.model.adapter.lora_alpha,\n                          target_modules=[\"query_proj\", \"value_proj\"],\n                          bias=\"none\")\n    return get_peft_model(base, lora_cfg)\n", "preprocess_py": "#!/usr/bin/env python\n\"\"\"src/preprocess.py\nStandalone dataset loader \u0026 tokenizer helpers (mirrors logic used in training).\nUseful for quick interactive checks.\n\"\"\"\nfrom __future__ import annotations\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\nfrom omegaconf import DictConfig\n\nCACHE_DIR = \".cache/\"\n\n\ndef build_loaders(cfg: DictConfig):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR)\n    dataset = load_dataset(cfg.dataset.name, cfg.dataset.get(\"subset\", None), cache_dir=CACHE_DIR)\n\n    task = cfg.dataset.get(\"subset\", \"mnli\")\n    s1, s2 = {\"mnli\": (\"premise\", \"hypothesis\"), \"rte\": (\"sentence1\", \"sentence2\"), \"sst2\": (\"sentence\", None)}.get(task, (\"premise\", \"hypothesis\"))\n\n    def tok(example):\n        args = (example[s1],) if s2 is None else (example[s1], example[s2])\n        return tokenizer(*args, truncation=True, padding=\"max_length\", max_length=cfg.dataset.max_seq_length)\n\n    dataset = dataset.map(tok, batched=True, load_from_cache_file=True)\n    keep_cols = {\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"}\n    for split in dataset.keys():\n        dataset[split] = dataset[split].remove_columns([c for c in dataset[split].column_names if c not in keep_cols])\n\n    collator = DataCollatorWithPadding(tokenizer)\n    def loader(split, bs, shuf):\n        return DataLoader(dataset[split], batch_size=bs, shuffle=shuf, collate_fn=collator)\n\n    return loader(\"train\", cfg.training.batch_size, True), loader(\"validation\", cfg.training.batch_size, False)\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"iwlora-glue\"\nversion = \"0.1.0\"\nrequires-python = \"\u003e=3.9\"\ndescription = \"Importance-Weighted LoRA experiments on GLUE\"\ndependencies = [\n  \"torch\u003e=2.1\",\n  \"transformers\u003e=4.37\",\n  \"datasets\u003e=2.14\",\n  \"peft\u003e=0.6.2\",\n  \"scikit-learn\u003e=1.3\",\n  \"hydra-core\u003e=1.3\",\n  \"wandb\u003e=0.16\",\n  \"optuna\u003e=3.5\",\n  \"seaborn\u003e=0.13\",\n  \"matplotlib\u003e=3.8\",\n  \"pandas\u003e=2.1\",\n  \"scipy\u003e=1.11\",\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "train_py": "#!/usr/bin/env python\n\"\"\"src/train.py\nSingle-run trainer. Implements Importance-Weighted LoRA (IW-LoRA) with full\nWandB \u0026 Optuna integration. No placeholders \u2013 full, production-ready code.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport hydra\nimport numpy as np\nimport optuna\nimport torch\nimport torch.nn.functional as F  # noqa: F401 (sometimes used in derived methods)\nimport wandb\nfrom datasets import load_dataset\nfrom omegaconf import DictConfig, OmegaConf\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n                          DataCollatorWithPadding, get_linear_schedule_with_warmup)\n\nfrom peft import LoraConfig, get_peft_model\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Helper utilities\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int) -\u003e None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_device() -\u003e torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef get_run_cfg(cfg: DictConfig) -\u003e DictConfig:  # handles run / runs mismatch\n    if \"run\" in cfg and cfg.run:\n        return cfg.run\n    if \"runs\" in cfg and cfg.runs:\n        return cfg.runs\n    raise KeyError(\"Neither cfg.run nor cfg.runs is present \u2013 check Hydra group name.\")\n\n# -----------------------------------------------------------------------------\n# Data pipeline (tokenisation + Dataloaders)\n# -----------------------------------------------------------------------------\n\ndef prepare_dataloaders(cfg: DictConfig) -\u003e Tuple[AutoTokenizer, DataLoader, DataLoader]:\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR)\n\n    dataset = load_dataset(cfg.dataset.name, cfg.dataset.get(\"subset\", None), cache_dir=CACHE_DIR)\n\n    task = cfg.dataset.get(\"subset\", \"mnli\")\n    sentence1_key, sentence2_key = {\n        \"mnli\": (\"premise\", \"hypothesis\"),\n        \"rte\": (\"sentence1\", \"sentence2\"),\n        \"sst2\": (\"sentence\", None),\n    }.get(task, (\"premise\", \"hypothesis\"))\n\n    def tokenize_fn(ex):\n        args = (ex[sentence1_key],) if sentence2_key is None else (ex[sentence1_key], ex[sentence2_key])\n        return tokenizer(*args,\n                         truncation=cfg.dataset.preprocessing.truncation,\n                         padding=cfg.dataset.preprocessing.padding,\n                         max_length=cfg.dataset.max_seq_length)\n\n    dataset = dataset.map(tokenize_fn, batched=True, load_from_cache_file=True)\n    keep_cols = {\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"}\n    for split in dataset.keys():\n        remove_cols = [c for c in dataset[split].column_names if c not in keep_cols]\n        dataset[split] = dataset[split].remove_columns(remove_cols)\n\n    data_collator = DataCollatorWithPadding(tokenizer)\n\n    def make_loader(name: str, shuffle: bool, bs: int):\n        return DataLoader(dataset[name], batch_size=bs, shuffle=shuffle, collate_fn=data_collator)\n\n    train_loader = make_loader(cfg.dataset.splits.train, True, cfg.training.batch_size)\n    val_loader = make_loader(cfg.dataset.splits.validation, False, cfg.training.batch_size)\n    return tokenizer, train_loader, val_loader\n\n# -----------------------------------------------------------------------------\n# Sensitivity collection for IW-LoRA\n# -----------------------------------------------------------------------------\n\ndef collect_sensitivity(model: torch.nn.Module, loader: DataLoader, steps: int) -\u003e Dict[str, float]:\n    device = next(model.parameters()).device\n    sens: Dict[str, float] = {n: 0.0 for n, _ in model.named_parameters() if \"lora_A\" in n}\n    model.train()\n    iterator = iter(loader)\n    pbar = tqdm(total=steps, desc=\"IW-LoRA warm-up\", leave=False)\n    for _ in range(steps):\n        try:\n            batch = next(iterator)\n        except StopIteration:\n            iterator = iter(loader)\n            batch = next(iterator)\n        batch = {k: v.to(device) for k, v in batch.items()}\n        loss = model(**batch).loss\n        loss.backward()\n        for n, p in model.named_parameters():\n            if n in sens and p.grad is not None:\n                sens[n] += p.grad.detach().pow(2).sum().item()\n        model.zero_grad(set_to_none=True)\n        pbar.update(1)\n    pbar.close()\n    total = sum(sens.values()) or 1e-12\n    for k in sens:\n        sens[k] /= total\n    return sens\n\n# -----------------------------------------------------------------------------\n# Evaluation helper returning accuracy + confusion matrix\n# -----------------------------------------------------------------------------\n\ndef evaluate(model: torch.nn.Module, loader: DataLoader) -\u003e Tuple[float, np.ndarray]:\n    device = next(model.parameters()).device\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            logits = outputs.logits\n            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n            labels.extend(batch[\"label\"].cpu().tolist())\n    acc = accuracy_score(labels, preds)\n    conf = confusion_matrix(labels, preds)\n    model.train()\n    return acc, conf\n\n# -----------------------------------------------------------------------------\n# Optuna objective (lightweight single-epoch run)\n# -----------------------------------------------------------------------------\n\ndef optuna_objective(trial: optuna.Trial, base_cfg: DictConfig) -\u003e float:\n    cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True, deepcopy=True))\n    # suggestions\n    lr = trial.suggest_float(\"learning_rate\", cfg.optuna.search_space.learning_rate.low,\n                             cfg.optuna.search_space.learning_rate.high, log=True)\n    bs = trial.suggest_categorical(\"batch_size\", cfg.optuna.search_space.batch_size.choices)\n    cfg.training.learning_rate = lr\n    cfg.training.batch_size = bs\n    if cfg.model.get(\"importance_weighted_regularisation\", {}).get(\"enabled\", False):\n        cfg.model.importance_weighted_regularisation.reg_alpha = trial.suggest_float(\n            \"iw_reg_alpha\", cfg.optuna.search_space.iw_reg_alpha.low,\n            cfg.optuna.search_space.iw_reg_alpha.high, log=True)\n\n    # prepare data\n    _, train_loader, val_loader = prepare_dataloaders(cfg)\n\n    # model\n    num_labels = 3  # MNLI\n    base = AutoModelForSequenceClassification.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR,\n                                                              num_labels=num_labels)\n    lora_cfg = LoraConfig(r=cfg.model.adapter.rank,\n                          lora_alpha=cfg.model.adapter.lora_alpha,\n                          target_modules=[\"query_proj\", \"value_proj\"],\n                          bias=\"none\")\n    model = get_peft_model(base, lora_cfg).to(get_device())\n\n    optim = torch.optim.AdamW(model.parameters(), lr=cfg.training.learning_rate,\n                              weight_decay=cfg.training.weight_decay)\n    scheduler = get_linear_schedule_with_warmup(optim, 0, len(train_loader))\n\n    # train 1 epoch (or up to 200 steps) \u2013 AMP off for simplicity\n    model.train()\n    for step, batch in enumerate(train_loader):\n        batch = {k: v.to(get_device()) for k, v in batch.items()}\n        optim.zero_grad(set_to_none=True)\n        loss = model(**batch).loss\n        loss.backward()\n        optim.step(); scheduler.step()\n        if step \u003e 200:\n            break\n    val_acc, _ = evaluate(model, val_loader)\n    return val_acc\n\n# -----------------------------------------------------------------------------\n# Training entry-point \u2013 full experiment\n# -----------------------------------------------------------------------------\n@hydra.main(config_path=\"../config\", version_base=None)\ndef main(cfg: DictConfig) -\u003e None:  # pylint: disable=too-many-locals,too-many-statements\n    os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"true\")\n    set_seed(cfg.training.get(\"seed\", 42))\n\n    # --------------------------------------------------------------\n    # Robust handling for trial / full mode\n    # --------------------------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.log_every_n_steps = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"cfg.mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    run_cfg = get_run_cfg(cfg)\n\n    # --------------------------------------------------------------\n    # Hyper-parameter search\n    # --------------------------------------------------------------\n    if cfg.optuna.get(\"n_trials\", 0) \u003e 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(lambda t: optuna_objective(t, cfg), n_trials=cfg.optuna.n_trials)\n        for k, v in study.best_params.items():\n            if k == \"learning_rate\":\n                cfg.training.learning_rate = v\n            elif k == \"batch_size\":\n                cfg.training.batch_size = v\n            elif k == \"iw_reg_alpha\":\n                cfg.model.importance_weighted_regularisation.reg_alpha = v\n\n    # --------------------------------------------------------------\n    # Initialise WandB (optional)\n    # --------------------------------------------------------------\n    if cfg.wandb.mode != \"disabled\":\n        wandb.init(entity=cfg.wandb.entity,\n                   project=cfg.wandb.project,\n                   id=run_cfg.run_id,\n                   config=OmegaConf.to_container(cfg, resolve=True),\n                   resume=\"allow\",\n                   mode=cfg.wandb.mode)\n        print(\"WandB URL:\", wandb.run.get_url())\n\n    # --------------------------------------------------------------\n    # Data \u0026 model\n    # --------------------------------------------------------------\n    tokenizer, train_loader, val_loader = prepare_dataloaders(cfg)\n\n    num_labels = 3  # adjust per task if needed\n    base_model = AutoModelForSequenceClassification.from_pretrained(cfg.model.name, cache_dir=CACHE_DIR,\n                                                                    num_labels=num_labels)\n    lora_cfg = LoraConfig(r=cfg.model.adapter.rank,\n                          lora_alpha=cfg.model.adapter.lora_alpha,\n                          target_modules=[\"query_proj\", \"value_proj\"],\n                          bias=\"none\")\n    model = get_peft_model(base_model, lora_cfg).to(get_device())\n\n    # --------------------------------------------------------------\n    # IW-LoRA importance weights\n    # --------------------------------------------------------------\n    importance_weights: Dict[str, float] = {}\n    if cfg.model.importance_weighted_regularisation.enabled:\n        importance_weights = collect_sensitivity(model, train_loader,\n                                                 cfg.model.importance_weighted_regularisation.sensitivity_warmup_steps)\n\n    # --------------------------------------------------------------\n    # Optimiser, scheduler, scaler\n    # --------------------------------------------------------------\n    total_steps = (len(train_loader) * cfg.training.epochs) // cfg.training.gradient_accumulation_steps\n    optimiser = torch.optim.AdamW(model.parameters(), lr=cfg.training.learning_rate,\n                                  weight_decay=cfg.training.weight_decay)\n    scheduler = get_linear_schedule_with_warmup(optimiser, cfg.training.warmup_steps, total_steps)\n    scaler = torch.cuda.amp.GradScaler(enabled=cfg.training.fp16 and torch.cuda.is_available())\n\n    # --------------------------------------------------------------\n    # Training loop\n    # --------------------------------------------------------------\n    best_val_acc = 0.0\n    global_step = 0\n    device = get_device()\n    for epoch in range(cfg.training.epochs):\n        model.train()\n        epoch_loss = 0.0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.training.epochs}\")\n        for step, batch in enumerate(pbar):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with torch.cuda.amp.autocast(enabled=cfg.training.fp16 and torch.cuda.is_available()):\n                outputs = model(**batch)\n                task_loss = outputs.loss\n                # IW-LoRA regularisation\n                if cfg.model.importance_weighted_regularisation.enabled:\n                    reg = 0.0\n                    beta = cfg.model.importance_weighted_regularisation.beta\n                    alpha_reg = cfg.model.importance_weighted_regularisation.reg_alpha\n                    for name, param in model.named_parameters():\n                        if \"lora_A\" in name or \"lora_B\" in name:\n                            key = name.replace(\"lora_B\", \"lora_A\")\n                            w_k = importance_weights.get(key, 0.0)\n                            reg += ((1 - w_k) ** beta) * param.pow(2).sum()\n                    loss = task_loss + alpha_reg * reg\n                else:\n                    loss = task_loss\n            scaler.scale(loss / cfg.training.gradient_accumulation_steps).backward()\n            if (step + 1) % cfg.training.gradient_accumulation_steps == 0:\n                scaler.step(optimiser)\n                scaler.update()\n                optimiser.zero_grad(set_to_none=True)\n                scheduler.step()\n                global_step += 1\n\n            epoch_loss += loss.item()\n            if cfg.wandb.mode != \"disabled\" and global_step % cfg.training.log_every_n_steps == 0:\n                wandb.log({\"train_loss\": loss.item(),\n                           \"lr\": scheduler.get_last_lr()[0],\n                           \"global_step\": global_step}, step=global_step)\n\n            if cfg.mode == \"trial\" and step \u003e 1:  # ultra-fast CI mode\n                break\n        pbar.close()\n\n        # ---------------- Validation ----------------\n        val_acc, conf = evaluate(model, val_loader)\n        best_val_acc = max(best_val_acc, val_acc)\n        if cfg.wandb.mode != \"disabled\":\n            wandb.log({\"validation_accuracy\": val_acc, \"epoch\": epoch + 1}, step=global_step)\n            wandb.summary[f\"confusion_matrix_epoch_{epoch+1}\"] = conf.tolist()\n        print(f\"Epoch {epoch+1} finished \u2013 val acc {val_acc:.4f}\")\n        if cfg.mode == \"trial\":\n            break\n\n    # --------------------------------------------------------------\n    # Wrap-up\n    # --------------------------------------------------------------\n    if cfg.wandb.mode != \"disabled\":\n        wandb.summary[\"best_val_acc\"] = best_val_acc\n        wandb.finish()\n    print(\"Training completed. Best validation acc:\", best_val_acc)\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "Several mandatory requirements are violated:\n\n1. Trial-mode computational budget is not respected.  When cfg.mode == \"trial\" the training loop is shortened, but the IW-LoRA warm-up still executes cfg.model.importance_weighted_regularisation.sensitivity_warmup_steps (150 forward/backward passes).  This contradicts requirement 9, which demands only 1–2 batches in trial mode.\n\n2. The parameter freeze_lora_during_warmup defined in the run config is ignored; LoRA weights remain trainable during the warm-up despite the flag.  This means the implementation is not fully faithful to the configuration (criterion 1 – complete implementation).\n\n3. results_dir passed from the CLI is never created/used in train.py, so artefacts other than WandB (e.g. checkpoints) cannot be stored there (minor, but violates criterion 2 on automatic configuration handling).\n\nBecause of issue (1) the code will exceed CI/fast-validation limits and therefore is not ready for immediate execution.  Please add a fast path that skips or drastically shortens the warm-up when mode==\"trial\", honour freeze_lora_during_warmup, and ensure results_dir is created/used in train.py."
}
