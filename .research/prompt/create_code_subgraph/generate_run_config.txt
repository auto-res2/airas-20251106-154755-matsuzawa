
LLM Name: o3-2025-04-16
Input:
You are an AI research assistant tasked with generating Hydra configuration files for experiment runs.

# Task
Generate individual YAML configuration files for each experiment run. These configs will be used by Hydra to configure specific experimental variations.

# Input Information

## Hypothesis
open_problems='LoRA distributes the same rank to every weight matrix. AdaLoRA fixes this with iterative SVD‐based pruning but: (1) adds ~10–15 % training time, (2) introduces many scheduler hyper-parameters, (3) is fragile to importance-score estimation errors. We need a cheaper, hyper-parameter-light way to let more important layers use more of the limited LoRA capacity.' method='Importance–Weighted LoRA (IW-LoRA)\nMinimal change: keep standard LoRA training but add a single extra L2 term whose coefficient is different for every weight matrix.\n1. Warm-up (calibration) phase: run 100–200 forward/backward passes with LoRA parameters frozen (or very small lr) and record the running average of per-matrix gradient sensitivity s_k = E[||∂L/∂W_k||_F].\n2. Define importance weight w_k = s_k / (Σ_j s_j).\n3. During actual fine-tuning (LoRA parameters trainable) minimise\n   L_total = L_task  +  α · Σ_k (1 – w_k)^β · ||Δ_k||_F² ,\n   where Δ_k is the LoRA update for matrix k, α≃1e-4 and β=1 (can be fixed).\n   Layers with high w_k receive almost no penalty and can use the rank budget freely, while unimportant layers are softly shrunk toward zero; the fixed global rank therefore re-allocates itself automatically without pruning or SVD.' experimental_setup='Backbone: DeBERTaV3-base.\nDatasets: GLUE (MNLI, SST-2, RTE).\nBaselines: (a) Full fine-tune, (b) LoRA (rank=8 everywhere), (c) AdaLoRA (rank budget equivalent to rank 8).\nIW-LoRA uses the same fixed rank=8 but the new loss term.\nMetrics: dev accuracy (MNLI), F1 (RTE), and average score.\nTraining: 3 epochs, lr 2e-4, batch 32. α=1e-4, β=1 fixed for all runs.\nHardware: single V100 GPU.' experimental_code="# key excerpt – add IW-LoRA regularisation\ndef collect_sensitivity(model, data_loader, steps=200):\n    sens = {n:0. for n,p in model.named_parameters() if 'lora_A' in n}\n    for i, batch in enumerate(data_loader):\n        if i==steps: break\n        loss = model(**batch).loss\n        loss.backward()\n        for n,p in model.named_parameters():\n            if n in sens:\n                sens[n] += p.grad.detach().pow(2).sum()\n        model.zero_grad()\n    # normalise\n    total = sum(sens.values())\n    for n in sens: sens[n] = sens[n]/total\n    return sens\n\nsens = collect_sensitivity(model, train_loader)\n\nalpha, beta = 1e-4, 1.0\n\n# training loop\nfor batch in train_loader:\n    outputs = model(**batch)\n    loss = outputs.loss\n    reg = 0.\n    for n,p in model.named_parameters():\n        if 'lora_A' in n or 'lora_B' in n:  # treat A and B together\n            key = n.replace('lora_A','lora_A') if 'lora_A' in n else n.replace('lora_B','lora_A')\n            reg += (1-sens[key])**beta * p.pow(2).sum()\n    loss = loss + alpha*reg\n    loss.backward()\n    optimizer.step(); optimizer.zero_grad()" expected_result='IW-LoRA should match AdaLoRA and outperform vanilla LoRA while training in roughly the same time as LoRA (≈10-15 % faster than AdaLoRA). On GLUE we expect:\nMNLI accuracy: LoRA 84.6, AdaLoRA 85.3, IW-LoRA ≈85.1.\nSST-2 accuracy: LoRA 93.2, AdaLoRA 93.8, IW-LoRA ≈93.7.\nRTE F1: LoRA 72.0, AdaLoRA 73.4, IW-LoRA ≈73.2.\nAverage gain of ~0.6 points over vanilla LoRA with negligible extra compute.' expected_conclusion='A single importance-weighted L2 term lets the model allocate its fixed low-rank capacity where it matters, approaching AdaLoRA’s accuracy without its computational overhead or complex pruning schedule. The change is only a few lines of code and one new global hyper-parameter (α), making the technique easy to adopt in any LoRA-based fine-tuning pipeline.'

## Research Method
Importance–Weighted LoRA (IW-LoRA)
Minimal change: keep standard LoRA training but add a single extra L2 term whose coefficient is different for every weight matrix.
1. Warm-up (calibration) phase: run 100–200 forward/backward passes with LoRA parameters frozen (or very small lr) and record the running average of per-matrix gradient sensitivity s_k = E[||∂L/∂W_k||_F].
2. Define importance weight w_k = s_k / (Σ_j s_j).
3. During actual fine-tuning (LoRA parameters trainable) minimise
   L_total = L_task  +  α · Σ_k (1 – w_k)^β · ||Δ_k||_F² ,
   where Δ_k is the LoRA update for matrix k, α≃1e-4 and β=1 (can be fixed).
   Layers with high w_k receive almost no penalty and can use the rank budget freely, while unimportant layers are softly shrunk toward zero; the fixed global rank therefore re-allocates itself automatically without pruning or SVD.

## Experimental Design
experiment_summary='The experiment evaluates the new Importance-Weighted LoRA (IW-LoRA) regularisation against the standard LoRA adaptation on a strong encoder model.  \n1. We fine-tune a single backbone (DeBERTaV3-base, ≈86 M parameters) on the GLUE MNLI natural-language-understanding benchmark.  \n2. A short, 150-step warm-up pass is run with LoRA parameters frozen to collect per-matrix gradient sensitivities s_k.  \n3. Sensitivities are normalised to importance weights w_k and stored.  \n4. Full fine-tuning then proceeds for three epochs. The ordinary task loss is augmented with the importance-weighted L2 term α·Σ_k(1-w_k)^β‖Δ_k‖² (α searched, β fixed to 1).  \n5. We compare final validation accuracy to a vanilla LoRA baseline with identical rank=8 adapters.  \n6. All runs use the same optimisation settings (AdamW, three epochs, learning-rate schedule) and are executed on a single NVIDIA A100 (the available environment far exceeds the memory footprint).  \nThe workflow therefore isolates the effect of the proposed regulariser while keeping every other variable identical.' evaluation_metrics=['Accuracy'] proposed_method='Importance-Weighted LoRA (IW-LoRA) is a drop-in regularisation for LoRA fine-tuning that redistributes a fixed low-rank budget toward layers that matter most.  \n• Theory: For each trainable weight matrix W_k equipped with a rank-r LoRA update Δ_k = A_kB_k, we approximate its task importance with the Frobenius norm of the gradient ‖∂L/∂W_k‖_F averaged over a small calibration run.  \n• Calibration: Freeze LoRA parameters, run 100–200 forward/backward passes, accumulate s_k = E[‖∂L/∂W_k‖_F].  \n• Importance weights: w_k = s_k / Σ_j s_j.  \n• Training objective: L_total = L_task + α Σ_k (1−w_k)^β ‖Δ_k‖_F² with global α (searched) and β=1. High-importance layers (large w_k) receive almost no penalty; low-importance layers are softly shrunk, effectively donating rank capacity to useful layers without explicit pruning or SVD.  \n• Implementation: only ~15 lines added to a standard LoRA training loop—store sensitivities, then, during each training step, compute the weighted L2 regulariser over all LoRA parameters and add it to the loss before back-propagation.  \n• Hyper-parameter economy: only α needs tuning; β can stay at 1 for all tasks.  \n• Computational cost: negligible—no iterative SVD, schedulers, or additional optimiser steps; calibration adds <2 % wall-time.' comparative_methods=['LoRA (rank = 8, no regularisation)'] models_to_use=['DeBERTaV3-base (≈86 M)'] datasets_to_use=['GLUE-MNLI'] hyperparameters_to_search={'learning_rate': '1e-5-5e-4', 'alpha_IW_LoRA': '1e-5-1e-3', 'batch_size': '16,32,64'} external_resources=ExternalResources(hugging_face=HuggingFace(models=[HuggingFaceResource(id='microsoft/deberta-v3-base', author='microsoft', sha='8ccc9b6f36199bec6961081d44eb72fb3f7353f3', created_at=datetime.datetime(2022, 3, 2, 23, 29, 5, tzinfo=TzInfo(UTC)), last_modified=datetime.datetime(2022, 9, 22, 12, 34, 19, tzinfo=TzInfo(UTC)), private=False, gated=False, disabled=False, downloads=2518675, likes=372, siblings=[HuggingFaceSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='config.json', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='pytorch_model.bin', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='rust_model.ot', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='spm.model', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='tf_model.h5', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)], card_data=HuggingFaceCardData(license='mit', language='en', library_name=None, pipeline_tag=None, tags=['deberta', 'deberta-v3', 'fill-mask'], datasets=[], model_type=None, base_model=None, task_categories=[], size_categories=[], metrics=[], widget=[]), tags=['transformers', 'pytorch', 'tf', 'rust', 'deberta-v2', 'deberta', 'deberta-v3', 'fill-mask', 'en', 'arxiv:2006.03654', 'arxiv:2111.09543', 'license:mit', 'endpoints_compatible', 'region:us'], pipeline_tag='fill-mask', library_name='transformers', readme='---\nlanguage: en\ntags: \n  - deberta\n  - deberta-v3\n  - fill-mask\nthumbnail: https://huggingface.co/front/thumbnails/microsoft.png\nlicense: mit\n---\n\n## DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. \n\nIn [DeBERTa V3](https://arxiv.org/abs/2111.09543), we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our [paper](https://arxiv.org/abs/2111.09543).\n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more implementation details and updates.\n\nThe DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters  with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.\n\n\n#### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 2.0 and MNLI tasks.\n\n| Model             |Vocabulary(K)|Backbone #Params(M)| SQuAD 2.0(F1/EM) | MNLI-m/mm(ACC)|\n|-------------------|----------|-------------------|-----------|----------|\n| RoBERTa-base      |50     |86                 | 83.7/80.5 | 87.6/-   |\n| XLNet-base        |32     |92                 | -/80.2    | 86.8/-   |\n| ELECTRA-base      |30    |86                  | -/80.5    | 88.8/    |\n| DeBERTa-base      |50     |100                |  86.2/83.1| 88.8/88.5|\n| DeBERTa-v3-base   |128|86                       | **88.4/85.4** | **90.6/90.7**|\n| DeBERTa-v3-base + SiFT |128|86                | -/- | 91.0/-|\n\nWe present the dev results on SQuAD 1.1/2.0 and MNLI tasks.\n\n#### Fine-tuning with HF transformers\n\n```bash\n#!/bin/bash\n\ncd transformers/examples/pytorch/text-classification/\n\npip install datasets\nexport TASK_NAME=mnli\n\noutput_dir="ds_results"\n\nnum_gpus=8\n\nbatch_size=8\n\npython -m torch.distributed.launch --nproc_per_node=${num_gpus} \\\n  run_glue.py \\\n  --model_name_or_path microsoft/deberta-v3-base \\\n  --task_name $TASK_NAME \\\n  --do_train \\\n  --do_eval \\\n  --evaluation_strategy steps \\\n  --max_seq_length 256 \\\n  --warmup_steps 500 \\\n  --per_device_train_batch_size ${batch_size} \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir $output_dir \\\n  --overwrite_output_dir \\\n  --logging_steps 1000 \\\n  --logging_dir $output_dir\n\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following papers:\n\n``` latex\n@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n```\n', model_index=None, widget_data=None, config=None, transformers_info=None, spaces=None, safetensors=None, used_storage=None, extracted_code='')], datasets=[HuggingFaceResource(id='Neuronovo/neuronovo-utc-data-glue-mnli', author='Neuronovo', sha='a1fbcdc0845b1793f066ceea222f1a60cc263a5f', created_at=datetime.datetime(2024, 1, 25, 19, 46, 58, tzinfo=TzInfo(UTC)), last_modified=datetime.datetime(2024, 1, 25, 21, 1, 39, tzinfo=TzInfo(UTC)), private=False, gated=False, disabled=False, downloads=56, likes=0, siblings=[HuggingFaceSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='data/test-00000-of-00001.parquet', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='data/train-00000-of-00001.parquet', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='data/validation-00000-of-00001.parquet', size=None, blob_id=None, lfs=None)], card_data=HuggingFaceCardData(license=None, language=[], library_name=None, pipeline_tag=None, tags=[], datasets=[], model_type=None, base_model=None, task_categories=[], size_categories=[], metrics=[], widget=[]), tags=['size_categories:1M<n<10M', 'format:parquet', 'modality:tabular', 'modality:text', 'library:datasets', 'library:pandas', 'library:mlcroissant', 'library:polars', 'region:us'], pipeline_tag=None, library_name=None, readme='---\ndataset_info:\n  features:\n  - name: x\n    dtype: string\n  - name: y\n    dtype: int64\n  - name: label_id\n    dtype: int64\n  - name: text\n    dtype: string\n  - name: id\n    dtype: int64\n  splits:\n  - name: train\n    num_bytes: 492690051\n    num_examples: 1119828\n  - name: validation\n    num_bytes: 25718605\n    num_examples: 58278\n  - name: test\n    num_bytes: 26234868\n    num_examples: 58941\n  download_size: 144048422\n  dataset_size: 544643524\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: data/train-*\n  - split: validation\n    path: data/validation-*\n  - split: test\n    path: data/test-*\n---\n', model_index=None, widget_data=None, config=None, transformers_info=None, spaces=None, safetensors=None, used_storage=None, extracted_code=None)])) experiment_code=None

## Experiment Runs

- Run ID: proposed-DeBERTaV3-base-86-M--GLUE-MNLI
  Method: proposed
  Model: DeBERTaV3-base (≈86 M)
  Dataset: GLUE-MNLI

- Run ID: comparative-1-DeBERTaV3-base-86-M--GLUE-MNLI
  Method: comparative-1
  Model: DeBERTaV3-base (≈86 M)
  Dataset: GLUE-MNLI


# Requirements

## Configuration Structure
Each run configuration should include:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings)
- optuna: Hyperparameter search space definition for Optuna optimization
  - Define search spaces for key hyperparameters using Optuna's suggest methods
  - Example: learning_rate: [1e-5, 1e-3], batch_size: [16, 32, 64]
- Any other experiment-specific settings

## Format
- Generate one YAML configuration per experiment run
- Ensure valid YAML syntax
- Use meaningful parameter values based on the research method and experimental design

## Example Configuration
```yaml
run_id: baseline_bert_imdb
method: baseline
model:
  name: bert-base-uncased
  hidden_size: 768
  num_layers: 12
dataset:
  name: imdb
  max_length: 512
  batch_size: 32
training:
  learning_rate: 2e-5
  epochs: 3
  optimizer: adamw
  warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
```

# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Instructions
1. Generate one YAML configuration for each experiment run listed above
2. Ensure configurations reflect the differences between baseline, proposed, and ablation methods
3. Use appropriate hyperparameters based on the experimental design
4. Include Optuna search space if hyperparameter optimization is beneficial for the experiment
5. For Optuna search spaces, use appropriate distribution types:
   - loguniform: For learning rates, regularization parameters
   - uniform: For dropout rates, weight decay
   - int: For hidden dimensions, number of layers
   - categorical: For discrete choices like batch size, optimizer type

Generate the configurations now:
Output:
{
  "run_configs": [
    {
      "run_id": "proposed-DeBERTaV3-base-86-M--GLUE-MNLI",
      "run_config_yaml": "run_id: proposed-DeBERTaV3-base-86-M--GLUE-MNLI\nmethod: proposed\nmodel:\n  name: microsoft/deberta-v3-base\n  adapter:\n    type: lora\n    rank: 8           # same global rank as baseline\n    lora_alpha: 16    # conventional LoRA scaling factor\n  importance_weighted_regularisation:\n    enabled: true\n    reg_alpha: 1e-4   # default value; tuned by Optuna under key ``iw_reg_alpha``\n    beta: 1.0\n    sensitivity_warmup_steps: 150\n    sensitivity_buffer_size: 1024  # mini-batches stored for running average\n    freeze_lora_during_warmup: true\n\ndataset:\n  name: glue\n  subset: mnli\n  splits:\n    train: train\n    validation: validation_matched\n  max_seq_length: 256\n  preprocessing:\n    truncation: true\n    padding: max_length\n\ntraining:\n  epochs: 3\n  batch_size: 32          # default; can be overridden by Optuna\n  learning_rate: 2.0e-4   # default; tuned by Optuna\n  optimizer: adamw\n  weight_decay: 0.01\n  lr_scheduler: linear\n  warmup_steps: 500\n  gradient_accumulation_steps: 1\n  fp16: true\n  seed: 42\n  log_every_n_steps: 50\n  save_strategy: epoch\n  evaluation_strategy: epoch\n\noptuna:\n  direction: maximize                 # maximise validation accuracy\n  metric_name: validation_accuracy\n  n_trials: 30\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1e-5\n      high: 5e-4\n    batch_size:\n      type: categorical\n      choices: [16, 32, 64]\n    iw_reg_alpha:\n      type: loguniform\n      low: 1e-5\n      high: 1e-3\n"
    },
    {
      "run_id": "comparative-1-DeBERTaV3-base-86-M--GLUE-MNLI",
      "run_config_yaml": "run_id: comparative-1-DeBERTaV3-base-86-M--GLUE-MNLI\nmethod: comparative-1\nmodel:\n  name: microsoft/deberta-v3-base\n  adapter:\n    type: lora\n    rank: 8\n    lora_alpha: 16\n  importance_weighted_regularisation:\n    enabled: false     # vanilla LoRA, no extra regulariser\n\ndataset:\n  name: glue\n  subset: mnli\n  splits:\n    train: train\n    validation: validation_matched\n  max_seq_length: 256\n  preprocessing:\n    truncation: true\n    padding: max_length\n\ntraining:\n  epochs: 3\n  batch_size: 32          # default; can be overridden by Optuna\n  learning_rate: 2.0e-4   # default; tuned by Optuna\n  optimizer: adamw\n  weight_decay: 0.01\n  lr_scheduler: linear\n  warmup_steps: 500\n  gradient_accumulation_steps: 1\n  fp16: true\n  seed: 42\n  log_every_n_steps: 50\n  save_strategy: epoch\n  evaluation_strategy: epoch\n\noptuna:\n  direction: maximize                 # maximise validation accuracy\n  metric_name: validation_accuracy\n  n_trials: 30\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1e-5\n      high: 5e-4\n    batch_size:\n      type: categorical\n      choices: [16, 32, 64]\n"
    }
  ]
}
